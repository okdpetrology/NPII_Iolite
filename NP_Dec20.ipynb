{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Made by Khalil Droubi, Geoscience Dept, UW-Madison\n",
    "###To process Iolite baseline-subtracted NP-II files for U-Th-Pb LA(SS)-ICPMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Makes Jupyter Notebook full width ###\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mod 11-37-2021:\n",
    "\n",
    "-Make sure that your sample names for MKED-1 and/or MudTank match the code!\n",
    "\n",
    "-Make sure that your analysis order is correct\n",
    "\n",
    "To Do:\n",
    "\n",
    "-Write function to chop lines up into smaller intervals, to test how long of a line we really need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "from math import log10, floor\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "from scipy.stats import linregress\n",
    "import webbrowser\n",
    "\n",
    "#Graphing stuff\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "\n",
    "#%pip install PyPDF2\n",
    "from PyPDF2 import PdfFileMerger, PdfFileReader\n",
    "\n",
    "#%pip install pdfkit\n",
    "import pdfkit\n",
    "\n",
    "#%pip install xlsxwriter\n",
    "import xlsxwriter\n",
    "\n",
    "#pd.set_option(\"display.precision\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants and published values\n",
    "\n",
    "# Initial coefficient\n",
    "\n",
    "l238U = 1.55125 * 10 ** (-10)  # lambda_238U Jaffey et al., (1971); Half-life = 4.4683 ± 0.0024 [10^9 years]\n",
    "l235U = 9.8485 * 10 ** (-10)  # lambda_235U Jaffey et al., (1971); Half-life = 7.0381 ± 0.0048 [10^8 years]\n",
    "l232Th = 4.9475 * 10 ** (-11)  # lambda_232Th \n",
    "#l231Pa = 2.10 * 10 ** (-5)  # Rempfer2017epsl\n",
    "#l230Th = 9.17 * 10 ** (-6)  # Cheng2013epsl\n",
    "U85r = 137.818  # 238U/235U; Hiess et al., (????) Default in IsoplotR, but the Isoplot default is usually 137.88\n",
    "\n",
    "\n",
    "#Published values dictionary MAY NEED TO ADD 207/235 -KD 10/18/21\n",
    "\n",
    "# These ratios need to be checked for accuracy, have reference added, and clarified for whether ratio is corrected or uncorrected for initial Pb. -KD 11/3/2021\n",
    "\n",
    "\n",
    "published_ratios_610 = {\n",
    "    '206/238': 0.258174418887103,\n",
    "    '208/232': 0.546910763260703,\n",
    "    '207/206': 0.909778846717897,\n",
    "    '207/235': (0.258174418887103) * (0.909778846717897) * U85r, #Calculated\n",
    "    '208/206': 2.16900334369684,\n",
    "    '206/204': 17.047,\n",
    "    '207/204': 15.509,\n",
    "    '208/204': 36.975,\n",
    "\n",
    "}\n",
    "published_ratios_612 = {\n",
    "    '206/238': 0.289090155580635,\n",
    "    '208/232': 0.598868250924868,\n",
    "    '207/206': 0.907335907335907,\n",
    "    '207/235': (0.289090155580635) * (0.907335907335907) * U85r, #Calculated\n",
    "    '208/206': 2.16450216450216,\n",
    "    '206/204': 17.094,\n",
    "    '207/204': 15.51,\n",
    "    '208/204': 37,      \n",
    "}\n",
    "published_ratios_BHVO = {\n",
    "    '206/238': 1.24298582359808,\n",
    "    '208/232': 0.811596334965975,\n",
    "    '207/206': 0.831206960977953,\n",
    "    '207/235': (1.24298582359808) * (0.831206960977953) * U85r, #Calculated\n",
    "    '208/206': 2.042918913147926,\n",
    "    '206/204': 18.733,\n",
    "    '207/204': 15.571,\n",
    "    '208/204': 38.27,      \n",
    "}   \n",
    "published_ratios_BCR_2G = {\n",
    "    '206/238': 1.90697056349179,\n",
    "    '208/232': 1.09082521297421,\n",
    "    '207/206': 0.832720490274447,\n",
    "    '207/235': (1.90697056349179) * (0.832720490274447) * U85r, #Calculated\n",
    "    '208/206': 2.06394884092726,\n",
    "    '206/204': 18.765,\n",
    "    '207/204': 15.626,\n",
    "    '208/204': 38.73,      \n",
    "}\n",
    "published_ratios_GSD_1G = {\n",
    "    '206/238': 0.367307298285922,\n",
    "    '208/232': 0.706245210573853,\n",
    "    '207/206': 0.80417794575821,\n",
    "    '207/235': (0.367307298285922) * (0.80417794575821) * U85r, #Calculated\n",
    "    '208/206': 1.98723121712038,\n",
    "    '206/204': 19.579,\n",
    "    '207/204': 15.745,\n",
    "    '208/204': 38.908,      \n",
    "}\n",
    "\n",
    "#Mod 9-1-2021: Changed GSE-1G values and MKED values\n",
    "published_ratios_GSE_1G = { \n",
    "    '206/238': 0.367307298285922, ###These are just the GSD-1G ratios\n",
    "    '208/232': 0.706245210573853, ###These are just the GSD-1G ratios\n",
    "    '207/206': 0.79232,\n",
    "    '208/206': 1.96460,\n",
    "    '206/204': 19.9250,\n",
    "    '207/204': 15.7870,\n",
    "    '208/204': 39.1450,      \n",
    "}\n",
    "#Mod 9-13-2021: Change these to measured values.\n",
    "published_ratios_MKED = {\n",
    "    '206/238': 0.265752478, #Total Sample from Spandler et al., 2015\n",
    "    #'206/238': 0.265752478 * 1.02, # Testing value -KD\n",
    "    \n",
    "    \n",
    "    #'206/238': 0.2634133, #ID-ICPMS (unpublished)\n",
    "    '208/232': 0.079191124, #ID-ICPMS (unpublished)\n",
    "    '207/206': 0.096, #Total Sample from Spandler et al., 2015\n",
    "    #'207/206': 0.095909435, #ID-ICPMS (unpublished)\n",
    "     '207/235': (0.265752478) * (0.096) * U85r, #Calculated with Spandler et al., 2015 values\n",
    "    '208/206': 0.852420222, #ID-ICPMS (unpublished)\n",
    "    #'206/204': 9090.909, #Total Sample from Spandler et al., 2015\n",
    "    '206/204': 10474.84592, #ID-ICPMS (unpublished)\n",
    "    '207/204': 1004.661031, #ID-ICPMS (unpublished)\n",
    "    '208/204': 8928.948295, #ID-ICPMS  (unpublished)   \n",
    "}    \n",
    "published_ratios_BLR_1 = {\n",
    "    '206/238': 0.176,\n",
    "    '208/232': 0.0533134203278787,\n",
    "    '207/206': 0.0743078,\n",
    "    '207/235': (0.176364) * (0.0743078) * U85r, #Calculated\n",
    "    '208/206': 0.17982,\n",
    "    '206/204': 975.36,\n",
    "    '207/204': 72.47686,\n",
    "    '208/204': 175.38924, \n",
    "}    \n",
    "\n",
    "published_ratios_BLR_1a = { #Arizona Laserchron values for 'BLS'\n",
    "    '206/238': 0.1769, #Arizona Laserchron values for 'BLS'\n",
    "    '208/232': 0.05332, #Arizona Laserchron values for 'BLS'\n",
    "    '207/206': 0.07431629, #Arizona Laserchron values for 'BLS'\n",
    "    '207/235': (0.1769) * (0.07431629) * U85r, #Calculated #Arizona Laserchron values for 'BLS'\n",
    "    '208/206': 0.015720513, # Calculated using U/Th ratio from Arizona... ?\n",
    "    '206/204': 975.36, # Aleinikoff et al., (2007) ?\n",
    "    '207/204': 72.47686, # Aleinikoff et al., (2007) ?\n",
    "    '208/204': 175.38924, # Aleinikoff et al., (2007) ?     \n",
    "    \n",
    "    \n",
    "}\n",
    "published_ratios_BLR_1b = { #KD MOD\n",
    "    '206/238': 0.1794688, \n",
    "    '208/232': 0.05332, #Arizona Laserchron values for 'BL\n",
    "    '207/206': 0.08714508,#Arizona Laserchron values for 'BLS'\n",
    "    '207/235': (0.1794688) * (0.08714508) * U85r,\n",
    "    '208/206': 0.015720513, # Calculated using U/Th ratio from Arizona... ?\n",
    "    '206/204': 975.36, # Aleinikoff et al., (2007) ?\n",
    "    '207/204': 72.47686, # Aleinikoff et al., (2007) ?\n",
    "    '208/204': 175.38924, # Aleinikoff et al., (2007) ?     \n",
    "    \n",
    "    \n",
    "}\n",
    "published_ratios_OLT_1 = {\n",
    "    '206/238': 0.170681666666667,\n",
    "    '208/232': 0.0516589274888366,\n",
    "    '207/206': 0.0731801314551979,\n",
    "    '207/235': (0.170681666666667) * (0.0731801314551979) * U85r, #Calculated\n",
    "    '208/206': 0.58025,\n",
    "    '206/204': 1847.16667,\n",
    "    '207/204': 135.17590,\n",
    "    '208/204': 1071.81688,      \n",
    "}\n",
    "published_ratios_OLT_2 = {\n",
    "    '206/238': 0.16693,\n",
    "    '208/232': 0.05064,\n",
    "    '207/206': 0.07248,\n",
    "    '207/235': (0.170681666666667) * (0.0731801314551979) * U85r, #Calculated\n",
    "    '208/206': 0.63815,\n",
    "    '206/204': 1453.00000,\n",
    "    '207/204': 105.31206,\n",
    "    '208/204': 927.22713,      \n",
    "}\n",
    "published_ratios_TCB = {\n",
    "    '206/238': 0.17086,\n",
    "    '208/232': 0.05178,\n",
    "    '207/206': 0.07327,\n",
    "    '207/235': (0.17086) * (0.07327) * U85r, #Calculated\n",
    "    '208/206': 0.63348,\n",
    "    '206/204': 2016.75000,\n",
    "    '207/204': 147.76034,\n",
    "    '208/204': 1277.56162,      \n",
    "}\n",
    "published_ratios_MudTank = {\n",
    "    '206/238': 0.052269449,\n",
    "    '208/232': 0.049381079,\n",
    "    '207/206': 0.071923689,\n",
    "    '207/235': (0.052269449) * (0.071923689) * U85r, #Calculated\n",
    "    '208/206': 0.068164043,\n",
    "    '206/204': 766.0145618,\n",
    "    '207/204': 55.09459,\n",
    "    '208/204': 52.21465,      \n",
    "                                \n",
    "}\n",
    "                                \n",
    "###Needed to add stuff for apatite\n",
    "#WRONG DO NOT USE. These are not correct at all.\n",
    "published_ratios_MT_apa = {\n",
    "    '206/238': 0.052269449,\n",
    "    '208/232': 0.049381079,\n",
    "    '207/206': 0.071923689,\n",
    "    '208/206': 0.068164043,\n",
    "    '206/204': 766.0145618,\n",
    "    '207/204': 55.09459,\n",
    "    '208/204': 52.21465,      \n",
    "}\n",
    "published_ratios_MT_apa = {\n",
    "    '206/238': 0.052269449,\n",
    "    '208/232': 0.049381079,\n",
    "    '207/206': 0.071923689,\n",
    "    '208/206': 0.068164043,\n",
    "    '206/204': 766.0145618,\n",
    "    '207/204': 55.09459,\n",
    "    '208/204': 52.21465,      \n",
    "}\n",
    "\n",
    "published_ratios_dict = {\n",
    "    'SRM NIST 610': published_ratios_610,\n",
    "    'SRM NIST 612': published_ratios_612,\n",
    "    'BHVO-2G': published_ratios_BHVO,\n",
    "    'BCR-2G': published_ratios_BCR_2G,\n",
    "    'GSD-1G': published_ratios_GSD_1G,\n",
    "    'GSE-1G': published_ratios_GSE_1G,\n",
    "    'Bear Lake': published_ratios_BLR_1,\n",
    "    'MKED-1': published_ratios_MKED,\n",
    "    'Mud Tank': published_ratios_MudTank,\n",
    "    'OLT-1': published_ratios_OLT_1,\n",
    "    'OLT-2': published_ratios_OLT_2,\n",
    "    'TCB': published_ratios_TCB,\n",
    "    'BLR-1': published_ratios_BLR_1b\n",
    "}\n",
    "\n",
    "published_df = pd.DataFrame(published_ratios_dict)\n",
    "published_df = pd.DataFrame.transpose(published_df)\n",
    "\n",
    "def published(std_name, ratio):\n",
    "    '''Returns the published ratio for a given reference material from the published values dataframe. Be sure that these are the correct values if you are using this function.'''\n",
    "    return published_df.loc[std_name][ratio]\n",
    "\n",
    "def published_full(std_name):\n",
    "    '''Returns dictionary of published ratios for the input reference material. Be sure that these are the correct values if you are using this function.'''\n",
    "    return published_ratios_dict[std_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     7,
     12,
     17,
     22,
     33,
     38,
     44
    ]
   },
   "outputs": [],
   "source": [
    "#Functions for Age Calculation: calc_t75(), calc_t68(), funct_t76(), calc_t76(), calc_t82(), calc_t76_2()\n",
    "\n",
    "##### Credit should be given to: https://github.com/anoda/UPbplot.py/blob/master/UPbplot.py code #####\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Age calculation for isotopic ratios (no correction)\n",
    "\n",
    "def calc_t75(r):\n",
    "    \"\"\" Calculation for 207Pb/235U age.\"\"\"\n",
    "    t = 1 / l235U * np.log(r + 1)\n",
    "    return t\n",
    "\n",
    "def calc_t68(r):         #PASSED\n",
    "    \"\"\" Calculation for 207Pb/235U age.\"\"\"\n",
    "    t = 1 / l238U * np.log(r + 1)\n",
    "    return t\n",
    "\n",
    "def func_t76(t, r):\n",
    "    \"\"\" Function for calc_t76.\"\"\"\n",
    "    res = abs(U85r * r - (np.exp(l235U * t) - 1) / (np.exp(l238U * t) - 1))\n",
    "    return res\n",
    "\n",
    "def calc_t76(r76):\n",
    "    '''This takes a list in as input. -KD'''\n",
    "    T = []\n",
    "    for i, r in enumerate(r76):\n",
    "        t = 1 / l238U * np.log(r + 1)  # initial time for calculation\n",
    "        T76 = optimize.leastsq(func_t76, t, args=(r))[0][0]\n",
    "        T.append(T76)\n",
    "    return T\n",
    "\n",
    "### KD modification###\n",
    "\n",
    "def calc_t82(r):         \n",
    "    \"\"\" Calculation for 208Pb/232U age. -KD\"\"\"\n",
    "    t = 1 / l232Th * np.log(r + 1)\n",
    "    return t\n",
    "\n",
    "def calc_t76_2(r):\n",
    "    '''Calculation of 207/206 Age from single ratio (float) inout. -KD'''\n",
    "    t = 1 / l238U * np.log(r + 1)  # initial time for calculation\n",
    "    T76 = optimize.leastsq(func_t76, t, args=(r))[0][0]\n",
    "    return T76\n",
    "\n",
    "def calc_concordant_ratios(mill_age):\n",
    "    ''' Use the decay equations for U238, U235, and Th232 to calculate radiogenic (*) ratios for a given age [Ma].'''\n",
    "    \n",
    "    ratio_dict = {}\n",
    "    age = mill_age * 1000000\n",
    "\n",
    "    ratio_dict['207*/206*'] = abs ((1/U85r) * (np.exp(l235U * age) - 1) / (np.exp(l238U * age) - 1))\n",
    "    ratio_dict['206*/238'] = np.exp(l238U * age) - 1\n",
    "    ratio_dict['207*/235'] = np.exp(l235U * age) - 1\n",
    "    ratio_dict['208*/232'] = np.exp(l232Th * age) - 1\n",
    "    ratio_dict['238/206*'] = 1 / ratio_dict['206*/238']\n",
    "    \n",
    "    return ratio_dict\n",
    "\n",
    "\n",
    "# IN PROGRESS 04Nov2021\n",
    "# def funct_t86(t, r86, r68, r82):\n",
    "#      \"\"\" Function for calc_t86.\"\"\"\n",
    "#     res = abs((r68*r86**1/r82) * r86 - (np.exp(l232Th * t) - 1) / (np.exp(l238U * t) - 1))\n",
    "#     return res\n",
    "\n",
    "# def calc_t86(r):\n",
    "#     '''Calculation of 208/206 Age from single ratio (float) inout. -KD'''\n",
    "#     t = 1 / l232Th * np.log(r + 1)  # initial time for calculation\n",
    "#     T86 = optimize.leastsq(func_t86, t, args=(r))[0][0]\n",
    "#     return T86\n",
    "\n",
    "    \n",
    "\n",
    "### END KD mod####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     8
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Functions for rounding significant figures and (unfinished) Stacey-Kramers corrections\n",
    "   \n",
    "def round_sig(x, sig=5):\n",
    "    '''Rounds value to specified number of significant figures.'''\n",
    "    return round(x, sig-int(floor(log10(abs(x))))-1)\n",
    "\n",
    "#Stacey-Kramer\n",
    "\n",
    "def SK_values(mill_age):\n",
    "    '''Input age [Ma] younger than 3.7 ga and get the Stacey-Kramer values.'''\n",
    "   \n",
    "    age = mill_age * 1000000\n",
    "\n",
    "    SK_dict = {}\n",
    "\n",
    "    SK_dict['SK_206_204'] = round_sig(11.152 + 9.7357*( np.exp(l238U * 3700000000)- np.exp(l238U * age)))\n",
    "\n",
    "    SK_dict['SK_207_204'] = round_sig(12.998 + (9.7357/137.88)* (np.exp(l235U * 3700000000)- np.exp(l235U * age)))\n",
    "\n",
    "    SK_dict['SK_208_204'] = round_sig(31.23 + 36.84 * (np.exp(l232Th * 3700000000) - np.exp(l232Th * age)))\n",
    "\n",
    "    SK_dict['SK_207_206'] = round_sig(SK_dict['SK_207_204'] / SK_dict['SK_206_204'])\n",
    "\n",
    "    SK_dict['SK_208_206'] = round_sig(SK_dict['SK_208_204'] / SK_dict['SK_206_204'])\n",
    "\n",
    "\n",
    "    return SK_dict\n",
    "\n",
    "# ex./ SK_values(960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     18
    ]
   },
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "# measured_76 = 0.0863356  # Measured 207/206\n",
    "\n",
    "# initial_76 = 0.9161 # Initial 207/206; Stacey-Kramers, intercept, or independent...\n",
    "# expected_76 = 0.0756 # The (radiogenic?) expected 207/206\n",
    "\n",
    "# measured_68 = 0.183344 # Measured 206/238\n",
    "# measured_82 = 5  # Measured 208/232\n",
    "# measured_86 = 5  # Measured 208/206\n",
    "# measured_75 = measured_68 * measured_76 * U85r  #'Measured' 207/235\n",
    "\n",
    "# ratio_238_to_232 = 5 # Measured 238/232\n",
    "\n",
    "# initial_86 = 1 # Initial 208/206; Stacey-Kramers, intercept, or independent...\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "def correction_207Pb(measured_68, measured_76,measured_82, measured_86, iterations = 5):\n",
    "    '''Performs 5 iterations of 207-Pb correction using Stacey-Kramers.'''\n",
    "    correction_dict = {}\n",
    "    correction_dict['206/238_uncorr'] = measured_68\n",
    "    for idx in range (iterations):\n",
    "        if idx == 0:\n",
    "            correction_dict['206/238_Age'] = calc_t68(correction_dict['206/238_uncorr']) / 1E6\n",
    "        else:\n",
    "            correction_dict['206/238_Age'] = calc_t68(correction_dict['206/238_corr']) / 1E6\n",
    "            \n",
    "        #correction_dict['206/238_Age'] = calc_t68(correction_dict['206/238_uncorr']) / 1E6\n",
    "        correction_dict['207/206_uncorr'] = measured_76\n",
    "        correction_dict['207/206_initial_SK'] = SK_values(correction_dict['206/238_Age'])['SK_207_206']\n",
    "        correction_dict['207/206_rad_expected'] = calc_concordant_ratios(correction_dict['206/238_Age'])['207*/206*']\n",
    "        correction_dict['f_206'] = (correction_dict['207/206_uncorr'] - correction_dict['207/206_rad_expected']) / (correction_dict['207/206_initial_SK'] - correction_dict['207/206_rad_expected'])\n",
    "        correction_dict['206/238_corr'] = (1 - correction_dict['f_206']) * measured_68\n",
    "      \n",
    "    \n",
    "    correction_dict['206/238_Age'] = calc_t68(correction_dict['206/238_corr']) / 1E6\n",
    "    \n",
    "    correction_dict['207/206_corr'] = (measured_76 - initial_76 * correction_dict['f_206']) / (1 - correction_dict['f_206']) \n",
    "    correction_dict['208/206_corr'] = (measured_86 - initial_86 * correction_dict['f_206']) / (1 - correction_dict['f_206']) \n",
    "    correction_dict['208/232_corr'] =  measured_82 - (measured_68 * initial_86 * ratio_238_to_232 * correction_dict['f_206'])\n",
    "    correction_dict['207/235_corr'] = measured_75 - (measured_68 * U85r * initial_76 *  correction_dict['f_206'])\n",
    "       \n",
    "    return correction_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     1,
     33,
     57
    ]
   },
   "outputs": [],
   "source": [
    "### Hard-code function for grouping samples by sampleID, and order dataframe based on chronologic order: group_samples(), comb_groups(), chronologic()\n",
    "def group_samples(data):\n",
    "    '''Function for grouping up samples by Sample ID. Hard-coded for names with \"1.n\" in title.'''\n",
    "    samples = data.index.values.tolist()\n",
    "    group = None\n",
    "    group_names = []\n",
    "    start = [0]\n",
    "    end = []\n",
    "    for idx in range(len(samples)):\n",
    "        if group == None:\n",
    "            group = samples[idx].split(' 1')[0]     #Hard-coding\n",
    "            group_names.append(group)\n",
    "        #print(samples[idx])\n",
    "        if group in samples[idx]:\n",
    "            pass\n",
    "        else:\n",
    "            end.append(idx)\n",
    "\n",
    "            start.append(idx)\n",
    "            name = samples[idx].split(' 1')[0]      #Hard-coding\n",
    "            group = name\n",
    "            group_names.append(name)\n",
    "    end.append(len(samples))\n",
    "\n",
    "#     print('start:', start)\n",
    "#     print('end:', end)\n",
    "#     print('group names:', group_names)\n",
    "    \n",
    "    start_index = start\n",
    "    end_index = end\n",
    "    \n",
    "    return start_index, end_index, group_names\n",
    "\n",
    "def comb_groups(group_list, data):\n",
    "    \"\"\" Function to combine a list of group names from dataset and output a list of rows to plot as well as the number of labels that will be necessary.\"\"\"\n",
    "    start, end, names = group_samples(data)\n",
    "    #test_group = ['Bancroft', 'BLR-1']\n",
    "\n",
    "    #len(test_group)\n",
    "    indx_list = []\n",
    "    for val in group_list:\n",
    "        #print(val)\n",
    "        indx_list.append(names.index(val))\n",
    "    #print(indx_list)\n",
    "    y_list = []\n",
    "    for ind in indx_list:\n",
    "        for i in range(end[ind]-start[ind]):\n",
    "            y1 = data.iloc[start[ind] + i]\n",
    "            y_list.append(y1)\n",
    "\n",
    "\n",
    "    df3 = pd.DataFrame(y_list)\n",
    "\n",
    "    labels = len(df3.index.tolist())\n",
    "\n",
    "    return y_list, labels\n",
    "\n",
    "def chronologic(result_df, order):\n",
    "    '''This function will reorder the rows of a dataframe by the index order that you designate.\n",
    "    In this case it is used to re-order dataframes by the Sample ID in chronologic order.'''\n",
    "    \n",
    "    new_result = result_df.copy(deep=True)\n",
    "    return new_result.reindex(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Functions for read_np2_timeseries(), calc_CPS(), and statistics_NP2()\n",
    "\n",
    "def read_np2_timeseries(excel_file):\n",
    "    ''' Excel input file is your baseline corrected time series export from Iolite for the NP-II.'''\n",
    "    df = pd.read_excel(excel_file, sheet_name = None)\n",
    "    keys = df.keys()\n",
    "    header_row = 0\n",
    "    new_dict = {}\n",
    "    for key in keys:\n",
    "        if 'time' in key: #Kind of hard-coded right now, so if names get weird may need to change\n",
    "            \n",
    "            df_test = df[key]\n",
    "\n",
    "            df_test.columns = df_test.iloc[header_row]\n",
    "            df_test = df_test.drop(header_row)\n",
    "            df_test = df_test.reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "            new_string = key.split('time')[0].rstrip()\n",
    "            new_dict[new_string] = df_test #test1_new\n",
    "    return new_dict\n",
    "\n",
    "def calc_CPS(np2_dict):\n",
    "    \"\"\" Function for calculating CPS from baseline-corrected signal, OPZ (On-Peak Zero), SNR (Signal-to-Noise Ratio), and isotope ratios.\"\"\"\n",
    "    columns = ['Absolute Time',\n",
    "     'Elapsed Time',\n",
    "     'm238_CPS',\n",
    "     'm232_CPS',\n",
    "     'm208_CPS',\n",
    "     'm207_CPS',\n",
    "     'm206_CPS',\n",
    "     'm204_CPS',\n",
    "     'm202_CPS']\n",
    "\n",
    "    new_col = ['Absolute Time',\n",
    "     'Elapsed Time',\n",
    "     '238_CPS',\n",
    "     '232_CPS',\n",
    "     '208_CPS',\n",
    "     '207_CPS',\n",
    "     '206_CPS',\n",
    "     '204_CPS',\n",
    "     '202_CPS']\n",
    "\n",
    "    cut_col = ['238_CPS',\n",
    "     '232_CPS',\n",
    "     '208_CPS',\n",
    "     '207_CPS',\n",
    "     '206_CPS',\n",
    "     '204_CPS',\n",
    "     '202_CPS']\n",
    "\n",
    "    calc_dict = {}\n",
    "    for key in np2_dict:\n",
    "        #print(key)\n",
    "        test_df1 = np2_dict[key]\n",
    "\n",
    "        for col in columns:\n",
    "                test_df2 = test_df1.apply(lambda x: x * 62500000 if 'CPS' in x.name else x)\n",
    "                test_df2 = test_df2[['Absolute Time',\n",
    "             'Elapsed Time',\n",
    "                 'm238_CPS',\n",
    "                 'm232_CPS',\n",
    "                 'm208_CPS',\n",
    "                 'm207_CPS',\n",
    "                 'm206_CPS',\n",
    "                 'm204_CPS',\n",
    "                 'm202_CPS',]]\n",
    "        test_df2.columns = new_col\n",
    "        test_df2 = test_df2[cut_col]\n",
    "        result = pd.concat([test_df1, test_df2], axis=1)\n",
    "        \n",
    "         #Calculating OPZ\n",
    "        result['OPZ_238'] = result.apply(lambda x: x['m238'] - x['m238_CPS'], axis=1)\n",
    "        result['OPZ_232'] = result.apply(lambda x: x['m232'] - x['m232_CPS'], axis=1)\n",
    "        result['OPZ_208'] = result.apply(lambda x: x['m208'] - x['m208_CPS'], axis=1)\n",
    "        result['OPZ_207'] = result.apply(lambda x: x['m207'] - x['m207_CPS'], axis=1)\n",
    "        result['OPZ_206'] = result.apply(lambda x: x['m206'] - x['m206_CPS'], axis=1)\n",
    "        result['OPZ_204'] = result.apply(lambda x: x['m204'] - x['m204_CPS'], axis=1)\n",
    "        result['OPZ_202'] = result.apply(lambda x: x['m202'] - x['m202_CPS'], axis=1)\n",
    "        result['OPZ_208/206'] = result.apply(lambda x: x['OPZ_208'] / x['OPZ_206'], axis=1)\n",
    "        result['OPZ_207/206'] = result.apply(lambda x: x['OPZ_207'] / x['OPZ_206'], axis=1)\n",
    "        result['OPZ_206/204_Hg-corrected'] = result.apply(lambda x: x['OPZ_206'] / (x['OPZ_204'] - (x['OPZ_202'] * 6.87/29.86)) , axis=1)\n",
    "       \n",
    "        #Calculating Signal-to-Noise Ratios [V]/[V]\n",
    "        result['SNR_238'] = result.apply(lambda x: x['m238_CPS']/ x['OPZ_238'], axis=1)\n",
    "        result['SNR_232'] = result.apply(lambda x: x['m232_CPS']/ x['OPZ_232'], axis=1)\n",
    "        result['SNR_208'] = result.apply(lambda x: x['m208_CPS']/ x['OPZ_208'], axis=1)\n",
    "        result['SNR_207'] = result.apply(lambda x: x['m207_CPS']/ x['OPZ_207'], axis=1)\n",
    "        result['SNR_206'] = result.apply(lambda x: x['m206_CPS']/ x['OPZ_206'], axis=1)\n",
    "        result['SNR_204'] = result.apply(lambda x: x['m204_CPS']/ x['OPZ_204'], axis=1)\n",
    "        result['SNR_202'] = result.apply(lambda x: x['m202_CPS']/ x['OPZ_202'], axis=1)\n",
    "        \n",
    "        #Calculating Ratios\n",
    "        result['206/238'] = result.apply(lambda x: x['206_CPS']/x['238_CPS'], axis=1)\n",
    "        result['208/232'] = result.apply(lambda x: x['208_CPS']/x['232_CPS'], axis=1)\n",
    "        result['207/206'] = result.apply(lambda x: x['207_CPS']/x['206_CPS'], axis=1)\n",
    "        result['208/206'] = result.apply(lambda x: x['208_CPS']/x['206_CPS'], axis=1)\n",
    "        result['206/204'] = result.apply(lambda x: x['206_CPS']/x['204_CPS'], axis=1)\n",
    "        result['208/204'] = result.apply(lambda x: x['208_CPS']/x['204_CPS'], axis=1)\n",
    "        result['207/204'] = result.apply(lambda x: x['207_CPS']/x['204_CPS'], axis=1)\n",
    "        ### KD added 10/18/2021\n",
    "        result['207/235'] = result.apply(lambda x: x['207/206'] * x['206/238'] * U85r, axis=1)\n",
    "        \n",
    "        #Calculating OPZ CPS\n",
    "        result['OPZ_238_CPS'] = result.apply(lambda x: x['OPZ_238'] * 62500000, axis=1)\n",
    "        result['OPZ_232_CPS'] = result.apply(lambda x: x['OPZ_232'] * 62500000, axis=1)\n",
    "        result['OPZ_208_CPS'] = result.apply(lambda x: x['OPZ_208'] * 62500000, axis=1)\n",
    "        result['OPZ_207_CPS'] = result.apply(lambda x: x['OPZ_207'] * 62500000, axis=1)\n",
    "        result['OPZ_206_CPS'] = result.apply(lambda x: x['OPZ_206'] * 62500000, axis=1)\n",
    "        result['OPZ_204_CPS'] = result.apply(lambda x: x['OPZ_204'] * 62500000, axis=1)\n",
    "        result['OPZ_202_CPS'] = result.apply(lambda x: x['OPZ_202'] * 62500000, axis=1)\n",
    "        \n",
    "        calc_dict[key] = result\n",
    "    \n",
    "    return calc_dict\n",
    "\n",
    "def statistics_NP2(calc_dict):\n",
    "    \"\"\"Function for calculating statistics of CPS and isotope ratios, prior to error minimization and correction.\"\"\"\n",
    "    \n",
    "    calc_list = ['238_CPS', '232_CPS',\n",
    "           '208_CPS', '207_CPS', '206_CPS', '204_CPS', '202_CPS', '206/238',\n",
    "           '208/232', '207/206', '208/206', '206/204','208/204','207/204', '207/235'  ]\n",
    "    mega_dict = {}\n",
    "\n",
    "    for sheet in calc_dict:\n",
    "        tester = calc_dict[sheet]\n",
    "        stats_dict = {}\n",
    "        for col in tester:\n",
    "\n",
    "            if col in calc_list:\n",
    "                #print(col)\n",
    "                if '/' in col:\n",
    "                    key = col + '_before rejection'\n",
    "                else:\n",
    "                    key = col + '_mean'\n",
    "                df_mean = tester[col].mean()\n",
    "                stats_dict[key] = df_mean\n",
    "                #This is not 2SE%, should probably fix labels... KD 14 June 2021\n",
    "                df_precision = (2 * tester[col].sem()) / df_mean * 100\n",
    "                \n",
    "                stats_dict[col + '_2se%'] = df_precision\n",
    "            if 'OPZ' in col:\n",
    "                 stats_dict[col + '_mean'] = tester[col].mean()\n",
    "            if 'SNR' in col:\n",
    "                 stats_dict[col + '_mean'] = tester[col].mean() \n",
    "                    \n",
    "        stats_dict['Time (s)'] = tester['Elapsed Time'].max()\n",
    "        \n",
    "        #new_string = sheet.replace('time series data', '')\n",
    "        new_string = sheet.split('time')[0].rstrip()\n",
    "        mega_dict[new_string] = stats_dict\n",
    "\n",
    "    df_1 = pd.DataFrame(mega_dict)\n",
    "    df_flip = pd.DataFrame.transpose(df_1)\n",
    "    return df_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     2,
     25,
     96
    ]
   },
   "outputs": [],
   "source": [
    "### Functions for group_samples(), regress_group(), regress_indiv(), and regress_compare()\n",
    "\n",
    "def group_samples2(tester):\n",
    "    '''Takes in the output from calc_CPS() and outputs a dictionary with individual analyses grouped by sample.'''\n",
    "    samples = tester.keys()\n",
    "    group_list = []\n",
    "    group_dict = {}\n",
    "    for sample in samples:\n",
    "        group = sample.split(' 1.')[0]\n",
    "\n",
    "        if len(group_list) == 0:\n",
    "            group_list.append(group)\n",
    "        elif group == group_list[-1]:\n",
    "            pass\n",
    "        else:\n",
    "            group_list.append(group)\n",
    "\n",
    "    for group in group_list:\n",
    "        group_dict[group] = []\n",
    "        for sample in samples:\n",
    "            if sample.split(' 1.')[0] == group:\n",
    "                group_dict[group].append(sample)\n",
    "\n",
    "    return group_dict \n",
    "\n",
    "def regress_group(tester, group_name,choice = False, start_clip = 0):\n",
    "    '''Does a linear regression for each sample and outputs a dictionary with mean, intercept, slope, coefficient of determination, and (intercept/average).'''\n",
    "    mked = group_samples2(tester)[group_name]\n",
    "\n",
    "    tester_list = []\n",
    "    for entry in mked:\n",
    "        tester_list.append(tester[entry])\n",
    "\n",
    "    mked_full = pd.concat(tester_list)\n",
    "    regress_data = mked_full[['Elapsed Time', '206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "\n",
    "    ### Regression Math ###\n",
    "    #start_clip = 1.0\n",
    "    mked_full = regress_data[regress_data['Elapsed Time'] > start_clip]\n",
    "\n",
    "    x = np.array(mked_full['Elapsed Time']).reshape((-1, 1))\n",
    "    y = mked_full['206/238']\n",
    "\n",
    "    model = LinearRegression().fit(x, y)\n",
    "\n",
    "    model_dict = {}\n",
    "    model_dict['average'] = y.mean()\n",
    "    model_dict['intercept'] = model.intercept_\n",
    "    model_dict['slope'] = float(model.coef_)\n",
    "\n",
    "    model_dict['coefficient of determination'] = model.score(x, y)\n",
    "    \n",
    "    new = model_dict['intercept']\n",
    "    model_dict['intercept / average'] = new/model_dict['average']\n",
    "   \n",
    "    if choice:\n",
    "        print('\\nRegression for', group_name)\n",
    "        print('intercept:', model.intercept_)\n",
    "        print('slope:', float(model.coef_))\n",
    "        print('coefficient of determination:', model.score(x, y))\n",
    "        print('intercept / average', model_dict['intercept / average'] )\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "def regress_indiv(tester, sample_name, choice = False, start_clip = 1, end_clip = 1):\n",
    "    '''Does a linear regression for each individual analysis and outputs a dictionary with mean, intercept, slope, coefficient of determination, and (intercept/average).'''\n",
    "    sample = tester[sample_name]\n",
    "    regress_data = sample[['Elapsed Time', '206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "    mked_full1 = regress_data[regress_data['Elapsed Time'] > start_clip]\n",
    "    mked_full = mked_full1[mked_full1['Elapsed Time'] > end_clip]\n",
    "    \n",
    "    x = np.array(mked_full['Elapsed Time']).reshape((-1, 1))\n",
    "    y = mked_full['206/238']\n",
    "\n",
    "    model = LinearRegression().fit(x, y)\n",
    "\n",
    "    model_dict = {}\n",
    "    model_dict['average'] = y.mean()\n",
    "    model_dict['intercept'] = model.intercept_\n",
    "    model_dict['slope'] = float(model.coef_)\n",
    "\n",
    "    model_dict['coefficient of determination'] = model.score(x, y)\n",
    "    \n",
    "    new = model_dict['intercept']\n",
    "    model_dict['intercept / average'] = new / model_dict['average']\n",
    "    \n",
    "    \n",
    "    if choice:\n",
    "        print('\\nRegression for', sample_name)\n",
    "        print('average:', model_dict['average'])\n",
    "        print('intercept:', new)\n",
    "        print('slope:', float(model.coef_))\n",
    "       #print('coefficient of determination:', model.score(x, y))\n",
    "       #print('intercept / average', new/orig)\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "def regress_compare(tester, std1, std2, clip1 = 0, clip2  = 0, endclip = 35, choice = False):    \n",
    "    '''This function takes in the output from calc_CPS(), and two selected sample groups, plus clip positions for start and end. It will plot things.'''\n",
    "    ###MKED###\n",
    "    #start_clip = 2\n",
    "    mked = group_samples2(tester)[std1]\n",
    "\n",
    "    tester_list = []\n",
    "    for entry in mked:\n",
    "        tester_list.append(tester[entry])\n",
    "\n",
    "    mked_full = pd.concat(tester_list)\n",
    "    regress_data = mked_full[['Elapsed Time', '206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "\n",
    "    x_orig = np.array(mked_full['Elapsed Time']).reshape((-1, 1))\n",
    "\n",
    "    ### Regression Math ###\n",
    "    #start_clip = 1.0\n",
    "    mked_full = regress_data[regress_data['Elapsed Time'] > clip1]\n",
    "\n",
    "    mked_full = mked_full[mked_full['Elapsed Time'] < endclip]\n",
    "    \n",
    "    x = np.array(mked_full['Elapsed Time']).reshape((-1, 1))\n",
    "    y = mked_full['206/238']\n",
    "    model = LinearRegression().fit(x, y)\n",
    "    model_dict = {}\n",
    "    model_dict['average'] = y.mean()\n",
    "    model_dict['intercept'] = model.intercept_\n",
    "    model_dict['slope'] = float(model.coef_)\n",
    "    model_dict['coefficient of determination'] = model.score(x, y)\n",
    "\n",
    "    fit = model.fit(x,y)\n",
    "    y_pred = model.predict(x_orig)\n",
    "\n",
    "    ### MKED ###\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, sharex = True, figsize = (24, 6))\n",
    "    ax[0].scatter(x,y, color=\"black\")\n",
    "    ax[0].plot(x_orig, y_pred, color='blue', linewidth = 3)\n",
    "    \n",
    "    # these are matplotlib.patch.Patch properties\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax[0].text(0.97, 0.97, std1, transform=ax[0].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    equation = 'y = ' + str(round_sig(model_dict['slope'], 3)) + 'x + ' + str(round_sig(model_dict['intercept']))\n",
    "    ax[0].text(0.97, 0.88, equation, transform=ax[0].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "\n",
    "    ### BLR-1 ###\n",
    "    #start_clip = 8\n",
    "    blr = group_samples2(tester)[std2]\n",
    "\n",
    "    tester_list = []\n",
    "    for entry in blr:\n",
    "        tester_list.append(tester[entry])\n",
    "\n",
    "    blr_full = pd.concat(tester_list)\n",
    "    regress_data = blr_full[['Elapsed Time', '206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "\n",
    "\n",
    "    x_orig2 = np.array(blr_full['Elapsed Time']).reshape((-1, 1))\n",
    "\n",
    "    ### Regression Math ###\n",
    "    #start_clip = 1.0\n",
    "    blr_full = regress_data[regress_data['Elapsed Time'] > clip2]\n",
    "    blr_full = blr_full[blr_full['Elapsed Time'] < endclip]\n",
    "    x2 = np.array(blr_full['Elapsed Time']).reshape((-1, 1))\n",
    "    y2 = blr_full['206/238']\n",
    "    model = LinearRegression().fit(x2, y2)\n",
    "\n",
    "    model_dict2 = {}\n",
    "    model_dict2['average'] = y2.mean()\n",
    "    model_dict2['intercept'] = model.intercept_\n",
    "    model_dict2['slope'] = float(model.coef_)\n",
    "    model_dict2['coefficient of determination'] = model.score(x, y)\n",
    "\n",
    "\n",
    "    fit2 = model.fit(x2,y2)\n",
    "    y_pred2 = model.predict(x_orig2)\n",
    "\n",
    "    ### BLR-1 ###\n",
    "    ax[1].scatter(x2,y2, color=\"black\")\n",
    "    ax[1].plot(x_orig2, y_pred2, color='blue', linewidth = 3)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax[1].text(0.97, 0.97, std2, transform=ax[1].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    equation2 = 'y = ' + str(round_sig(model_dict2['slope'], 3)) + 'x + ' + str(round_sig(model_dict2['intercept']))\n",
    "    ax[1].text(0.97, 0.88, equation2, transform=ax[1].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    \n",
    "    y3 = y_pred2 / (y_pred[:len(y_pred2)])\n",
    "    y4 = model_dict2['intercept'] / (y_pred[:len(y_pred2)])\n",
    "\n",
    "    ax[2].plot(y3[0:50], color='green', linewidth = 3)\n",
    "    ax[2].plot(y4[0:50], color='blue', linewidth = 3)\n",
    "    \n",
    "    \n",
    "#     #Still in Development\n",
    "#     ax[0].spines['left'].set_position(('data', 0))\n",
    "#     ax[1].spines['left'].set_position(('data', 0))\n",
    "#     ax[2].spines['left'].set_position(('data', 0))\n",
    "    \n",
    "    ###\n",
    "    #print(equation)\n",
    "    #print(model_dict)\n",
    "    #print(model_dict2)\n",
    "    print('int2/int1: ',model_dict2['intercept']/model_dict['intercept'])\n",
    "    print('int2/avg1 ', model_dict2['intercept']/model_dict['average'])\n",
    "    print(std1, ' chopped at: ',clip1, '   ', model_dict2['intercept']/y_pred[clip1])\n",
    "    print()\n",
    "    if choice:\n",
    "        return model_dict, model_dict2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Functions for error minimization (Could use some revision...): ranked_minimization2() and statistics_ranktest2()\n",
    "\n",
    "### Getting rid of negatives\n",
    "\n",
    "def ranked_minimization2(sheet, ratio, reject_percentage = 20):\n",
    "    ''' Function for excluding negative isotope ratios and then performing a ranked error minimization by excluding a specified percentage of points that lie outside of 1SD of mean. Excludeds negative values BEFORE error minimization.'''\n",
    "    \n",
    "    mytest = tester[sheet].copy(deep=True)\n",
    "\n",
    "    df_mean_before = mytest[ratio].mean()\n",
    "    df_1std_before = mytest[ratio].std()\n",
    "    df_count_before = mytest[ratio].count()\n",
    "    df_2se_perc_before = (2 * mytest[ratio].sem()) / df_mean_before * 100\n",
    "\n",
    "    dif_mean = ratio + '_dif_from_mean'\n",
    "    dif_1SD = ratio + '_dif_from_1SD'\n",
    "    mytest[dif_mean] = mytest.apply(lambda x: abs(x[ratio] - df_mean_before), axis=1)\n",
    "    mytest[dif_1SD] = mytest.apply(lambda x: x[dif_mean] - df_1std_before, axis=1)\n",
    "\n",
    "    mytest_noNeg = mytest[mytest[ratio] > 0]\n",
    "    \n",
    "    mytest2 = mytest_noNeg.sort_values(by = dif_1SD, ascending = False)\n",
    "    #mytest2.head()\n",
    "\n",
    "    ratios_to_reject = int(mytest_noNeg[ratio].count() * reject_percentage / 100)\n",
    "    #print(ratios_to_reject)\n",
    "\n",
    "    after_rejection = mytest2[ratios_to_reject:]\n",
    "    \n",
    "    \n",
    "\n",
    "    df_mean_after = after_rejection[ratio].mean()\n",
    "    df_1std_after = after_rejection[ratio].std()\n",
    "    df_count_after = after_rejection[ratio].count()\n",
    "   \n",
    "    df_2se_perc_after = (2 * after_rejection[ratio].sem()) / df_mean_after * 100\n",
    "    df_2sd_perc_after = (2 * after_rejection[ratio].std()) / df_mean_after * 100\n",
    "    \n",
    "\n",
    "    # print(df_mean_after)\n",
    "    # print(df_1std_after)\n",
    "    # print(df_2se_perc_after)\n",
    "\n",
    "    results_dict = {}\n",
    "    \n",
    "    results_dict['avg_before'] = df_mean_before\n",
    "    results_dict['1sd_before'] = df_1std_before\n",
    "    results_dict['2se%_before'] = df_2se_perc_before\n",
    "    results_dict['avg_after'] = df_mean_after\n",
    "    results_dict['1sd_after'] = df_1std_after\n",
    "    results_dict['2se%_after'] = df_2se_perc_after\n",
    "    results_dict['2σ%_after'] = df_2sd_perc_after\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "def statistics_ranktest2(calc_dict, reject_percentage = 20):\n",
    "    '''Calculates statistics while Incorporating the ranked error minimization.'''\n",
    "    calc_list = ['238_CPS', '232_CPS',\n",
    "           '208_CPS', '207_CPS', '206_CPS', '204_CPS', '202_CPS', '206/238',\n",
    "           '208/232', '207/206', '208/206', '206/204','208/204','207/204', '207/235'  ]\n",
    "    mega_dict = {}\n",
    "\n",
    "    for sheet in calc_dict:\n",
    "        tester = calc_dict[sheet]\n",
    "        stats_dict = {}\n",
    "        \n",
    "        \n",
    "        ###Regression tests###\n",
    "        \n",
    "        regress_dict = regress_indiv(calc_dict, sheet, False, 1, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ###End Regression tests###\n",
    "        \n",
    "        for col in tester:\n",
    "\n",
    "            if col in calc_list:\n",
    "                #print(col)\n",
    "                if '/' in col:\n",
    "                    key_bf = col + '_before rejection'\n",
    "                    key_af = col + '_after rejection'\n",
    "                    key_bf_se = col + '_before rejection 2se%'\n",
    "                    key_af_se = col + '_after rejection 2se%'\n",
    "                    key_af_2sd = col + '_after rejection 2σ%'\n",
    "                    \n",
    "                    ranked_dict = ranked_minimization2(sheet, col, reject_percentage)\n",
    "                    \n",
    "                    stats_dict[key_bf] = ranked_dict['avg_before']\n",
    "                    stats_dict[key_bf_se] = ranked_dict['2se%_before']\n",
    "                    stats_dict[key_af] = ranked_dict['avg_after']\n",
    "                    \n",
    "                    stats_dict[key_af_se] = ranked_dict['2se%_after']\n",
    "                    stats_dict[key_af_2sd] = ranked_dict['2σ%_after']\n",
    "                    \n",
    "                    ###Regression tests### \n",
    "                    if col == '206/238':\n",
    "                        key_int = col + '_intercept'\n",
    "                        stats_dict[key_int] = regress_dict['intercept']\n",
    "\n",
    "                    \n",
    "                    ###End Regression tests###\n",
    "                    \n",
    "                else:\n",
    "                    key = col + '_mean'\n",
    "                    df_mean = tester[col].mean()\n",
    "                    stats_dict[key] = df_mean\n",
    "                    #This is not 2SE%, should probably fix labels... KD 14 June 2021\n",
    "                    df_precision = (2 * tester[col].std()) / df_mean * 100\n",
    "                    stats_dict[col + '_2σ%'] = df_precision\n",
    "            if 'OPZ' in col:\n",
    "                 stats_dict[col + '_mean'] = tester[col].mean()\n",
    "            if 'SNR' in col:\n",
    "                 stats_dict[col + '_mean'] = tester[col].mean()\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "        stats_dict['Time (s)'] = tester['Elapsed Time'].max()\n",
    "        \n",
    "        #new_string = sheet.replace('time series data', '')\n",
    "        new_string = sheet.split('time')[0].rstrip()\n",
    "        mega_dict[new_string] = stats_dict\n",
    "\n",
    "    df_1 = pd.DataFrame(mega_dict)\n",
    "    df_flip = pd.DataFrame.transpose(df_1)\n",
    "   \n",
    "    return df_flip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     2,
     41,
     123
    ]
   },
   "outputs": [],
   "source": [
    "### Functions for fractionation output .xlsx: frac_np2_timeseries(), frac_calc_CPS(), fractionation_output()\n",
    "\n",
    "def frac_np2_timeseries(excel_file):\n",
    "    ''' Excel input file is your baseline corrected time series export from Iolite for the NP-II.'''\n",
    "    df = pd.read_excel(excel_file, sheet_name = None)\n",
    "    keys = df.keys()\n",
    "    header_row = 0\n",
    "    new_dict = {}\n",
    "    frac_dict = []\n",
    "    for key in keys:\n",
    "        if 'time' in key: #Kind of hard-coded right now, so if names get weird may need to change\n",
    "            \n",
    "            df_test = df[key]\n",
    "\n",
    "            df_test.columns = df_test.iloc[header_row]\n",
    "            df_test = df_test.drop(header_row)\n",
    "            df_test = df_test.reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "            new_string = key.split('time')[0].rstrip()\n",
    "            \n",
    "            #Adding column with SAMPLE ID\n",
    "            cols = list(df_test.columns.values)\n",
    "            df_test['SAMPLE ID'] = new_string\n",
    "            new_cols = ['SAMPLE ID'] + cols\n",
    "            df_test = df_test[new_cols]\n",
    "            \n",
    "            new_dict[new_string] = df_test #test1_new\n",
    "            frac_dict.append(new_string)\n",
    "\n",
    "    combo = new_dict[frac_dict[0]]\n",
    "    for idx in enumerate(frac_dict):\n",
    "        #print(idx[0])\n",
    "        if idx[0] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            combo = pd.concat([combo, new_dict[idx[1]]])\n",
    "    #print(frac_dict)\n",
    "    #print(combo)\n",
    "    return new_dict, combo\n",
    "\n",
    "def frac_calc_CPS(combo):\n",
    "    columns = ['Absolute Time',\n",
    "     'Elapsed Time',\n",
    "     'm238_CPS',\n",
    "     'm232_CPS',\n",
    "     'm208_CPS',\n",
    "     'm207_CPS',\n",
    "     'm206_CPS',\n",
    "     'm204_CPS',\n",
    "     'm202_CPS']\n",
    "\n",
    "    new_col = ['Absolute Time',\n",
    "     'Elapsed Time',\n",
    "     '238_CPS',\n",
    "     '232_CPS',\n",
    "     '208_CPS',\n",
    "     '207_CPS',\n",
    "     '206_CPS',\n",
    "     '204_CPS',\n",
    "     '202_CPS']\n",
    "\n",
    "    cut_col = ['238_CPS',\n",
    "     '232_CPS',\n",
    "     '208_CPS',\n",
    "     '207_CPS',\n",
    "     '206_CPS',\n",
    "     '204_CPS',\n",
    "     '202_CPS']\n",
    "\n",
    "#     calc_dict = {}\n",
    "#     for key in np2_dict:\n",
    "#         #print(key)\n",
    "#         test_df1 = np2_dict[key]\n",
    "    test_df1 = combo\n",
    "    for col in columns:\n",
    "            test_df2 = test_df1.apply(lambda x: x * 62500000 if 'CPS' in x.name else x)\n",
    "            test_df2 = test_df2[['Absolute Time',\n",
    "         'Elapsed Time',\n",
    "             'm238_CPS',\n",
    "             'm232_CPS',\n",
    "             'm208_CPS',\n",
    "             'm207_CPS',\n",
    "             'm206_CPS',\n",
    "             'm204_CPS',\n",
    "             'm202_CPS',]]\n",
    "    test_df2.columns = new_col\n",
    "    test_df2 = test_df2[cut_col]\n",
    "    result = pd.concat([test_df1, test_df2], axis=1)\n",
    "\n",
    "     #Calculating OPZ\n",
    "    result['OPZ_238'] = result.apply(lambda x: x['m238'] - x['m238_CPS'], axis=1)\n",
    "    result['OPZ_232'] = result.apply(lambda x: x['m232'] - x['m232_CPS'], axis=1)\n",
    "    result['OPZ_208'] = result.apply(lambda x: x['m208'] - x['m208_CPS'], axis=1)\n",
    "    result['OPZ_207'] = result.apply(lambda x: x['m207'] - x['m207_CPS'], axis=1)\n",
    "    result['OPZ_206'] = result.apply(lambda x: x['m206'] - x['m206_CPS'], axis=1)\n",
    "    result['OPZ_204'] = result.apply(lambda x: x['m204'] - x['m204_CPS'], axis=1)\n",
    "    result['OPZ_202'] = result.apply(lambda x: x['m202'] - x['m202_CPS'], axis=1)\n",
    "    result['OPZ_208/206'] = result.apply(lambda x: x['OPZ_208'] / x['OPZ_206'], axis=1)\n",
    "    result['OPZ_207/206'] = result.apply(lambda x: x['OPZ_207'] / x['OPZ_206'], axis=1)\n",
    "    result['OPZ_206/204_Hg-corrected'] = result.apply(lambda x: x['OPZ_206'] / (x['OPZ_204'] - (x['OPZ_202'] * 6.87/29.86)) , axis=1)\n",
    "\n",
    "    #Calculating Signal-to-Noise Ratios [V]/[V]\n",
    "    result['SNR_238'] = result.apply(lambda x: x['m238_CPS']/ x['OPZ_238'], axis=1)\n",
    "    result['SNR_232'] = result.apply(lambda x: x['m232_CPS']/ x['OPZ_232'], axis=1)\n",
    "    result['SNR_208'] = result.apply(lambda x: x['m208_CPS']/ x['OPZ_208'], axis=1)\n",
    "    result['SNR_207'] = result.apply(lambda x: x['m207_CPS']/ x['OPZ_207'], axis=1)\n",
    "    result['SNR_206'] = result.apply(lambda x: x['m206_CPS']/ x['OPZ_206'], axis=1)\n",
    "    result['SNR_204'] = result.apply(lambda x: x['m204_CPS']/ x['OPZ_204'], axis=1)\n",
    "    result['SNR_202'] = result.apply(lambda x: x['m202_CPS']/ x['OPZ_202'], axis=1)\n",
    "\n",
    "    #Calculating Ratios\n",
    "    result['206/238'] = result.apply(lambda x: x['206_CPS']/x['238_CPS'], axis=1)\n",
    "    result['208/232'] = result.apply(lambda x: x['208_CPS']/x['232_CPS'], axis=1)\n",
    "    result['207/206'] = result.apply(lambda x: x['207_CPS']/x['206_CPS'], axis=1)\n",
    "    result['208/206'] = result.apply(lambda x: x['208_CPS']/x['206_CPS'], axis=1)\n",
    "    result['206/204'] = result.apply(lambda x: x['206_CPS']/x['204_CPS'], axis=1)\n",
    "    result['208/204'] = result.apply(lambda x: x['208_CPS']/x['204_CPS'], axis=1)\n",
    "    result['207/204'] = result.apply(lambda x: x['207_CPS']/x['204_CPS'], axis=1)\n",
    "\n",
    "    \n",
    "    return result\n",
    "\n",
    "def fractionation_output(excel_file):\n",
    "    frac_tester, frac_combo = frac_np2_timeseries(excel_file)\n",
    "    combo_tester = frac_calc_CPS(frac_combo)\n",
    "    \n",
    "    new_filename = str(excel_file.split('.')[0]) + '_fractionation.xlsx'   \n",
    "    with pd.ExcelWriter(new_filename) as writer:\n",
    "        combo_tester.to_excel(writer, sheet_name = 'Fractionation_ALL', index = False)\n",
    "       \n",
    "        worksheet = writer.sheets['Fractionation_ALL']  # pull worksheet object\n",
    "        worksheet.set_column(0, 0, 24)\n",
    "        worksheet.freeze_panes(1,1)   \n",
    "        \n",
    "    #return combo_tester\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     2,
     22
    ]
   },
   "outputs": [],
   "source": [
    "### Functions for exporting the time-series processed data to Excel: files_process_toEXCEL(), file_process_combine()\n",
    "\n",
    "def files_process_toEXCEL(calc_dict, excel_name, order):\n",
    "    ''' Takes the dataframe from calcCPS() and outputs the time-series processed .xlsx with group statistics in group order and chronologic order. \n",
    "    If chronologic order is not updated, it might break...'''\n",
    "    \n",
    "    with pd.ExcelWriter(excel_name) as writer:\n",
    "        for sheet in calc_dict:\n",
    "            calc_dict[sheet].to_excel(writer, sheet_name = sheet, index = False)\n",
    "        \n",
    "        statistics_NP2(calc_dict).to_excel(writer, sheet_name = 'Statistics', index = True)\n",
    "        \n",
    "        chronologic(statistics_NP2(calc_dict), order).to_excel(writer, sheet_name = 'Chrono_Statistics', index = True)\n",
    "        \n",
    "        worksheet = writer.sheets['Statistics']  # pull worksheet object\n",
    "        worksheet.set_column(0, 0, 24)\n",
    "        worksheet.freeze_panes(1,1)\n",
    "        \n",
    "        worksheet = writer.sheets['Chrono_Statistics']  # pull worksheet object\n",
    "        worksheet.set_column(0, 0, 24)\n",
    "        worksheet.freeze_panes(1,1)\n",
    "        \n",
    "def file_process_combine(filename):\n",
    "    ''' CURRENTLY NOT IN USE. This function will take the filename and output the time-series data processed in .xlsx format. Useful for quick data check.'''\n",
    "    calc_dict = calc_CPS(read_np2_timeseries(filename))\n",
    "    new_filename = str(filename.split('.')[0]) + '_processed.xlsx'\n",
    "    files_process_toEXCEL(calc_dict, new_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     1,
     85
    ]
   },
   "outputs": [],
   "source": [
    "### Functons for graphing and report generation: U_Pb_plots(), U_Pb_report()\n",
    "def U_Pb_plots(calc_dict, sample, choice = True):\n",
    "    ''' Makes plots for CPS and isotope ratios plotted against time. The input is the dataframe from calc_CPS() and a sample.'''\n",
    "    key_list = ['238_CPS', '232_CPS',\n",
    "       '208_CPS', '207_CPS', '206_CPS', '204_CPS', '202_CPS', '206/238',\n",
    "       '208/232', '207/206', '208/206', '206/204']\n",
    "    \n",
    "    zet = calc_dict[sample]\n",
    "    new_string = sample.split('time')[0].rstrip()\n",
    "    y_list = []\n",
    "    for key in key_list:\n",
    "        y_list.append(zet[key])\n",
    "    \n",
    "    x = zet['Elapsed Time']\n",
    "    \n",
    "    #mod 12 September 2021\n",
    "    y_238over232 = zet['238_CPS'] / zet['232_CPS']\n",
    "    new_list = ['238_CPS', '232_CPS',\n",
    "       '208_CPS', '207_CPS', '206_CPS', '204_CPS', '238/232', '206/238',\n",
    "       '208/232', '207/206', '208/206', '206/204']\n",
    "    \n",
    "    y_list[6] = y_238over232\n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(4, 3, sharex = True, figsize = (12, 12))\n",
    "    fig.suptitle(new_string, fontsize=24)\n",
    "    \n",
    "    ax_list = [\n",
    "        axs[0, 0], \n",
    "        axs[0, 1],   \n",
    "        axs[0, 2], \n",
    "        axs[1, 0], \n",
    "        axs[1, 1],\n",
    "        axs[1, 2],\n",
    "        axs[2, 0], \n",
    "        axs[2, 1], \n",
    "        axs[2, 2],\n",
    "        axs[3, 0], \n",
    "        axs[3, 1], \n",
    "        axs[3, 2]   \n",
    "        ]\n",
    "\n",
    "    axs[0, 0].plot(x, y_list[0])\n",
    "    axs[0, 1].plot(x, y_list[1])\n",
    "    axs[0, 2].plot(x, y_list[2])\n",
    "    axs[1, 0].plot(x, y_list[3])\n",
    "    axs[1, 1].plot(x, y_list[4])\n",
    "    axs[1, 2].plot(x, y_list[5])\n",
    "    axs[2, 0].plot(x,  y_list[6])\n",
    "    axs[2, 1].plot(x, y_list[7])\n",
    "    axs[2, 2].plot(x, y_list[8])\n",
    "    axs[3, 0].plot(x, y_list[9])\n",
    "    axs[3, 0].set(xlabel = 'Time (s)')\n",
    "    axs[3, 1].plot(x, y_list[10])\n",
    "    axs[3, 1].set(xlabel = 'Time (s)')\n",
    "    axs[3, 2].plot(x, y_list[11])\n",
    "    axs[3, 2].set(xlabel = 'Time (s)')\n",
    "    for idx in range(len(ax_list)):\n",
    "        ax_list[idx].ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "        ax_list[idx].set_title(new_list[idx])\n",
    "        y_mean = [np.mean(y_list[idx])]*len(x)\n",
    "        # Plot the average line\n",
    "        mean_line = ax_list[idx].plot(x,y_mean, label='Mean', linestyle='--', color = \"black\")\n",
    "        # Make a legend\n",
    "        legend = ax_list[idx].legend(loc='upper right')\n",
    "    \n",
    "    MYDIR = (\"Figures\")\n",
    "    CHECK_FOLDER = os.path.isdir(MYDIR)\n",
    "\n",
    "    # If folder doesn't exist, then create it.\n",
    "    if not CHECK_FOLDER:\n",
    "        os.makedirs(MYDIR)\n",
    "        #print(\"created folder : \", MYDIR)\n",
    "    \n",
    "    #new_string = sample.replace('time series data', '').rstrip()\n",
    "    \n",
    "    filename = os.path.join(MYDIR, new_string + '.pdf')\n",
    "    plt.savefig(filename)\n",
    "    print('Plot for ', new_string, \" is complete.\")\n",
    "    \n",
    "    if choice == False:\n",
    "        plt.close()\n",
    "    #else:\n",
    "        #plt.close()\n",
    "        \n",
    "def U_Pb_report(calc_dict, intro_filename, intro = False, output_name = 'U-Pb_output.pdf'):\n",
    "    \"\"\" Makes the U-Pb plots into a super long report. The input is the dataframe from calcCPS(). Could definitely be revised...\"\"\"\n",
    "    MYDIR = (\"Figures\")\n",
    "    mergedObject = PdfFileMerger()\n",
    "    \n",
    "    if intro:\n",
    "        mergedObject.append(PdfFileReader(intro_filename, 'rb'))\n",
    "        print(f'Succesfully incorporated {intro_filename} into PDF.')\n",
    "    pd.set_option('precision', 2)\n",
    "    stats = statistics_NP2(calc_dict)\n",
    "    stat_dict = {}\n",
    "    stat_dict['stat1'] = stats.iloc[:, 14:]\n",
    "    stat_dict['stat2'] = stats.iloc[:, :8]\n",
    "    stat_dict['stat3'] = stats.iloc[:, 8:14]\n",
    "    html_list = []\n",
    "\n",
    "    for key in stat_dict:\n",
    "        name = key + \".pdf\"\n",
    "        stats_html = stat_dict[key].to_html()\n",
    "        pdfkit.from_string(stats_html, name)\n",
    "        mergedObject.append(PdfFileReader(name, 'rb'))\n",
    "\n",
    "    file_list = []\n",
    "    keys = calc_dict.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        #print(key)\n",
    "        U_Pb_plots(calc_dict, key, False)\n",
    "        new_string = key.split('time')[0].rstrip()\n",
    "        filename = os.path.join(MYDIR, new_string + '.pdf')\n",
    "\n",
    "        mergedObject.append(PdfFileReader(filename, 'rb'))\n",
    "\n",
    "    if '.pdf' in output_name:\n",
    "        pass\n",
    "    else:\n",
    "        output_name = output_name + '.pdf'\n",
    "    \n",
    "    #output_name = \"U-Pb_output.pdf\"  \n",
    "    mergedObject.write(output_name)\n",
    "\n",
    "    print(f'PDF file named: {output_name} is complete.')        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     2,
     15,
     67,
     87,
     101,
     112
    ]
   },
   "outputs": [],
   "source": [
    "### Functions for calculating excess variance (and weighted mean and MSWD, if needed): excV_prep(), MSWD_stats(), excess_var(), all_excVariance(), group_excVariance(), stat_to_excV()\n",
    "\n",
    "def excV_prep(stat_test1):\n",
    "    '''CURRENTLY NOT IN USE. Reduces and changes order of ratios and uncertainties required to calculate excess variance. Might be sueful later for error propogation or Excel export.'''\n",
    "    \n",
    "    \n",
    "    new_key_order = ['206/238_after rejection','206/238_after rejection 2se%', '208/232_after rejection', '208/232_after rejection 2se%',\n",
    "        '207/206_after rejection','207/206_after rejection 2se%', '208/206_after rejection', '208/206_after rejection 2se%',\n",
    "        '206/204_after rejection','206/204_after rejection 2se%', '208/204_after rejection', '208/204_after rejection 2se%',\n",
    "       '207/204_after rejection','207/204_after rejection 2se%', \n",
    "            ]\n",
    "    stat_test2 = stat_test1[new_key_order]\n",
    "    \n",
    "    return stat_test2\n",
    "\n",
    "def MSWD_stats(ratio_input, excess_variance, mked_df, choice = 'MSWD'):\n",
    "    '''Given an input ratio, excess variance, and dataframe with the correct columns, this function will output the MSWD for that ratio. If choice is not \"MSWD\" this function will output all steps of the MSWD calculation.'''\n",
    "    ratio = ratio_input +'_after rejection'\n",
    "\n",
    "    mked_stat_dict = {}\n",
    "\n",
    "    #print(ratio)\n",
    "    error_string = str(ratio + ' 2se%')\n",
    "\n",
    "    new_string1 = ratio + ' abs err'\n",
    "\n",
    "    mod_string1 = ratio + ' mod err'\n",
    "    #print(error_string)\n",
    "    new_string2 = ratio + ' x/(σ^2)'\n",
    "    new_string3 = ratio + ' 1/(σ^2)'\n",
    "\n",
    "    mked_df[new_string1] = mked_df.apply(lambda x: (x[error_string] / 100 ) * (x[ratio]), axis=1)\n",
    "    mked_df[mod_string1] = mked_df.apply(lambda x: (math.sqrt( x[error_string]**2 + excess_variance**2 ) / 100 ) * (x[ratio]), axis=1)\n",
    "\n",
    "    mked_df[new_string2] = mked_df.apply(lambda x: x[ratio] / ((x[mod_string1] / 2)**2), axis=1)\n",
    "    mked_df[new_string3] = mked_df.apply(lambda x: (x[new_string2] / x[ratio]), axis=1)\n",
    "\n",
    "    wtd_mean_string = str(ratio.split()[0]) + '_wtdMean'\n",
    "    one_sig2_string = str(ratio.split()[0]) + '_sig^2'\n",
    "    one_sig_string = str(ratio.split()[0]) + '_sig'\n",
    "\n",
    "    new_string2 = ratio + ' x/(σ^2)'\n",
    "    new_string3 = ratio + ' 1/(σ^2)'\n",
    "\n",
    "\n",
    "    mked_stat_dict[wtd_mean_string] = mked_df[new_string2].sum() / (mked_df[new_string3].sum())\n",
    "    mked_stat_dict[one_sig2_string] = 1 / (mked_df[new_string3].sum())\n",
    "\n",
    "    mked_stat_dict[one_sig_string] = math.sqrt(mked_stat_dict[one_sig2_string])\n",
    "\n",
    "    z_string = str(ratio.split()[0]) + '_Z'\n",
    "    wtd_mean_string = str(ratio.split()[0]) + '_wtdMean'\n",
    "    new_string3 = ratio + ' 1/(σ^2)'\n",
    "    S_string = ratio + '_S'\n",
    "    MSWD_string = ratio + '_MSWD'\n",
    "\n",
    "\n",
    "    mked_df[z_string] = mked_df.apply(lambda x: ( ((x[ratio]  - mked_stat_dict[wtd_mean_string])**2) * x[new_string3]), axis=1)\n",
    "    mked_stat_dict[S_string]=  mked_df[z_string].sum()\n",
    "    mked_stat_dict[ratio + \"_MSWD\"] = mked_stat_dict[S_string] / ((mked_df[z_string].count()) - 1)\n",
    "\n",
    "    if choice == 'MSWD':\n",
    "        return mked_stat_dict[MSWD_string]  \n",
    "\n",
    "    else:\n",
    "        return MKED_stat_dict\n",
    "    \n",
    "def excess_var(ratio, mked_df):\n",
    "    ''' Given a specific ratio and the primary standard dataframe with correct columns, \n",
    "    this function will calculate the excess variance (rounded to less than 6 significant figures) needed to correct for overdispersion.'''\n",
    "    eVar = 0\n",
    "    #pick = '207/206'\n",
    "    MSWD = MSWD_stats(ratio, eVar, mked_df)\n",
    "    print('Initial MSWD=', MSWD)\n",
    "    \n",
    "    for i in range(5):\n",
    "        while MSWD > 1.000:\n",
    "            eVar = eVar + 1/(10**i)\n",
    "            MSWD = MSWD_stats(ratio, eVar, mked_df)\n",
    "\n",
    "        eVar = eVar - 1/(10**i)\n",
    "        MSWD = MSWD_stats(ratio, eVar, mked_df)\n",
    "\n",
    "    print('excess variance=', round_sig(eVar, 6), 'MSWD=', round_sig(MSWD, 5), '\\n')\n",
    "    \n",
    "    return  round_sig(eVar, 6) \n",
    "\n",
    "def all_excVariance(mked_df):\n",
    "    ''' Uses the excess_var() function to calculate the excess variance for all relevant ratios.'''\n",
    "    ratios = ['206/238','208/232','207/206','208/206',\n",
    "            '206/204','207/204','208/204'   ]\n",
    "\n",
    "    eVar_dict = {}\n",
    "\n",
    "    for rat in ratios:\n",
    "        print(rat)\n",
    "        new_string = str(rat + '_excess variance')\n",
    "        eVar_dict[new_string] = excess_var(rat, mked_df)\n",
    "    \n",
    "    return eVar_dict\n",
    "\n",
    "def group_excVariance(stat_test2, group_name):\n",
    "    ''' Takes an input from statistics_ranktest2() and a chosen primary standard and calculates the excess variance for each isotope ratio.'''\n",
    "    AB_err_tester = stat_test2.copy()\n",
    "\n",
    "    y_list, labels = comb_groups([group_name],AB_err_tester)\n",
    "    mked_df = pd.DataFrame(y_list)\n",
    "    print('Calculating excess variance for: ', group_name, '\\n')\n",
    "    return all_excVariance(mked_df)   \n",
    "\n",
    "#mked_excV = group_excVariance(stat_test2, 'MKED-1')\n",
    "\n",
    "def stat_to_excV(tester, primary_std = 'MKED-1', reject_percentage = 0):\n",
    "    ''' Function to take the resulting df from calc_CPS() and calculate the excess variance for a chosen primary standard and error minimization.'''\n",
    "    stattie = statistics_ranktest2(tester, reject_percentage)\n",
    "    new_excV = group_excVariance(stattie, primary_std)\n",
    "    \n",
    "    return new_excV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Functions for doing correction math and error minimization: stat_correcter2(), stat_rank_and_correct()\n",
    "\n",
    "excv_dict = {}\n",
    "def stat_correcter2(stats_df, std = 'glass', excV_dict = excv_dict):\n",
    "    ''' Input dataframe that was output from statistics_ranktest2\n",
    "    function. Performs correction using specified primary standard. Slightly hard-coded for published standard values.'''\n",
    "    headers_to_keep = [\n",
    "        '206/238_after rejection',\n",
    "        '206/238_after rejection 2se%',\n",
    "        '206/238_intercept',       #Regression test\n",
    "        '208/232_after rejection',\n",
    "        '208/232_after rejection 2se%',\n",
    "        '207/206_after rejection',\n",
    "        '207/206_after rejection 2se%',\n",
    "        '208/206_after rejection',\n",
    "        '208/206_after rejection 2se%',\n",
    "        '206/204_after rejection',\n",
    "        '206/204_after rejection 2se%',\n",
    "        '208/204_after rejection',\n",
    "        '208/204_after rejection 2se%',\n",
    "        '207/204_after rejection',\n",
    "        '207/204_after rejection 2se%',\n",
    "        '207/235_after rejection',\n",
    "        '207/235_after rejection 2se%',\n",
    "        \n",
    "    ]\n",
    "\n",
    "    headers_to_math = [\n",
    "        '206/238_after rejection',\n",
    "        '206/238_intercept',       #Regression test\n",
    "        '208/232_after rejection',  \n",
    "        '207/206_after rejection',    \n",
    "        '208/206_after rejection',    \n",
    "        '206/204_after rejection',    \n",
    "        '208/204_after rejection',   \n",
    "        '207/204_after rejection',\n",
    "        '207/235_after rejection',\n",
    "    ]\n",
    "    \n",
    "### Correction Math\n",
    "    \n",
    "    #Calling relevant standard information\n",
    "    \n",
    "    if std == 'glass':\n",
    "        standard = 'SRM NIST 610'   \n",
    "    elif std == '612':\n",
    "        standard = 'SRM NIST 612'        \n",
    "    elif std == 'BHVO':\n",
    "        standard = 'BHVO-2G' \n",
    "    elif std == 'BLR-1':\n",
    "        standard = 'BLR-1'\n",
    "    else:\n",
    "        standard = 'MKED-1'\n",
    "       \n",
    "    published_ratios = published_full(standard) \n",
    "    \n",
    "    short_tester = stats_df[headers_to_keep] #Reduces columns. Might break because 2sigma has been excluded.\n",
    "    result = short_tester.copy(deep=True)\n",
    "    \n",
    "    #Getting list of group names and corresponding index positons\n",
    "    \n",
    "    start, end, group_names = group_samples(result)\n",
    "\n",
    "    #print('start:', start)\n",
    "    #print('end:', end)\n",
    "    #print('group names:', group_names)\n",
    "\n",
    " \n",
    "    stats_dict = {}\n",
    "    corrected_stats_dict = {}\n",
    "    for idx in range(len(group_names)):\n",
    "        group_dict = {}\n",
    "        #print(idx)\n",
    "        group_df = short_tester.iloc[start[idx]:end[idx]]\n",
    "        #print(group_df.index.values.tolist())\n",
    "        for col in headers_to_math:\n",
    "            \n",
    "            ###Regression Test###\n",
    "            if col == '206/238_intercept':\n",
    "                df_mean = group_df[col].mean()\n",
    "                group_dict[col + '_mean'] = df_mean\n",
    "            ###End Regression Test###\n",
    "            else:\n",
    "                #print(group_df[col])\n",
    "                name = col.split('_')[0]\n",
    "                df_mean = group_df[col].mean()\n",
    "                #print(df_mean)\n",
    "                group_dict[name + '_mean'] = df_mean\n",
    "                #External error\n",
    "                df_2sd_perc = (2 * group_df[col].std())/df_mean * 100\n",
    "\n",
    "                group_dict[name + '_2sd%'] = df_2sd_perc\n",
    "\n",
    "                df_2se_perc = (2 * group_df[col].sem())/df_mean * 100\n",
    "\n",
    "                group_dict[name + '_2se%'] = df_2se_perc        #Not sure if the group 2se% is really a relevant statistic...\n",
    "\n",
    "        stats_dict[group_names[idx]] = group_dict\n",
    "    \n",
    "    grouper = pd.DataFrame(stats_dict)\n",
    "       \n",
    "    primary_std = grouper[standard]\n",
    "        \n",
    "    \n",
    "        #Applying correction Hard coding the primary standard\n",
    "    result['206/238 corrected'] = short_tester.apply(lambda x: x['206/238_after rejection']/ (primary_std['206/238_mean'] / published_ratios['206/238']), axis=1)\n",
    "    result['208/232 corrected'] = short_tester.apply(lambda x: x['208/232_after rejection']/ (primary_std['208/232_mean'] / published_ratios['208/232']), axis=1)\n",
    "    result['207/206 corrected'] = short_tester.apply(lambda x: x['207/206_after rejection']/ (primary_std['207/206_mean'] / published_ratios['207/206']), axis=1)\n",
    "    result['208/206 corrected'] = short_tester.apply(lambda x: x['208/206_after rejection']/(primary_std['208/206_mean'] / published_ratios['208/206']), axis=1)\n",
    "    result['206/204 corrected'] = short_tester.apply(lambda x: x['206/204_after rejection']/(primary_std['206/204_mean'] / published_ratios['206/204']), axis=1)\n",
    "    result['207/204 corrected'] = short_tester.apply(lambda x: x['207/204_after rejection']/(primary_std['207/204_mean'] / published_ratios['207/204']), axis=1)\n",
    "    result['208/204 corrected'] = short_tester.apply(lambda x: x['208/204_after rejection']/(primary_std['208/204_mean'] / published_ratios['208/204']), axis=1)\n",
    "    result['207/235 corrected'] = short_tester.apply(lambda x: x['207/235_after rejection']/(primary_std['207/235_mean'] / published_ratios['207/235']), axis=1)\n",
    "    \n",
    "    result['206/238_int corrected'] = short_tester.apply(lambda x: x['206/238_intercept']/ (primary_std['206/238_intercept_mean'] / published_ratios['206/238']), axis=1)\n",
    "    \n",
    "#     #Making 2se for plotting in EXCEL\n",
    "    \n",
    "    result['206/238_after rejection 2se'] = short_tester.apply(lambda x: x['206/238_after rejection'] * x['206/238_after rejection 2se%'] / 100, axis=1)\n",
    "    result['208/232_after rejection 2se'] = short_tester.apply(lambda x: x['208/232_after rejection'] * x['208/232_after rejection 2se%'] / 100, axis=1)\n",
    "    result['207/206_after rejection 2se'] = short_tester.apply(lambda x: x['207/206_after rejection'] * x['207/206_after rejection 2se%'] / 100, axis=1)\n",
    "    result['208/206_after rejection 2se'] = short_tester.apply(lambda x: x['208/206_after rejection'] * x['208/206_after rejection 2se%'] / 100, axis=1)\n",
    "    result['206/204_after rejection 2se'] = short_tester.apply(lambda x: x['206/204_after rejection'] * x['206/204_after rejection 2se%'] / 100, axis=1)\n",
    "    result['207/204_after rejection 2se'] = short_tester.apply(lambda x: x['207/204_after rejection'] * x['207/204_after rejection 2se%'] / 100, axis=1)\n",
    "    result['208/204_after rejection 2se'] = short_tester.apply(lambda x: x['208/204_after rejection'] * x['208/204_after rejection 2se%'] / 100, axis=1)\n",
    "    result['207/235_after rejection 2se'] = short_tester.apply(lambda x: x['207/235_after rejection'] * x['207/235_after rejection 2se%'] / 100, axis=1)  \n",
    "    \n",
    "    result_headers = [\n",
    "        '206/238 corrected',\n",
    "        '208/232 corrected',\n",
    "        '207/206 corrected',\n",
    "        '208/206 corrected',\n",
    "        '206/204 corrected',\n",
    "        '207/204 corrected',\n",
    "        '208/204 corrected',\n",
    "        '207/235 corrected',\n",
    "    ]      \n",
    "    \n",
    "    for idx in range(len(group_names)):\n",
    "        group_dict = {}\n",
    "        #print(idx)\n",
    "        group_df = result.iloc[start[idx]:end[idx]]\n",
    "        #print(group_df.index.values.tolist())\n",
    "        #print(group_df)\n",
    "        for col in result_headers:\n",
    "            #print(group_df[col])\n",
    "            name = col.split('_')[0]\n",
    "            df_mean = group_df[col].mean()\n",
    "            #print(df_mean)\n",
    "            group_dict[name + '_mean'] = df_mean\n",
    "            df_2sd_perc = (2 * group_df[col].std())/df_mean * 100\n",
    "            group_dict[name + '_2sd%'] = df_2sd_perc \n",
    "            df_2sd = 2 * group_df[col].std()\n",
    "            group_dict[name + '_2sd'] = df_2sd\n",
    "            \n",
    "            df_2se_perc = (2 * group_df[col].sem())/df_mean * 100\n",
    "            group_dict[name + '_2se%'] = df_2se_perc\n",
    "            df_2se = 2 * group_df[col].sem()\n",
    "            group_dict[name + '_2se'] = df_2se\n",
    "        \n",
    "        corrected_stats_dict[group_names[idx]] = group_dict\n",
    "   \n",
    "    corrected_grouper = pd.DataFrame(corrected_stats_dict)\n",
    "     \n",
    "    grouper_flip = pd.DataFrame.transpose(grouper)\n",
    "    corrected_flip = pd.DataFrame.transpose(corrected_grouper)\n",
    "    grouper_list = [grouper_flip, corrected_flip]\n",
    "    \n",
    "    grouper_comb = pd.concat(grouper_list, axis=1)\n",
    "    \n",
    "        \n",
    "    #print('Primary Standard= ', standard)\n",
    "    \n",
    "    #Calculate excess variance\n",
    "    #excV_dict = group_excVariance(result, standard)\n",
    "    \n",
    "    #print(excV_dict)\n",
    "    ####\n",
    "    \n",
    "    external_err_std = grouper_comb.loc[standard]  \n",
    "     \n",
    "    result['206/238 corrected BB error%'] = short_tester.apply(lambda x: ((x['206/238_after rejection 2se%'])**2 + (external_err_std['206/238 corrected_2sd%'])**2)**0.5, axis=1)\n",
    "    result['208/232 corrected BB error%'] = short_tester.apply(lambda x: ((x['208/232_after rejection 2se%'])**2 + (external_err_std['208/232 corrected_2sd%'])**2)**0.5, axis=1)\n",
    "    result['207/206 corrected BB error%'] = short_tester.apply(lambda x: ((x['207/206_after rejection 2se%'])**2 + (external_err_std['207/206 corrected_2sd%'])**2)**0.5, axis=1)\n",
    "    result['208/206 corrected BB error%'] = short_tester.apply(lambda x: ((x['208/206_after rejection 2se%'])**2 + (external_err_std['208/206 corrected_2sd%'])**2)**0.5, axis=1)\n",
    "    result['206/204 corrected BB error%'] = short_tester.apply(lambda x: ((x['206/204_after rejection 2se%'])**2 + (external_err_std['206/204 corrected_2sd%'])**2)**0.5, axis=1)\n",
    "    result['207/204 corrected BB error%'] = short_tester.apply(lambda x: ((x['207/204_after rejection 2se%'])**2 + (external_err_std['207/204 corrected_2sd%'])**2)**0.5, axis=1)\n",
    "    result['208/204 corrected BB error%'] = short_tester.apply(lambda x: ((x['208/204_after rejection 2se%'])**2 + (external_err_std['208/204 corrected_2sd%'])**2)**0.5, axis=1)\n",
    "    result['207/235 corrected BB error%'] = short_tester.apply(lambda x: ((x['207/235_after rejection 2se%'])**2 + (external_err_std['207/235 corrected_2sd%'])**2)**0.5, axis=1)\n",
    "       \n",
    "    result['206/238 corrected BB error'] = result.apply(lambda x: (x['206/238 corrected BB error%'])* x['206/238 corrected']/100, axis=1)\n",
    "    result['208/232 corrected BB error'] = result.apply(lambda x: (x['208/232 corrected BB error%'])* x['208/232 corrected']/100, axis=1)\n",
    "    result['207/206 corrected BB error'] = result.apply(lambda x: (x['207/206 corrected BB error%'])* x['207/206 corrected']/100, axis=1) \n",
    "    result['208/206 corrected BB error'] = result.apply(lambda x: (x['208/206 corrected BB error%'])* x['208/206 corrected']/100, axis=1)\n",
    "    result['206/204 corrected BB error'] = result.apply(lambda x: (x['206/204 corrected BB error%'])* x['206/204 corrected']/100, axis=1)   \n",
    "    result['207/204 corrected BB error'] = result.apply(lambda x: (x['207/204 corrected BB error%'])* x['207/204 corrected']/100, axis=1)\n",
    "    result['208/204 corrected BB error'] = result.apply(lambda x: (x['208/204 corrected BB error%'])* x['208/204 corrected']/100, axis=1)\n",
    "    result['207/235 corrected BB error'] = result.apply(lambda x: (x['207/235 corrected BB error%'])* x['207/235 corrected']/100, axis=1)\n",
    "    \n",
    "    #Propagating the measurement error (internal precision; 2se%) with \n",
    "    #the primary standard excess variance(% to add to the primary standard internal precision to correct for overdispersion). \n",
    "    #This is following the direction of Horstwood et al., (2016) on how to propogate the population uncertainty (Sx).\n",
    "    \n",
    "    \n",
    "    \n",
    "    result['206/238 corrected Sx%'] = result.apply(lambda x: ((x['206/238_after rejection 2se%'])**2 + (excV_dict['206/238_excess variance'])**2)**0.5, axis=1)\n",
    "    result['208/232 corrected Sx%'] = result.apply(lambda x: ((x['208/232_after rejection 2se%'])**2 + (excV_dict['208/232_excess variance'])**2)**0.5, axis=1)\n",
    "    result['207/206 corrected Sx%'] = result.apply(lambda x: ((x['207/206_after rejection 2se%'])**2 + (excV_dict['207/206_excess variance'])**2)**0.5, axis=1)\n",
    "    result['208/206 corrected Sx%'] = result.apply(lambda x: ((x['208/206_after rejection 2se%'])**2 + (excV_dict['208/206_excess variance'])**2)**0.5, axis=1)\n",
    "    result['206/204 corrected Sx%'] = result.apply(lambda x: ((x['206/204_after rejection 2se%'])**2 + (excV_dict['206/204_excess variance'])**2)**0.5, axis=1)\n",
    "    result['207/204 corrected Sx%'] = result.apply(lambda x: ((x['207/204_after rejection 2se%'])**2 + (excV_dict['207/204_excess variance'])**2)**0.5, axis=1)\n",
    "    result['208/204 corrected Sx%'] = result.apply(lambda x: ((x['208/204_after rejection 2se%'])**2 + (excV_dict['208/204_excess variance'])**2)**0.5, axis=1)\n",
    "    \n",
    "    result['207/235 corrected Sx%'] = result.apply(lambda x: ((x['206/238 corrected Sx%'])**2 + (x['207/206 corrected Sx%'])**2)**0.5, axis=1) #Just quadratic addition of 206/238 error and 207/206 error\n",
    "      \n",
    "     \n",
    "    new_headers = [\n",
    "        '206/238 corrected BB error%',\n",
    "        '208/232 corrected BB error%',\n",
    "        '207/206 corrected BB error%',\n",
    "        '208/206 corrected BB error%',\n",
    "        '206/204 corrected BB error%',\n",
    "        '207/204 corrected BB error%',\n",
    "        '208/204 corrected BB error%',\n",
    "        '207/235 corrected BB error%',\n",
    "        '206/238 corrected BB error',\n",
    "        '208/232 corrected BB error',\n",
    "        '207/206 corrected BB error',\n",
    "        '208/206 corrected BB error',\n",
    "        '206/204 corrected BB error',\n",
    "        '207/204 corrected BB error',\n",
    "        '208/204 corrected BB error',\n",
    "        '207/235 corrected BB error',\n",
    "        '206/238 corrected Sx%',\n",
    "        '208/232 corrected Sx%',\n",
    "        '207/206 corrected Sx%',\n",
    "        '208/206 corrected Sx%',\n",
    "        '206/204 corrected Sx%',\n",
    "        '207/204 corrected Sx%',\n",
    "        '208/204 corrected Sx%',\n",
    "        '207/235 corrected Sx%',\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    new_stats_dict = {}\n",
    "    for idx in range(len(group_names)):\n",
    "        group_dict = {}\n",
    "        #print(idx)\n",
    "        group_df = result.iloc[start[idx]:end[idx]]\n",
    "        #print(group_df)\n",
    "        for col in new_headers:\n",
    "            df_mean = group_df[col].mean()\n",
    "            group_dict[col + '_mean'] = df_mean\n",
    "        \n",
    "        new_stats_dict[group_names[idx]] = group_dict \n",
    "            \n",
    "    extra_grouper = pd.DataFrame(new_stats_dict)\n",
    "     \n",
    "    corrected_flip2 = pd.DataFrame.transpose(extra_grouper)\n",
    "    grouper_list2 = [grouper_comb, corrected_flip2]\n",
    "    \n",
    "    grouper_comb_final = pd.concat(grouper_list2, axis=1)\n",
    "    #print('GROUPER',grouper_comb_final.keys())\n",
    "    #print('RESULT', result.keys())\n",
    "    ############\n",
    "\n",
    "    \n",
    "    grouper_order = ['206/238_mean', '206/238_2sd%','206/238_2se%', '208/232_mean', '208/232_2sd%','208/232_2se%',\n",
    "       '207/206_mean', '207/206_2sd%','207/206_2se%', '208/206_mean', '208/206_2sd%', '208/206_2se%',\n",
    "       '206/204_mean', '206/204_2sd%','206/204_2se%', '208/204_mean', '208/204_2sd%','208/204_2se%',\n",
    "       '207/204_mean', '207/204_2sd%', '207/204_2se%','207/235_mean', '207/235_2sd%', '207/235_2se%',\n",
    "                \n",
    "        '206/238 corrected_mean','206/238 corrected_2sd','206/238 corrected_2sd%','206/238 corrected_2se%',\n",
    "       '206/238 corrected_2se', '206/238 corrected BB error_mean', '206/238 corrected BB error%_mean', '206/238 corrected Sx%_mean',\n",
    "                     \n",
    "       '208/232 corrected_mean','208/232 corrected_2sd', '208/232 corrected_2sd%', '208/232 corrected_2se%', \n",
    "        '208/232 corrected_2se','208/232 corrected BB error_mean', '208/232 corrected BB error%_mean', '208/232 corrected Sx%_mean',\n",
    "                     \n",
    "       '207/206 corrected_mean', '207/206 corrected_2sd', '207/206 corrected_2sd%','207/206 corrected_2se%',\n",
    "       '207/206 corrected_2se', '207/206 corrected BB error_mean','207/206 corrected BB error%_mean', '207/206 corrected Sx%_mean',\n",
    "                     \n",
    "        '208/206 corrected_mean', '208/206 corrected_2sd','208/206 corrected_2sd%', '208/206 corrected_2se%', \n",
    "        '208/206 corrected_2se','208/206 corrected BB error_mean', '208/206 corrected BB error%_mean', '208/206 corrected Sx%_mean',\n",
    "                     \n",
    "        '206/204 corrected_mean','206/204 corrected_2sd','206/204 corrected_2sd%', '206/204 corrected_2se%',\n",
    "       '206/204 corrected_2se', '206/204 corrected BB error_mean', '206/204 corrected BB error%_mean', '206/204 corrected Sx%_mean',\n",
    "                     \n",
    "      '207/204 corrected_mean','207/204 corrected_2sd', '207/204 corrected_2sd%', '207/204 corrected_2se%', \n",
    "        '207/204 corrected_2se', '207/204 corrected BB error_mean', '207/204 corrected BB error%_mean', '207/204 corrected Sx%_mean',\n",
    "                     \n",
    "      '208/204 corrected_mean', '208/204 corrected_2sd','208/204 corrected_2sd%','208/204 corrected_2se%',\n",
    "       '208/204 corrected_2se', '208/204 corrected BB error_mean', '208/204 corrected BB error%_mean', '208/204 corrected Sx%_mean',\n",
    "                     \n",
    "       '207/235 corrected_mean', '207/235 corrected_2sd', '207/235 corrected_2sd%','207/235 corrected_2se%',\n",
    "       '207/235 corrected_2se', '207/235 corrected BB error_mean','207/235 corrected BB error%_mean', '207/235 corrected Sx%_mean',\n",
    "            ]\n",
    "    \n",
    "    result_order = ['206/238_after rejection', '206/238_after rejection 2se%','206/238_after rejection 2se',\n",
    "                   '208/232_after rejection','208/232_after rejection 2se%','208/232_after rejection 2se',\n",
    "                   '207/206_after rejection',  '207/206_after rejection 2se%','207/206_after rejection 2se',\n",
    "                   '208/206_after rejection', '208/206_after rejection 2se%','208/206_after rejection 2se',\n",
    "                    '206/204_after rejection', '206/204_after rejection 2se%','206/204_after rejection 2se',\n",
    "                   '208/204_after rejection', '208/204_after rejection 2se%','208/204_after rejection 2se',\n",
    "                   '207/204_after rejection',  '207/204_after rejection 2se%','207/204_after rejection 2se',\n",
    "                   '207/235_after rejection','207/235_after rejection 2se%','207/235_after rejection 2se',\n",
    "                    \n",
    "                   '206/238 corrected', '206/238_int corrected', '206/238 corrected BB error', '206/238 corrected BB error%', '206/238 corrected Sx%',\n",
    "                    '208/232 corrected', '208/232 corrected BB error','208/232 corrected BB error%', '208/232 corrected Sx%',\n",
    "                    '207/206 corrected', '207/206 corrected BB error', '207/206 corrected BB error%', '207/206 corrected Sx%',\n",
    "                    '208/206 corrected', '208/206 corrected BB error', '208/206 corrected BB error%', '208/206 corrected Sx%',\n",
    "                    '206/204 corrected', '206/204 corrected BB error', '206/204 corrected BB error%', '206/204 corrected Sx%',\n",
    "                    '207/204 corrected', '207/204 corrected BB error', '207/204 corrected BB error%', '207/204 corrected Sx%',\n",
    "                   '208/204 corrected',  '208/204 corrected BB error', '208/204 corrected BB error%', '208/204 corrected Sx%',\n",
    "                    '207/235 corrected', '207/235 corrected BB error', '207/235 corrected BB error%', '207/235 corrected Sx%',\n",
    "                     ]\n",
    "        \n",
    "        \n",
    "    result_plotter = ['206/238 corrected',\n",
    "                      '206/238_int corrected', '206/238 corrected Sx%','206/238 corrected BB error%', \n",
    "                      '206/238_after rejection 2se%', \n",
    "       '207/206 corrected', '207/206 corrected Sx%','207/206 corrected BB error%', \n",
    "                      '207/206_after rejection 2se%',  \n",
    "        '208/232 corrected', '208/232 corrected Sx%','208/232 corrected BB error%',  \n",
    "                      '208/232_after rejection 2se%', \n",
    "        '208/206 corrected',  '208/206 corrected Sx%', '208/206 corrected BB error%', \n",
    "                      '208/206_after rejection 2se%',\n",
    "        '206/204 corrected', '206/204 corrected Sx%', '206/204 corrected BB error%', \n",
    "                      '206/204_after rejection 2se%', \n",
    "        '207/204 corrected', '207/204 corrected Sx%', '207/204 corrected BB error%', \n",
    "                      '207/204_after rejection 2se%', \n",
    "       '208/204 corrected', '208/204 corrected Sx%',  '208/204 corrected BB error%', \n",
    "                      '208/204_after rejection 2se%', \n",
    "        '207/235 corrected', '207/235 corrected Sx%', '207/235 corrected BB error%', \n",
    "                      '207/235_after rejection 2se%', \n",
    "         ]\n",
    "          \n",
    "    grouper_plotter = ['206/238 corrected_mean','206/238 corrected_2sd','206/238 corrected BB error_mean', '206/238 corrected Sx%_mean',\n",
    "       '208/232 corrected_mean','208/232 corrected_2sd','208/232 corrected BB error_mean', '208/232 corrected Sx%_mean',\n",
    "       '207/206 corrected_mean', '207/206 corrected_2sd', '207/206 corrected BB error_mean', '207/206 corrected Sx%_mean',\n",
    "        '208/206 corrected_mean', '208/206 corrected_2sd','208/206 corrected BB error_mean', '208/206 corrected Sx%_mean',\n",
    "        '206/204 corrected_mean','206/204 corrected_2sd', '206/204 corrected BB error_mean', '206/204 corrected Sx%_mean',\n",
    "      '207/204 corrected_mean','207/204 corrected_2sd', '207/204 corrected BB error_mean', '207/204 corrected Sx%_mean',\n",
    "      '208/204 corrected_mean', '208/204 corrected_2sd', '208/204 corrected BB error_mean', '208/204 corrected Sx%_mean',\n",
    "     '207/235 corrected_mean','207/235 corrected_2sd', '207/235 corrected BB error_mean', '207/235 corrected Sx%_mean',                 \n",
    "        ]\n",
    "    \n",
    "   \n",
    "    result = result[result_order]\n",
    "    grouper_comb_final = grouper_comb_final[grouper_order]\n",
    "                                                        \n",
    "    result_plot = result[result_plotter]\n",
    "    grouper_plot = grouper_comb_final[grouper_plotter]                                                    \n",
    "                                                                                                           \n",
    "    \n",
    "    return result, grouper_comb_final, result_plot, grouper_plot\n",
    "\n",
    "excv_dict = {}\n",
    "def stat_rank_and_correct(calc_dict, rank_perc = 0, std = 'glass', excv_dict = excv_dict):\n",
    "    ''' Function for automating minimization and correction of isotope ratio data.'''\n",
    "    stat_tester = statistics_ranktest2(calc_dict, rank_perc)\n",
    "    \n",
    "    \n",
    "    print('')\n",
    "    #print('Error minimization rejection percentage= ', rank_perc)\n",
    "    result, grouper_comb_final, result_plot, grouper_plot = stat_correcter2(stat_tester, std, excv_dict)\n",
    "    \n",
    "    #print('Debug')\n",
    "    stat1_order = ['238_CPS_mean', '238_CPS_2σ%', '232_CPS_mean', '232_CPS_2σ%',\n",
    "       '208_CPS_mean', '208_CPS_2σ%', '207_CPS_mean', '207_CPS_2σ%',\n",
    "       '206_CPS_mean', '206_CPS_2σ%', '204_CPS_mean', '204_CPS_2σ%',\n",
    "       '202_CPS_mean', '202_CPS_2σ%', 'OPZ_238_mean', 'OPZ_232_mean',\n",
    "       'OPZ_208_mean', 'OPZ_207_mean', 'OPZ_206_mean', 'OPZ_204_mean',\n",
    "       'OPZ_202_mean','OPZ_208/206_mean','OPZ_207/206_mean','OPZ_206/204_Hg-corrected_mean',\n",
    "        'SNR_238_mean', 'SNR_232_mean',\n",
    "       'SNR_208_mean', 'SNR_207_mean', 'SNR_206_mean', 'SNR_204_mean',\n",
    "       'SNR_202_mean', '206/238_before rejection',\n",
    "       '206/238_before rejection 2se%',  '208/232_before rejection',\n",
    "       '208/232_before rejection 2se%',  '207/206_before rejection',\n",
    "       '207/206_before rejection 2se%',  '208/206_before rejection',\n",
    "       '208/206_before rejection 2se%',  '206/204_before rejection',\n",
    "       '206/204_before rejection 2se%',  '208/204_before rejection',\n",
    "       '208/204_before rejection 2se%',  '207/204_before rejection',\n",
    "       '207/204_before rejection 2se%',  'Time (s)']\n",
    "    stat1 = stat_tester[stat1_order]\n",
    "    \n",
    "    stat2 = pd.concat([stat1, result], axis=1)\n",
    "    stat_new = stat2\n",
    "    group_new = grouper_comb_final\n",
    "    new_grouper_plot = grouper_plot\n",
    "    \n",
    "    stat2_order = ['Time (s)','238_CPS_mean', '238_CPS_2σ%', '232_CPS_mean', '232_CPS_2σ%',\n",
    "    '208_CPS_mean', '208_CPS_2σ%', '207_CPS_mean', '207_CPS_2σ%',\n",
    "       '206_CPS_mean', '206_CPS_2σ%', '204_CPS_mean', '204_CPS_2σ%',\n",
    "       '202_CPS_mean', '202_CPS_2σ%', 'OPZ_238_mean', 'OPZ_232_mean',\n",
    "       'OPZ_208_mean', 'OPZ_207_mean', 'OPZ_206_mean', 'OPZ_204_mean',\n",
    "       'OPZ_202_mean', 'OPZ_208/206_mean','OPZ_207/206_mean','OPZ_206/204_Hg-corrected_mean',\n",
    "        'SNR_238_mean', 'SNR_232_mean',\n",
    "       'SNR_208_mean', 'SNR_207_mean', 'SNR_206_mean', 'SNR_204_mean',\n",
    "       'SNR_202_mean', '206/238_before rejection',\n",
    "       '206/238_before rejection 2se%',  '208/232_before rejection',\n",
    "       '208/232_before rejection 2se%',  '207/206_before rejection',\n",
    "       '207/206_before rejection 2se%',  '208/206_before rejection',\n",
    "       '208/206_before rejection 2se%',  '206/204_before rejection',\n",
    "       '206/204_before rejection 2se%',  '208/204_before rejection',\n",
    "       '208/204_before rejection 2se%',  '207/204_before rejection', '207/204_before rejection 2se%',  \n",
    "       '206/238_after rejection', '206/238_after rejection 2se%','206/238_after rejection 2se',\n",
    "       '208/232_after rejection', '208/232_after rejection 2se%','208/232_after rejection 2se',\n",
    "       '207/206_after rejection', '207/206_after rejection 2se%','207/206_after rejection 2se',\n",
    "       '208/206_after rejection', '208/206_after rejection 2se%','208/206_after rejection 2se',\n",
    "       '206/204_after rejection', '206/204_after rejection 2se%','206/204_after rejection 2se',\n",
    "       '208/204_after rejection', '208/204_after rejection 2se%','208/204_after rejection 2se',\n",
    "       '207/204_after rejection', '207/204_after rejection 2se%','207/204_after rejection 2se',\n",
    "       '206/238 corrected','206/238_int corrected', '206/238 corrected BB error', '206/238 corrected BB error%', '206/238 corrected Sx%',\n",
    "       '208/232 corrected', '208/232 corrected BB error','208/232 corrected BB error%', '208/232 corrected Sx%',\n",
    "       '207/206 corrected', '207/206 corrected BB error', '207/206 corrected BB error%', '207/206 corrected Sx%',\n",
    "       '208/206 corrected', '208/206 corrected BB error', '208/206 corrected BB error%', '208/206 corrected Sx%',\n",
    "       '206/204 corrected', '206/204 corrected BB error', '206/204 corrected BB error%', '206/204 corrected Sx%',\n",
    "       '207/204 corrected', '207/204 corrected BB error', '207/204 corrected BB error%', '207/204 corrected Sx%',\n",
    "       '208/204 corrected',  '208/204 corrected BB error', '208/204 corrected BB error%', '208/204 corrected Sx%',         \n",
    "        ]\n",
    "  \n",
    "    \n",
    "    stat2 = stat_new[stat2_order]\n",
    "    \n",
    "    return stat2, group_new, result_plot, new_grouper_plot\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     19,
     68,
     211,
     214,
     254,
     257,
     293,
     296
    ]
   },
   "outputs": [],
   "source": [
    "### Functions for actually running correction-math, calculating ages, and exporting: calc_all_ages(), correction_to_dict(),\n",
    "\n",
    "def calc_all_ages(plot):\n",
    "    ''' Calculates isotope decay system ages for 206/238, 207/206, 207/235, and 208/232'''\n",
    "     #Calculating Ages\n",
    "    plot['206/238 Age [Ma]'] = plot.apply(lambda x: calc_t68(x['206/238 corrected'])/ 1E6, axis=1)\n",
    "    plot['207/206 Age [Ma]'] = plot.apply(lambda x: calc_t76_2(x['207/206 corrected'])/ 1E6, axis=1)\n",
    "    plot['207/235 Age [Ma]'] = plot.apply(lambda x: calc_t75(x['207/235 corrected'])/ 1E6, axis=1)\n",
    "    plot['208/232 Age [Ma]'] = plot.apply(lambda x: calc_t82(x['208/232 corrected'])/ 1E6, axis=1)\n",
    "    plot['238/206'] = plot.apply(lambda x: 1 / x['206/238 corrected'], axis=1)\n",
    "    plot['238/206_int'] = plot.apply(lambda x: 1 / x['206/238_int corrected'], axis=1)\n",
    "    \n",
    "    first_column = plot.pop('238/206')\n",
    "    plot.insert(0, '238/206', first_column)\n",
    "    second_column = plot.pop('238/206_int')\n",
    "    plot.insert(1, '238/206_int', second_column)\n",
    "    \n",
    "    return plot\n",
    "\n",
    "def df_correction_207Pb(ttn_zero_full_plot, iterations = 5):\n",
    "    '''Performs 5 iterations of 207-Pb correction using Stacey-Kramers on the input plot dataframe.''' #Need to be able to do this with different initial Pb values\n",
    "    \n",
    "    df =  ttn_zero_full_plot\n",
    "\n",
    "\n",
    "    for idx in range (iterations):\n",
    "        if idx == 0:\n",
    "            df['206/238_Age'] = df.apply(lambda x: calc_t68(x['206/238 corrected']) / 1E6, axis=1)\n",
    "            \n",
    "        else:\n",
    "            df['206/238_Age'] = df.apply(lambda x: calc_t68(x['206/238_corr']) / 1E6, axis=1)\n",
    "             \n",
    "        df['207/206_initial_SK'] = df.apply(lambda x: SK_values(x['206/238_Age'])['SK_207_206'], axis=1)\n",
    "        df['207/206_rad_expected'] = df.apply(lambda x: calc_concordant_ratios(x['206/238_Age'])['207*/206*'], axis=1)\n",
    "        df['f_206'] = df.apply(lambda x: (x['207/206 corrected'] - x['207/206_rad_expected']) / (x['207/206_initial_SK'] - x['207/206_rad_expected']), axis=1)\n",
    "        df['206/238_corr'] = df.apply(lambda x: (1 - x['f_206']) * x['206/238 corrected'], axis=1)\n",
    "        \n",
    "    df['208/206_initial_SK'] = df.apply(lambda x: SK_values(x['206/238_Age'])['SK_208_206'], axis=1)   \n",
    "    df['206/238_Age'] = df.apply(lambda x: calc_t68(x['206/238_corr']) / 1E6, axis=1)\n",
    "    \n",
    "    df['207/206_corr'] = df.apply(lambda x: (x['207/206 corrected'] -  x['207/206_initial_SK'] * x['f_206']) / (1 - x['f_206']), axis=1)\n",
    "    df['208/206_corr'] = df.apply(lambda x: (x['208/206 corrected'] -  x['208/206_initial_SK'] * x['f_206']) / (1 - x['f_206']), axis=1)\n",
    "    df['238/232_calc'] = df.apply(lambda x: (x['208/232 corrected']) *  (1 / x['208/206 corrected']) * (1 / x['206/238 corrected']), axis=1)\n",
    "    df['208/232_corr'] = df.apply(lambda x: (x['208/232 corrected']) -  (x['206/238 corrected'] * x['208/206_initial_SK'] * x['238/232_calc'] * x['f_206']),  axis=1)                                       \n",
    "    df['207/235_corr'] = df.apply(lambda x: x['207/235 corrected']  -  (x['206/238 corrected'] * x['207/206_initial_SK'] * U85r * x['f_206']),  axis=1)\n",
    "\n",
    "    \n",
    "    to_keep = ['206/238 corrected','206/238_int corrected', '206/238 corrected Sx%',\n",
    "       '206/238 corrected BB error%', '206/238_after rejection 2se%',\n",
    "       '207/206 corrected', '207/206 corrected Sx%',\n",
    "       '207/206 corrected BB error%', '207/206_after rejection 2se%',\n",
    "       '208/232 corrected', '208/232 corrected Sx%',\n",
    "       '208/232 corrected BB error%', '208/232_after rejection 2se%',\n",
    "       '208/206 corrected', '208/206 corrected Sx%',\n",
    "       '208/206 corrected BB error%', '208/206_after rejection 2se%',\n",
    "       '206/204 corrected', '206/204 corrected Sx%',\n",
    "       '206/204 corrected BB error%', '206/204_after rejection 2se%',\n",
    "       '207/204 corrected', '207/204 corrected Sx%',\n",
    "       '207/204 corrected BB error%', '207/204_after rejection 2se%',\n",
    "       '208/204 corrected', '208/204 corrected Sx%',\n",
    "       '208/204 corrected BB error%', '208/204_after rejection 2se%',\n",
    "       '207/235 corrected', '207/235 corrected Sx%',\n",
    "       '207/235 corrected BB error%', '207/235_after rejection 2se%',\n",
    "        'f_206',  '206/238_corr', '207/206_corr', '208/206_corr', '208/232_corr', '207/235_corr'     \n",
    "              ]\n",
    "    \n",
    "    return df[to_keep]\n",
    "\n",
    "def calc_all_ages_corr207(plot):\n",
    "    ''' Calculates isotope decay system ages for 206/238, 207/206, 207/235, and 208/232 using uncorrected and corrected values.'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    plot = df_correction_207Pb(plot)\n",
    "    \n",
    "    plot = plot.copy()\n",
    "    \n",
    "     #Calculating Ages\n",
    "    plot['206/238 Age [Ma]_uncorr'] = plot.apply(lambda x: calc_t68(x['206/238 corrected'])/ 1E6, axis=1)\n",
    "    plot['207/206 Age [Ma]_uncorr'] = plot.apply(lambda x: calc_t76_2(x['207/206 corrected'])/ 1E6, axis=1)\n",
    "    plot['207/235 Age [Ma]_uncorr'] = plot.apply(lambda x: calc_t75(x['207/235 corrected'])/ 1E6, axis=1)\n",
    "    plot['208/232 Age [Ma]_uncorr'] = plot.apply(lambda x: calc_t82(x['208/232 corrected'])/ 1E6, axis=1)\n",
    "\n",
    "    plot['206/238 Age [Ma]_corr207'] = plot.apply(lambda x: calc_t68(x['206/238_corr'])/ 1E6, axis=1)\n",
    "    plot['207/206 Age [Ma]_corr207'] = plot.apply(lambda x: calc_t76_2(x['207/206_corr'])/ 1E6, axis=1)\n",
    "    plot['207/235 Age [Ma]_corr207'] = plot.apply(lambda x: calc_t75(x['207/235_corr'])/ 1E6, axis=1)\n",
    "    plot['208/232 Age [Ma]_corr207'] = plot.apply(lambda x: calc_t82(x['208/232_corr'])/ 1E6, axis=1)\n",
    "    \n",
    "    \n",
    "    plot['238/206_corr207'] = plot.apply(lambda x: 1 / x['206/238_corr'], axis=1)\n",
    "    \n",
    "    plot['238/206'] = plot.apply(lambda x: 1 / x['206/238 corrected'], axis=1)\n",
    "    \n",
    "    first_column = plot.pop('238/206')\n",
    "    plot.insert(0, '238/206', first_column)\n",
    "    \n",
    "    return plot\n",
    "\n",
    "def correction_to_dict(tester, date, standard):\n",
    "    '''Performs correction with NIST 612 and MKED-1, at 0%, 20%, error minimization. Outputs a list of dicts: full_export, plot_export, and chronologic_export.'''\n",
    "    \n",
    "    print('Primary Standard= ', 'SRM NIST 612')\n",
    "    print('Error minimization rejection percentage= ', 0)\n",
    "    excV_dict1 = stat_to_excV(tester, 'SRM NIST 612', 0)\n",
    "    glass612_zero_full, glass612_zero_groups, glass612_zero_full_plot, glass612_zero_groups_plot  = stat_rank_and_correct(tester, 0, '612',excV_dict1 )\n",
    "    \n",
    "    print('Primary Standard= ', 'SRM NIST 612')\n",
    "    print('Error minimization rejection percentage= ', 20)\n",
    "    excV_dict2 = stat_to_excV(tester, 'SRM NIST 612', 20)\n",
    "    glass612_20_full, glass612_20_groups, glass612_20_full_plot, glass612_20_groups_plot = stat_rank_and_correct(tester, 20, '612', excV_dict2)\n",
    "    #glass612_40_full, glass612_40_groups, glass612_40_full_plot, glass612_40_groups_plot = stat_rank_and_correct(tester, 40, '612')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Primary Standard= ', standard)\n",
    "    print('Error minimization rejection percentage= ', 0)\n",
    "    excV_dict3 = stat_to_excV(tester, standard, 0)\n",
    "    ttn_zero_full, ttn_zero_groups, ttn_zero_full_plot, ttn_zero_groups_plot  = stat_rank_and_correct(tester, 0, standard, excV_dict3)\n",
    "    \n",
    "    print('Primary Standard= ', standard)\n",
    "    print('Error minimization rejection percentage= ', 20)\n",
    "    excV_dict4 = stat_to_excV(tester, standard, 20)\n",
    "    ttn_20_full, ttn_20_groups, ttn_20_full_plot, ttn_20_groups_plot = stat_rank_and_correct(tester, 20, standard, excV_dict4)\n",
    "    #ttn_40_full, ttn_40_groups, ttn_40_full_plot, ttn_40_groups_plot = stat_rank_and_correct(tester, 40, 'ttn')\n",
    "\n",
    "    \n",
    "    export_dict = {'glass612_zero_full':glass612_zero_full, \n",
    "               'glass612_zero_groups':glass612_zero_groups,\n",
    "               'glass612_20_full': glass612_20_full, \n",
    "               'glass612_20_groups': glass612_20_groups,\n",
    "               #'glass612_40_full':glass612_40_full, \n",
    "               #'glass612_40_groups':glass612_40_groups,\n",
    "               \n",
    "               \n",
    "               'ttn_zero_full': ttn_zero_full, \n",
    "               'ttn_zero_groups': ttn_zero_groups,\n",
    "               'ttn_20_full': ttn_20_full, \n",
    "               'ttn_20_groups': ttn_20_groups,\n",
    "               #'ttn_40_full': ttn_40_full, \n",
    "               #'ttn_40_groups':ttn_40_groups\n",
    "              }\n",
    "\n",
    "    plot_dict = {\n",
    "               'glass612_zero_full':calc_all_ages_corr207(glass612_zero_full_plot), \n",
    "               'glass612_zero_groups':glass612_zero_groups_plot,\n",
    "               'glass612_20_full': calc_all_ages_corr207(glass612_20_full_plot), \n",
    "               'glass612_20_groups': glass612_20_groups_plot,\n",
    "               #'glass612_40_full':calc_all_ages(glass612_40_full_plot), \n",
    "               #'glass612_40_groups':glass612_40_groups_plot,\n",
    "             \n",
    "             \n",
    "               'ttn_zero_full': calc_all_ages_corr207(ttn_zero_full_plot), \n",
    "               'ttn_zero_groups': ttn_zero_groups_plot,\n",
    "               'ttn_20_full': calc_all_ages_corr207(ttn_20_full_plot), \n",
    "               'ttn_20_groups': ttn_20_groups_plot,\n",
    "               #'ttn_40_full': calc_all_ages(ttn_40_full_plot), \n",
    "               #'ttn_40_groups':ttn_40_groups_plot\n",
    "              }\n",
    "\n",
    "    chrono_dict = {\n",
    "            'glass612_zero_full': chronologic(glass612_zero_full, chrono_order), \n",
    "            'glass612_zero_groups': glass612_zero_groups,\n",
    "                }\n",
    "\n",
    "    return [export_dict, plot_dict, chrono_dict]\n",
    "\n",
    "def export_corr_full(export_dict, date, bracket = False):\n",
    "    '''Export for full data worksheets'''\n",
    "\n",
    "    \n",
    "    if bracket:\n",
    "        excel_name = 'Processed_LASS_data_difRejects_difStds_BRACKETS_' + date + '.xlsx'\n",
    "    else:    \n",
    "        excel_name = 'Processed_LASS_data_difRejects_difStds_' + date + '.xlsx'\n",
    "    with pd.ExcelWriter(excel_name, engine = 'xlsxwriter') as writer:\n",
    "          # Get the xlsxwriter workbook objects.\n",
    "        workbook  = writer.book\n",
    "        col_format1 = workbook.add_format()\n",
    "        col_format2 = workbook.add_format()\n",
    "        col_format3 = workbook.add_format()\n",
    "        col_format1.set_bg_color('#E6E6FA') #Lavender\n",
    "        col_format2.set_bg_color('#98FB98') #Pale Green\n",
    "        col_format3.set_bg_color('#98FB98') #Antique White\n",
    "\n",
    "\n",
    "        for sheet in export_dict:\n",
    "            export_dict[sheet].to_excel(writer, sheet_name = sheet, index = True)\n",
    "            worksheet = writer.sheets[sheet]  # pull worksheet object\n",
    "            test_list = list(export_dict[sheet].keys())\n",
    "            worksheet.set_column(0, 0, 20)\n",
    "            worksheet.freeze_panes(1,1)\n",
    "            for idx in enumerate(test_list):\n",
    "                if 'full' in sheet:\n",
    "                    if 'BB' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                    elif 'corrected' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format1)\n",
    "                    elif 'SNR_20' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format2)\n",
    "\n",
    "                    else:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                elif 'groups' in sheet:\n",
    "                    if 'BB' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                    elif 'mean' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format2)\n",
    "                    elif 'MSWD' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format3)\n",
    "                    elif 'Weighted' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format1)        \n",
    "                    else:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "\n",
    "def export_corr_plot(plot_dict, date, bracket = False):\n",
    "    '''Export for plotting data worksheets''' \n",
    "\n",
    "    if bracket:\n",
    "        excel_name = 'Processed_LASS_data_difRejects_difStds_BRACKETS_' + date + '_plot.xlsx'\n",
    "    else: \n",
    "        excel_name = 'Processed_LASS_data_difRejects_difStds_' + date + '_plot.xlsx'\n",
    "\n",
    "    with pd.ExcelWriter(excel_name, engine = 'xlsxwriter') as writer:\n",
    "        # Get the xlsxwriter workbook objects.\n",
    "        workbook  = writer.book\n",
    "        col_format1 = workbook.add_format()\n",
    "        col_format2 = workbook.add_format()\n",
    "        col_format3 = workbook.add_format()\n",
    "        col_format1.set_bg_color('#E6E6FA') #Lavender\n",
    "        col_format2.set_bg_color('#98FB98') #Pale Green\n",
    "        col_format3.set_bg_color('#98FB98') #Antique White\n",
    "\n",
    "        for sheet in plot_dict:\n",
    "\n",
    "            plot_dict[sheet].to_excel(writer, sheet_name = sheet, index = True)\n",
    "            worksheet = writer.sheets[sheet]  # pull worksheet object\n",
    "            test_list = list(plot_dict[sheet].keys())\n",
    "            worksheet.set_column(0, 0, 20)\n",
    "            worksheet.freeze_panes(1,1)\n",
    "            for idx in enumerate(test_list):\n",
    "                if 'full' in sheet:\n",
    "                    if 'BB' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                    else:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format1)\n",
    "                elif 'groups' in sheet:\n",
    "                    if 'BB' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                    elif 'mean' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format1)\n",
    "                    elif 'MSWD' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format3)\n",
    "                    elif 'Weighted' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format2)         \n",
    "                    else:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20) \n",
    "                        \n",
    "def export_corr_chrono(chrono_dict, date):\n",
    "    '''Export for chronologic data worksheets'''\n",
    "    \n",
    "    excel_name = 'Processed_LASS_data_difRejects_difStds_' + date +'_chrono.xlsx'\n",
    "    with pd.ExcelWriter(excel_name, engine = 'xlsxwriter') as writer:\n",
    "          # Get the xlsxwriter workbook objects.\n",
    "        workbook  = writer.book\n",
    "        col_format1 = workbook.add_format()\n",
    "        col_format2 = workbook.add_format()\n",
    "        col_format1.set_bg_color('#E6E6FA') #Lavender\n",
    "        col_format2.set_bg_color('#98FB98') #Pale Green\n",
    "\n",
    "        for sheet in chrono_dict:\n",
    "\n",
    "            chrono_dict[sheet].to_excel(writer, sheet_name = sheet, index = True)\n",
    "            worksheet = writer.sheets[sheet]  # pull worksheet object\n",
    "            test_list = list(chrono_dict[sheet].keys())\n",
    "            worksheet.set_column(0, 0, 20)\n",
    "            worksheet.freeze_panes(1,1)\n",
    "            for idx in enumerate(test_list):\n",
    "                if 'full' in sheet:\n",
    "                    if 'BB' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                    elif 'corrected' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format1)\n",
    "                    elif 'SNR_20' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format2)\n",
    "                    elif 'OPZ' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format1)   \n",
    "                    else:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                elif 'groups' in sheet:\n",
    "                    if 'BB' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                    elif 'mean' in idx[1]:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format2)\n",
    "                    else:\n",
    "                        worksheet.set_column(idx[0]+1, idx[0]+1, 20)   \n",
    "                                               \n",
    "def corr_to_excel(tester, date, standard):\n",
    "    corr_list = correction_to_dict(tester, date, standard)\n",
    "    print('\\nSTEP 6. correction_to_dict() is complete.\\n')\n",
    "    export = corr_list[0]\n",
    "    plot = corr_list[1]\n",
    "    #chrono = corr_list[2]\n",
    "   \n",
    "    export_corr_full(export, date)\n",
    "    print('STEP 7. export_corr_full() is complete.\\n')\n",
    "    export_corr_plot(plot, date)\n",
    "    print('STEP 8. export_corr_plot() is complete.\\n')\n",
    "    #export_corr_chrono(chrono, date)\n",
    "    \n",
    "    return plot['ttn_zero_full']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     2,
     26,
     61
    ]
   },
   "outputs": [],
   "source": [
    "### Miscellaneous functions: print_chrono(), export_isoplot(), file_to_output()\n",
    "chrono_order = []\n",
    "def print_chrono(chrono_order = chrono_order):\n",
    "    '''This just prints lists in three's so it is easier to read as output.'''\n",
    "    for i in enumerate(chrono_order):\n",
    "        co = chrono_order\n",
    "        x = 0 + 3 * i[0]\n",
    "        y = 1 + 3 * i[0]\n",
    "        z = 2 + 3 * i[0]\n",
    "        limit = len(chrono_order)\n",
    "        #print(i)\n",
    "        \n",
    "        if (z == limit):\n",
    "            print(co[x],co[y], sep = ', ')\n",
    "            print('\\n')\n",
    "            break\n",
    "        elif (y == limit):\n",
    "            print(co[x])\n",
    "            print('\\n')\n",
    "            break\n",
    "        elif (x == limit):\n",
    "            print('\\n')\n",
    "            break\n",
    "        else:\n",
    "            print(co[x],co[y],co[z], sep = ', ')\n",
    "                       \n",
    "def export_isoplot(filename, isoplot, date, bracket = False, Pbcorr = False):\n",
    "    '''Export for isoplot worksheet; MKED-1 std, 0% error minimization.'''\n",
    "    \n",
    "    \n",
    "    if Pbcorr:\n",
    "        iso_order = ['238/206','238/206_int','238/206_corr207','206/238_int corrected', '206/238 corrected Sx%',  '207/206 corrected', '207/206_corr', '207/206 corrected Sx%', '206/238 corrected BB error%', \n",
    "            '207/206 corrected BB error%', '206/238_after rejection 2se%','207/206_after rejection 2se%', \n",
    "             '206/238 Age [Ma]_uncorr', '207/206 Age [Ma]_uncorr', '207/235 Age [Ma]_uncorr','208/232 Age [Ma]_uncorr',\n",
    "                '206/238 Age [Ma]_corr207', '207/206 Age [Ma]_corr207', '207/235 Age [Ma]_corr207','208/232 Age [Ma]_corr207']\n",
    "    else:    \n",
    "        iso_order = ['238/206', '238/206_int','206/238 corrected Sx%',  '207/206 corrected', '207/206 corrected Sx%', '206/238 corrected BB error%', \n",
    "            '207/206 corrected BB error%', '206/238_after rejection 2se%','207/206_after rejection 2se%', '206/238_int corrected', \n",
    "            '206/238 Age [Ma]', '207/206 Age [Ma]', '207/235 Age [Ma]','208/232 Age [Ma]',\n",
    "                ]  \n",
    "            \n",
    "    isoplot = isoplot[iso_order]\n",
    "    indices = isoplot.index.values.tolist()\n",
    "\n",
    "    for name in indices:\n",
    "        if 'SRM NIST' in name:\n",
    "            isoplot = isoplot.drop(name, axis=0)\n",
    "    if bracket:\n",
    "        excel_name = str(filename.split('.')[0]) + '_isoplot_BRACKETS_' + date + '.xlsx'\n",
    "    else:    \n",
    "        excel_name = str(filename.split('.')[0]) + '_isoplot_' + date + '.xlsx'\n",
    "    with pd.ExcelWriter(excel_name, engine = 'xlsxwriter') as writer:\n",
    "          # Get the xlsxwriter workbook objects.\n",
    "        workbook  = writer.book\n",
    "        isoplot.to_excel(writer, sheet_name = 'Isoplot', index = True)\n",
    "        worksheet = writer.sheets['Isoplot']  # pull worksheet object\n",
    "        \n",
    "        for idx in range(20):\n",
    "            worksheet.set_column(idx, idx, 22)\n",
    "        worksheet.freeze_panes(1,1)\n",
    "                   \n",
    "def file_to_output(filename, date, order, standard, report_choice = True):\n",
    "    ''' Just combines all of the steps for data reduction.'''\n",
    "    print(f'File chosen = {filename} \\n\\nDate chosen = {date} \\n\\nChronologic order defined as: ')\n",
    "\n",
    "    print_chrono(order)\n",
    "\n",
    "    test_df = read_np2_timeseries(filename)   #ACTUAL CODE\n",
    "\n",
    "    print('STEP 1. read_np2_timeseries() is complete.\\n')\n",
    "    global tester\n",
    "    tester = calc_CPS(test_df)     #ACTUAL CODE\n",
    "\n",
    "    print('STEP 2. calc_CPS() is complete.\\n')\n",
    "\n",
    "    #Visual check correct baseline-subtracted file is being used.\n",
    "    print('Check that these are correct time-series: \\n')\n",
    "\n",
    "    print_chrono(list(tester.keys()))\n",
    "\n",
    "    if report_choice: \n",
    "        #Fractionation output\n",
    "        fractionation_output(filename)   #ACTUAL CODE\n",
    "        print('STEP 3. fractionation_output() is complete.\\n')\n",
    "\n",
    "    #Processing data\n",
    "    excel_name = str(filename.split('.')[0]) + '_processed.xlsx'    #ACTUAL CODE\n",
    "    files_process_toEXCEL(tester, excel_name, order)    #ACTUAL CODE\n",
    "    print('STEP 4. files_process_toEXCEL() is complete.\\n')\n",
    "\n",
    "        \n",
    "    if report_choice:    \n",
    "        #Visual report of each line scan\n",
    "        report_name = 'Splitstream_' + date + '_results'    #ACTUAL CODE\n",
    "        U_Pb_report(tester, 'SS2_Oct14.pdf', False, report_name)     #ACTUAL CODE; Placeholder .pdf name might cause problems later.\n",
    "        print('\\nSTEP 5. U_Pb_report() is complete.\\n')\n",
    "\n",
    "    #This is a time-consuming function\n",
    "     \n",
    "    \n",
    "    isoplot = corr_to_excel(tester, date, standard)  #ACTUAL CODE            \n",
    "    \n",
    "    export_isoplot(filename, isoplot, date)\n",
    "    print('STEP 9. export_isoplot() is complete.\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "### Functions for bracketing: bracket(), order_to_brackets(), file_to_blockBrackets(), file_Bracket_to_output()\n",
    "\n",
    "def bracket(order, input_std = 'MKED-1'):\n",
    "    '''Function for creating block-brackets based on chronologic order of LASS-ICPMS run and selected bracketing standard. Outputs list of indices for brackets.'''\n",
    "    idx_list = []\n",
    "    for idx in range(len(order)):\n",
    "        if input_std in order[idx]:\n",
    "            idx_list.append(idx)\n",
    "\n",
    "    groups_mked = []\n",
    "    group_list = []\n",
    "    #print(idx_list)\n",
    "    for entry in idx_list:\n",
    "        #print(entry)\n",
    "        if len(group_list) == 0:\n",
    "            #print(entry)\n",
    "            group_list.append(entry)\n",
    "            continue\n",
    "        if entry == idx_list[-1]:\n",
    "            group_list.append(entry)\n",
    "            groups_mked.append(group_list)\n",
    "            continue\n",
    "        if (entry - 1) == group_list[-1]:\n",
    "            #print(entry)\n",
    "            group_list.append(entry)\n",
    "        \n",
    "        else:\n",
    "            groups_mked.append(group_list)\n",
    "            #print(entry)\n",
    "            group_list = [entry]\n",
    "    #print(groups_mked)\n",
    "    brack_list = []\n",
    "\n",
    "    for idx in range(len(groups_mked) - 1):\n",
    "\n",
    "        bracket = [groups_mked[idx][0],groups_mked[idx + 1][-1]]\n",
    "        brack_list.append(bracket)\n",
    "        \n",
    "    full_bracket = [groups_mked[0][0],groups_mked[-1][-1]]\n",
    "    print('full bracket = ', full_bracket)\n",
    "    return brack_list, full_bracket  \n",
    "\n",
    "def order_to_brackets(order, input_std = 'MKED-1'):\n",
    "    '''Takes in the chronologic order and primary standard, and chops the full analysis list into block-brackets. Outputs dict of each bracket.'''\n",
    "    brackets, full_bracket = bracket(order, input_std)\n",
    "    \n",
    "    brack_dict = {}\n",
    "\n",
    "    for idx in range(len(brackets)):\n",
    "        #print(idx, len(brackets))\n",
    "        name = 'bracket_' + str(idx + 1)\n",
    "        \n",
    "        if idx == 0:\n",
    "            \n",
    "            brack_dict[name] = order[0:(brackets[idx][1] + 1)]\n",
    "            \n",
    "        elif idx == (len(brackets) - 1):\n",
    "            #print('DEBUG')\n",
    "            brack_dict[name] = order[brackets[idx][0]:]\n",
    "            #print(brack_dict[name])\n",
    "        else:\n",
    "            brack_dict[name] = order[brackets[idx][0]:(brackets[idx][1] + 1)]\n",
    "        \n",
    "    #full_brack = order[full_bracket[0]:(full_bracket[1]+1)]\n",
    "    #print(brack_dict)\n",
    "    return brack_dict\n",
    "\n",
    "def file_to_blockBrackets(filename, order, input_std = 'MKED-1'):\n",
    "    '''Function to take .xlsx time-series file and export a dictionary of the different block-brackets based on input chronologic order and primary standard.'''\n",
    "    test_df = read_np2_timeseries(filename)   #ACTUAL CODE\n",
    "    \n",
    "    stat_test_order = statistics_NP2(test_df).index.values.tolist()   \n",
    "\n",
    "    new_order = stat_test_order\n",
    "    \n",
    "    \n",
    "    brack_dict = order_to_brackets(order, input_std)\n",
    "    \n",
    "    #test_df_brack = {key: test_df[key] for key in full_bracket}\n",
    "    \n",
    "    \n",
    "    bracket_df_dict = {}\n",
    "    \n",
    "    for entry in brack_dict:\n",
    "        \n",
    "        bracket_df_dict[entry] = {key: test_df[key] for key in brack_dict[entry]}\n",
    "\n",
    "    return bracket_df_dict, new_order\n",
    "\n",
    "def file_Bracket_to_output(filename, date, order, primary_std, Pb_corr = False, debug = False):\n",
    "    ''' Just combines all of the steps for clock-bracket data reduction.'''\n",
    "    print(f'File chosen = {filename} \\n\\nDate chosen = {date} \\n\\nChronologic order defined as: ')\n",
    "    print_chrono(order)\n",
    "    brack_df_dict, new_order = file_to_blockBrackets(filename, order, primary_std)\n",
    "    global tester\n",
    "    \n",
    "    #Getting excess variance for all of the standard analyses\n",
    "    \n",
    "    test_df = read_np2_timeseries(filename)   #ACTUAL CODE\n",
    "    tester = calc_CPS(test_df)     #ACTUAL CODE\n",
    "    #Processing data\n",
    "    excel_name = str(filename.split('.')[0]) + '_processed.xlsx'    #ACTUAL CODE\n",
    "    files_process_toEXCEL(tester, excel_name, order)    #ACTUAL CODE\n",
    "    \n",
    "    print('Primary Standard= ', primary_std)\n",
    "    print('Error minimization rejection percentage= ', 0)\n",
    "    \n",
    "    excV_dict = stat_to_excV(tester, primary_std, reject_percentage = 0)\n",
    "    \n",
    "    print('\\nSTEP 1. stat_to_excV() is complete.\\n')\n",
    "    \n",
    "\n",
    "    ### Make loop that does each bracket\n",
    "    print('STEP 2. file_to_blockBrackets() is complete.\\n')\n",
    "\n",
    "    concat_export_full = []\n",
    "    concat_export_group = []\n",
    "    concat_plot_full = []\n",
    "    concat_plot_group = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for bracket in brack_df_dict:\n",
    "        test_df = brack_df_dict[bracket]\n",
    "        \n",
    "        tester = calc_CPS(test_df)     #ACTUAL CODE\n",
    "\n",
    "        print('\\nSTEP 3. calc_CPS() is complete for ' + bracket + '.\\n')\n",
    "\n",
    "        if primary_std == 'SRM NIST 612':\n",
    "            std_entry = '612'\n",
    "        elif primary_std == 'BLR-1':\n",
    "            std_entry = 'BLR-1'\n",
    "        else:\n",
    "            std_entry = 'ttn'\n",
    "        \n",
    "   \n",
    "        ttn_zero_full, ttn_zero_groups, ttn_zero_full_plot, ttn_zero_groups_plot  = stat_rank_and_correct(tester, 0, std_entry, excV_dict)\n",
    "\n",
    "        concat_export_full.append(ttn_zero_full)\n",
    "        concat_export_group.append(ttn_zero_groups)\n",
    "        concat_plot_full.append(ttn_zero_full_plot)\n",
    "        concat_plot_group.append(ttn_zero_groups_plot)   \n",
    "    \n",
    "    conc_export_full = pd.concat(concat_export_full)\n",
    "    conc_export_group = pd.concat(concat_export_group)\n",
    "    conc_plot_full = pd.concat(concat_plot_full)\n",
    "    conc_plot_group = pd.concat(concat_plot_group)\n",
    "    \n",
    "    export_dict = {std_entry + '_zero_full': conc_export_full[~conc_export_full.index.duplicated(keep='first')], \n",
    "               std_entry + '_zero_groups': conc_export_group.sort_index(ascending=False),\n",
    "              }\n",
    "\n",
    "    export_corr_full(export_dict, date, True)\n",
    "    \n",
    "    if Pb_corr:\n",
    "        plot_dict = { std_entry + '_zero_full': calc_all_ages_corr207(conc_plot_full), \n",
    "               std_entry + '_zero_groups': conc_plot_group.sort_index(ascending=False),\n",
    "              }\n",
    "    else:\n",
    "        plot_dict = { std_entry + '_zero_full': calc_all_ages(conc_plot_full), \n",
    "               std_entry + '_zero_groups': conc_plot_group.sort_index(ascending=False),\n",
    "              }\n",
    "\n",
    "    print('STEP 4. Separate brackets have been concatenated.\\n') \n",
    "\n",
    "   \n",
    "    print('STEP 5. export_corr_full() is complete.\\n')\n",
    "    export_corr_plot(plot_dict, date, True)\n",
    "    print('STEP 6. export_corr_plot() is complete.\\n')\n",
    "\n",
    "    isoplot = plot_dict[std_entry + '_zero_full']\n",
    "    isoplot_new = isoplot[~isoplot.index.duplicated(keep='first')]\n",
    "    \n",
    "    #print(new_order)\n",
    "    new_list = []\n",
    "    for entry in new_order:\n",
    "        if entry not in isoplot_new.index.values.tolist():\n",
    "            continue\n",
    "        else:\n",
    "            new_list.append(entry)\n",
    "    isoplot_new = isoplot_new.loc[new_list]\n",
    "    \n",
    "    export_isoplot(filename, isoplot_new, date, True, False)\n",
    "    print('STEP 7. export_isoplot() is complete.\\n')\n",
    "    \n",
    "    if debug:\n",
    "        return isoplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     3,
     201,
     322,
     575
    ]
   },
   "outputs": [],
   "source": [
    "#Regression Nonsense\n",
    "\n",
    "\n",
    "def regress_indiv2(tester, sample_name, choice = False, start_clip = 0, end_clip = 40, date = ''):\n",
    "    '''Does a linear regression for each individual analysis and outputs a dictionary with mean, intercept, slope, coefficient of determination, and (intercept/average).'''\n",
    "    sample = tester[sample_name]\n",
    "    \n",
    "    new_string = sample_name + '_' + date\n",
    "    \n",
    "    regress_data = sample[['Elapsed Time', '206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "    x_orig = np.array(regress_data['Elapsed Time']).reshape((-1, 1))\n",
    "    mked_full = regress_data[regress_data['Elapsed Time'] > start_clip]\n",
    "    mked_full = mked_full[mked_full['Elapsed Time'] < end_clip]\n",
    "    x = np.array(mked_full['Elapsed Time']).reshape((-1, 1))\n",
    "    \n",
    "    \n",
    "    # PLot 1 #\n",
    "    y = mked_full['206/238']\n",
    "\n",
    "    model = LinearRegression().fit(x, y)\n",
    "\n",
    "    model_dict = {}\n",
    "    model_dict['average 206/238'] = y.mean()\n",
    "    model_dict['intercept 206/238'] = model.intercept_\n",
    "    model_dict['slope 206/238'] = float(model.coef_)\n",
    "    model_dict['coefficient of determination 206/238'] = model.score(x, y)\n",
    "    model_dict['intercept / average 206/238'] =  model_dict['intercept 206/238'] / model_dict['average 206/238']\n",
    "    \n",
    "    y_pred = model.predict(x_orig)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, sharex = True, figsize = (24, 8))\n",
    "    fig.suptitle(new_string, fontsize=24)\n",
    "    \n",
    "    \n",
    "    ax[0].scatter(x,y, color=\"black\")\n",
    "    ax[0].plot(x_orig, y_pred, color='blue', linewidth = 3)\n",
    "    ax[0].axhline(y= y.mean(), color='green', linestyle='-')\n",
    "    ax[0].set_title('206/238', fontsize = 20)\n",
    "    \n",
    "    # these are matplotlib.patch.Patch properties\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax[0].text(0.97, 0.97, sample_name, transform=ax[0].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    equation = 'y = ' + str(round_sig(model_dict['slope 206/238'], 3)) + 'x + ' + str(round_sig(model_dict['intercept 206/238']))\n",
    "    ax[0].text(0.97, 0.88, equation, transform=ax[0].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    # Plot 1 END #\n",
    "    \n",
    "    # PLot 2 #\n",
    "    y2 = mked_full['208/232']\n",
    "\n",
    "    model2 = LinearRegression().fit(x, y2)\n",
    "\n",
    "    model_dict2 = {}\n",
    "    model_dict2['average 208/232'] = y2.mean()\n",
    "    model_dict2['intercept 208/232'] = model2.intercept_\n",
    "    model_dict2['slope 208/232'] = float(model2.coef_)\n",
    "    model_dict2['coefficient of determination 208/232'] = model2.score(x, y2)\n",
    "    model_dict2['intercept / average 208/232'] =  model_dict2['intercept 208/232'] / model_dict2['average 208/232']\n",
    "    \n",
    "    y_pred2 = model2.predict(x_orig)\n",
    "\n",
    "    \n",
    "    ax[1].scatter(x,y2, color=\"black\")\n",
    "    ax[1].plot(x_orig, y_pred2, color='blue', linewidth = 3)\n",
    "    ax[1].axhline(y = y2.mean(), color='green', linestyle='-')\n",
    "    ax[1].set_title('208/232', fontsize = 20)\n",
    "    \n",
    "    # these are matplotlib.patch.Patch properties\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax[1].text(0.97, 0.97, sample_name, transform=ax[1].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    equation = 'y = ' + str(round_sig(model_dict2['slope 208/232'], 3)) + 'x + ' + str(round_sig(model_dict2['intercept 208/232']))\n",
    "    ax[1].text(0.97, 0.88, equation, transform=ax[1].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    # Plot 2 END #\n",
    "    \n",
    "    \n",
    "    plt.xlim([0, end_clip])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    MYDIR = (\"Regression Figures\" + date)\n",
    "    CHECK_FOLDER = os.path.isdir(MYDIR)\n",
    "\n",
    "    # If folder doesn't exist, then create it.\n",
    "    if not CHECK_FOLDER:\n",
    "        os.makedirs(MYDIR)\n",
    "        #print(\"created folder : \", MYDIR)\n",
    "    \n",
    "    #new_string = sample.replace('time series data', '').rstrip()\n",
    "    \n",
    "    filename = os.path.join(MYDIR, new_string + '.pdf')\n",
    "    plt.savefig(filename)\n",
    "    print('Plot for ', new_string, \" is complete.\")\n",
    "    \n",
    "    if choice == False:\n",
    "        plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #plt.close()\n",
    "    \n",
    "    #if choice:\n",
    "        #print('\\nRegression for', sample_name)\n",
    "        #print('average:', model_dict['average'])\n",
    "        #print('intercept:', new)\n",
    "        #print('slope:', float(model.coef_))\n",
    "       #print('coefficient of determination:', model.score(x, y))\n",
    "       #print('intercept / average', new/orig)\n",
    "    \n",
    "    return model_dict, model_dict2\n",
    "\n",
    "\n",
    "### Code for outputting full regression report\n",
    "\n",
    "# tester_list = [tester,tester2, tester3,tester4]\n",
    "# dates = ['Nov16', 'Nov17','Nov22','Nov23']\n",
    "\n",
    "\n",
    "\n",
    "# full_dict = {}\n",
    "# for i in range(len(tester_list)):\n",
    "# #     if i == 1:\n",
    "# #         break\n",
    "        \n",
    "#     int_dict_206 = {}\n",
    "#     int_dict_208 = {}\n",
    "#     z_dict = {}\n",
    "#     testy = tester_list[i]\n",
    "    \n",
    "#     output_name = 'Regress_' + dates[i]\n",
    "#     #MYDIR = (\"Regression Figures\"+dates[i])\n",
    "#     #mergedObject = PdfFileMerger()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     for sample in testy:\n",
    "\n",
    "        \n",
    "#         first, second  = regress_indiv2(testy, sample, choice = False, start_clip = 2, date = dates[i])\n",
    "#         int_dict_206[sample] = first\n",
    "#         int_dict_208[sample] = second\n",
    "#         #fit = model.fit(x,y)\n",
    "    \n",
    "#         z_dict[sample] = {**first, **second}\n",
    "    \n",
    "\n",
    "#         new_string = sample + '_' + dates[i]\n",
    "#         #filename = os.path.join(MYDIR, new_string + '.pdf')\n",
    "\n",
    "#         #mergedObject.append(PdfFileReader(filename, 'rb'))\n",
    "\n",
    "#     if '.pdf' in output_name:\n",
    "#         pass\n",
    "#     else:\n",
    "#         output_name = output_name + '.pdf'\n",
    "    \n",
    "#     #output_name = \"U-Pb_output.pdf\"  \n",
    "#     #mergedObject.write(output_name)\n",
    "\n",
    "#     #print(f'PDF file named: {output_name} is complete.')    \n",
    "\n",
    "#     full_dict[dates[i]] = z_dict\n",
    "        \n",
    "        \n",
    "# nov16 = pd.DataFrame(full_dict['Nov16']).transpose()\n",
    "# nov17 = pd.DataFrame(full_dict['Nov17']).transpose()\n",
    "# nov22 = pd.DataFrame(full_dict['Nov22']).transpose()\n",
    "# nov23 = pd.DataFrame(full_dict['Nov23']).transpose()\n",
    "# nov_dict = {'nov16':nov16,'nov17': nov17,'nov22': nov22,'nov23': nov23}\n",
    "\n",
    "\n",
    "# excel_name = 'regress1.xlsx'\n",
    "# with pd.ExcelWriter(excel_name, engine = 'xlsxwriter') as writer:\n",
    "#       # Get the xlsxwriter workbook objects.\n",
    "#     workbook  = writer.book\n",
    "    \n",
    "#     for sheet in nov_dict:\n",
    "#         nov_dict[sheet].to_excel(writer, sheet_name = sheet, index = True)\n",
    "\n",
    "#         worksheet = writer.sheets[sheet]  # pull worksheet object\n",
    "\n",
    "#         for idx in range(20):\n",
    "#             worksheet.set_column(idx, idx, 22)\n",
    "#         worksheet.freeze_panes(1,1)\n",
    "        \n",
    "        \n",
    "def regress_compare(tester, std1, std2, clip1 = 0, clip2  = 0, endclip = 35, choice = False):    \n",
    "    '''This function takes in the output from calc_CPS(), and two selected sample groups, plus clip positions for start and end. It will plot things.'''\n",
    "    ###MKED###\n",
    "    #start_clip = 2\n",
    "    mked = group_samples2(tester)[std1]\n",
    "\n",
    "    tester_list = []\n",
    "    for entry in mked:\n",
    "        tester_list.append(tester[entry])\n",
    "\n",
    "    mked_full = pd.concat(tester_list)\n",
    "    regress_data = mked_full[['Elapsed Time', '206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "\n",
    "    x_orig = np.array(mked_full['Elapsed Time']).reshape((-1, 1))\n",
    "\n",
    "    ### Regression Math ###\n",
    "    #start_clip = 1.0\n",
    "    mked_full = regress_data[regress_data['Elapsed Time'] > clip1]\n",
    "\n",
    "    mked_full = mked_full[mked_full['Elapsed Time'] < endclip]\n",
    "    \n",
    "    x = np.array(mked_full['Elapsed Time']).reshape((-1, 1))\n",
    "    y = mked_full['206/238']\n",
    "    model = LinearRegression().fit(x, y)\n",
    "    model_dict = {}\n",
    "    model_dict['average'] = y.mean()\n",
    "    model_dict['intercept'] = model.intercept_\n",
    "    model_dict['slope'] = float(model.coef_)\n",
    "    model_dict['coefficient of determination'] = model.score(x, y)\n",
    "\n",
    "    fit = model.fit(x,y)\n",
    "    y_pred = model.predict(x_orig)\n",
    "\n",
    "    ### MKED ###\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, sharex = True, figsize = (24, 6))\n",
    "    ax[0].scatter(x,y, color=\"black\")\n",
    "    ax[0].plot(x_orig, y_pred, color='blue', linewidth = 3)\n",
    "    \n",
    "    # these are matplotlib.patch.Patch properties\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax[0].text(0.97, 0.97, std1, transform=ax[0].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    equation = 'y = ' + str(round_sig(model_dict['slope'], 3)) + 'x + ' + str(round_sig(model_dict['intercept']))\n",
    "    ax[0].text(0.97, 0.88, equation, transform=ax[0].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "\n",
    "    ### BLR-1 ###\n",
    "    #start_clip = 8\n",
    "    blr = group_samples2(tester)[std2]\n",
    "\n",
    "    tester_list = []\n",
    "    for entry in blr:\n",
    "        tester_list.append(tester[entry])\n",
    "\n",
    "    blr_full = pd.concat(tester_list)\n",
    "    regress_data = blr_full[['Elapsed Time', '206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "\n",
    "\n",
    "    x_orig2 = np.array(blr_full['Elapsed Time']).reshape((-1, 1))\n",
    "\n",
    "    ### Regression Math ###\n",
    "    #start_clip = 1.0\n",
    "    blr_full = regress_data[regress_data['Elapsed Time'] > clip2]\n",
    "    blr_full = blr_full[blr_full['Elapsed Time'] < endclip]\n",
    "    x2 = np.array(blr_full['Elapsed Time']).reshape((-1, 1))\n",
    "    y2 = blr_full['206/238']\n",
    "    model = LinearRegression().fit(x2, y2)\n",
    "\n",
    "    model_dict2 = {}\n",
    "    model_dict2['average'] = y2.mean()\n",
    "    model_dict2['intercept'] = model.intercept_\n",
    "    model_dict2['slope'] = float(model.coef_)\n",
    "    model_dict2['coefficient of determination'] = model.score(x, y)\n",
    "\n",
    "\n",
    "    fit2 = model.fit(x2,y2)\n",
    "    y_pred2 = model.predict(x_orig2)\n",
    "\n",
    "    ### BLR-1 ###\n",
    "    ax[1].scatter(x2,y2, color=\"black\")\n",
    "    ax[1].plot(x_orig2, y_pred2, color='blue', linewidth = 3)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax[1].text(0.97, 0.97, std2, transform=ax[1].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    equation2 = 'y = ' + str(round_sig(model_dict2['slope'], 3)) + 'x + ' + str(round_sig(model_dict2['intercept']))\n",
    "    ax[1].text(0.97, 0.88, equation2, transform=ax[1].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    \n",
    "    y3 = y_pred2 / (y_pred[:len(y_pred2)])\n",
    "    y4 = model_dict2['intercept'] / (y_pred[:len(y_pred2)])\n",
    "\n",
    "    ax[2].plot(y3[0:50], color='green', linewidth = 3)\n",
    "    ax[2].plot(y4[0:50], color='blue', linewidth = 3)\n",
    "    \n",
    "    \n",
    "#     #Still in Development\n",
    "#     ax[0].spines['left'].set_position(('data', 0))\n",
    "#     ax[1].spines['left'].set_position(('data', 0))\n",
    "#     ax[2].spines['left'].set_position(('data', 0))\n",
    "    \n",
    "    ###\n",
    "    #print(equation)\n",
    "    #print(model_dict)\n",
    "    #print(model_dict2)\n",
    "    print('int2/int1: ',model_dict2['intercept']/model_dict['intercept'])\n",
    "    print('int2/avg1 ', model_dict2['intercept']/model_dict['average'])\n",
    "    print(std1, ' chopped at: ',clip1, '   ', model_dict2['intercept']/y_pred[clip1])\n",
    "    print()\n",
    "    if choice:\n",
    "        return model_dict, model_dict2\n",
    "    \n",
    "    \n",
    "################################\n",
    "def regress_compare2(tester, std1, std2,std3, std4, clip1 = 0, clip2  = 0, clip3 = 0, clip4 = 0, endclip = 35, choice = False):    \n",
    "    '''This function takes in the output from calc_CPS(), and plots 4 standards with std1 = primary, plus clip positions for start and end. It will plot things.'''\n",
    "    \n",
    "   \n",
    "    #d1 = tester['MKED-1 1.10']['238_CPS']\n",
    "\n",
    "    #dt = 200\n",
    "    #dt_206_fac = (1 / (1 - t1*(dt/1E9)))\n",
    "    \n",
    "    \n",
    "    ###MKED###\n",
    "    #start_clip = 2\n",
    "    mked = group_samples2(tester)[std1]\n",
    "\n",
    "    tester_list = []\n",
    "    for entry in mked:\n",
    "        tester_list.append(tester[entry])\n",
    "\n",
    "    mked_full = pd.concat(tester_list)\n",
    "    regress_data = mked_full[['Elapsed Time','206_CPS', '206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "\n",
    "    x_orig = np.array(mked_full['Elapsed Time']).reshape((-1, 1))\n",
    "\n",
    "    ### Regression Math ###\n",
    "    #start_clip = 1.0\n",
    "    mked_full = regress_data[regress_data['Elapsed Time'] > clip1]\n",
    "\n",
    "    mked_full = mked_full[mked_full['Elapsed Time'] < endclip]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x = np.array(mked_full['Elapsed Time']).reshape((-1, 1))\n",
    "    y = mked_full['206/238']\n",
    "    t1 = mked_full['206_CPS']\n",
    "    #y = y1 * (1 / (1 - t1*(dt/1E9)))\n",
    "    model = LinearRegression().fit(x, y)\n",
    "    model_dict = {}\n",
    "    model_dict['average'] = y.mean()\n",
    "    model_dict['intercept'] = model.intercept_\n",
    "    model_dict['slope'] = float(model.coef_)\n",
    "    model_dict['coefficient of determination'] = model.score(x, y)\n",
    "\n",
    "    fit = model.fit(x,y)\n",
    "    y_pred = model.predict(x_orig)\n",
    "\n",
    "    ### MKED ###\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, sharex = True, figsize = (28, 6))\n",
    "    ax[0].scatter(x,y, color=\"black\")\n",
    "    ax[0].plot(x_orig, y_pred, color='blue', linewidth = 3)\n",
    "    ax[0].axhline(y= y.mean(), color='green', linestyle='-')\n",
    "    \n",
    "    \n",
    "    # these are matplotlib.patch.Patch properties\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax[0].text(0.97, 0.97, std1, transform=ax[0].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    equation = 'y = ' + str(round_sig(model_dict['slope'], 3)) + 'x + ' + str(round_sig(model_dict['intercept']))\n",
    "    ax[0].text(0.97, 0.88, equation, transform=ax[0].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "\n",
    "    ### BLR-1 ###\n",
    "    #start_clip = 8\n",
    "    blr = group_samples2(tester)[std2]\n",
    "\n",
    "    tester_list = []\n",
    "    for entry in blr:\n",
    "        tester_list.append(tester[entry])\n",
    "\n",
    "    blr_full = pd.concat(tester_list)\n",
    "    regress_data = blr_full[['Elapsed Time','206_CPS', '206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "\n",
    "\n",
    "    x_orig2 = np.array(blr_full['Elapsed Time']).reshape((-1, 1))\n",
    "\n",
    "    ### Regression Math ###\n",
    "    #start_clip = 1.0\n",
    "    blr_full = regress_data[regress_data['Elapsed Time'] > clip2]\n",
    "    blr_full = blr_full[blr_full['Elapsed Time'] < endclip]\n",
    "    x2 = np.array(blr_full['Elapsed Time']).reshape((-1, 1))\n",
    "    y2 = blr_full['206/238']\n",
    "    model = LinearRegression().fit(x2, y2)\n",
    "\n",
    "    model_dict2 = {}\n",
    "    model_dict2['average'] = y2.mean()\n",
    "    model_dict2['intercept'] = model.intercept_\n",
    "    model_dict2['slope'] = float(model.coef_)\n",
    "    model_dict2['coefficient of determination'] = model.score(x, y)\n",
    "\n",
    "\n",
    "    fit2 = model.fit(x2,y2)\n",
    "    y_pred2 = model.predict(x_orig2)\n",
    "\n",
    "    ### BLR-1 ###\n",
    "    ax[1].scatter(x2,y2, color=\"black\")\n",
    "    ax[1].plot(x_orig2, y_pred2, color='blue', linewidth = 3)\n",
    "    ax[1].axhline(y= y2.mean(), color='green', linestyle='-')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # place a text box in upper left in axes coords\n",
    "    ax[1].text(0.97, 0.97, std2, transform=ax[1].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    equation2 = 'y = ' + str(round_sig(model_dict2['slope'], 3)) + 'x + ' + str(round_sig(model_dict2['intercept']))\n",
    "    ax[1].text(0.97, 0.88, equation2, transform=ax[1].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    ### Bancroft ###\n",
    "    #start_clip = 8\n",
    "    banc = group_samples2(tester)[std3]\n",
    "\n",
    "    tester_list = []\n",
    "    for entry in banc:\n",
    "        tester_list.append(tester[entry])\n",
    "\n",
    "    banc_full = pd.concat(tester_list)\n",
    "    regress_data = banc_full[['Elapsed Time','206_CPS', '206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "\n",
    "\n",
    "    x_orig3 = np.array(banc_full['Elapsed Time']).reshape((-1, 1))\n",
    "\n",
    "    ### Regression Math ###\n",
    "    #start_clip = 1.0\n",
    "    banc_full = regress_data[regress_data['Elapsed Time'] > clip3]\n",
    "    banc_full = banc_full[banc_full['Elapsed Time'] < endclip]\n",
    "    x3 = np.array(banc_full['Elapsed Time']).reshape((-1, 1))\n",
    "    y3 = banc_full['206/238']\n",
    "    model = LinearRegression().fit(x3, y3)\n",
    "\n",
    "    model_dict3 = {}\n",
    "    model_dict3['average'] = y3.mean()\n",
    "    model_dict3['intercept'] = model.intercept_\n",
    "    model_dict3['slope'] = float(model.coef_)\n",
    "    model_dict3['coefficient of determination'] = model.score(x3, y3)\n",
    "\n",
    "\n",
    "    fit3 = model.fit(x3,y3)\n",
    "    y_pred3 = model.predict(x_orig3)\n",
    "\n",
    "    ### Bancroft ###\n",
    "    ax[2].scatter(x3,y3, color=\"black\")\n",
    "    ax[2].plot(x_orig3, y_pred3, color='blue', linewidth = 3)\n",
    "    ax[2].axhline(y= y3.mean(), color='green', linestyle='-')\n",
    "    \n",
    "    # place a text box in upper left in axes coords\n",
    "    ax[2].text(0.97, 0.97, std3, transform=ax[2].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    equation2 = 'y = ' + str(round_sig(model_dict3['slope'], 3)) + 'x + ' + str(round_sig(model_dict3['intercept']))\n",
    "    ax[2].text(0.97, 0.88, equation2, transform=ax[2].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    ### Bear Lake ###\n",
    "    #start_clip = 8\n",
    "    bear = group_samples2(tester)[std4]\n",
    "\n",
    "    tester_list = []\n",
    "    for entry in bear:\n",
    "        tester_list.append(tester[entry])\n",
    "\n",
    "    bear_full = pd.concat(tester_list)\n",
    "    regress_data = bear_full[['Elapsed Time', '206_CPS','206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "\n",
    "    x_orig4 = np.array(bear_full['Elapsed Time']).reshape((-1, 1))\n",
    "\n",
    "    ### Regression Math ###\n",
    "    #start_clip = 1.0\n",
    "    bear_full = regress_data[regress_data['Elapsed Time'] > clip4]\n",
    "    bear_full = bear_full[bear_full['Elapsed Time'] < endclip]\n",
    "    x4 = np.array(bear_full['Elapsed Time']).reshape((-1, 1))\n",
    "    y4 = bear_full['206/238']\n",
    "    model = LinearRegression().fit(x4, y4)\n",
    "\n",
    "    model_dict4 = {}\n",
    "    model_dict4['average'] = y4.mean()\n",
    "    model_dict4['intercept'] = model.intercept_\n",
    "    model_dict4['slope'] = float(model.coef_)\n",
    "    model_dict4['coefficient of determination'] = model.score(x4, y4)\n",
    "\n",
    "\n",
    "    fit4 = model.fit(x4,y4)\n",
    "    y_pred4 = model.predict(x_orig4)\n",
    "\n",
    "    ### Bear Lake ###\n",
    "    ax[3].scatter(x4,y4, color=\"black\")\n",
    "    ax[3].plot(x_orig4, y_pred4, color='blue', linewidth = 3)\n",
    "    \n",
    "    \n",
    "    ax[3].axhline(y= y4.mean(), color='green', linestyle='-')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #ax[3].plot(y4.mean(), color='green', linewidth = 3)\n",
    "    \n",
    "    # place a text box in upper left in axes coords\n",
    "    ax[3].text(0.97, 0.97, std4, transform=ax[3].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    equation2 = 'y = ' + str(round_sig(model_dict4['slope'], 3)) + 'x + ' + str(round_sig(model_dict4['intercept']))\n",
    "    ax[3].text(0.97, 0.88, equation2, transform=ax[3].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     y3 = y_pred2 / (y_pred[:len(y_pred2)])\n",
    "#     y4 = model_dict2['intercept'] / (y_pred[:len(y_pred2)])\n",
    "\n",
    "#     ax[2].plot(y3[0:50], color='green', linewidth = 3)\n",
    "#     ax[2].plot(y4[0:50], color='blue', linewidth = 3)\n",
    "    \n",
    "    \n",
    "#     #Still in Development\n",
    "#     ax[0].spines['left'].set_position(('data', 0))\n",
    "#     ax[1].spines['left'].set_position(('data', 0))\n",
    "#     ax[2].spines['left'].set_position(('data', 0))\n",
    "    \n",
    "    ###\n",
    "    #print(equation)\n",
    "    #print(model_dict)\n",
    "    #print(model_dict2)\n",
    "    space = '                          '\n",
    "   \n",
    "    blr_val = model_dict2['intercept']/model_dict['intercept'] * published('MKED-1', '206/238')\n",
    "    banc_val = model_dict3['intercept']/model_dict['intercept'] * published('MKED-1', '206/238')\n",
    "    bear_val = model_dict4['intercept']/model_dict['intercept'] * published('MKED-1', '206/238')\n",
    "    \n",
    "    blr_age = round_sig(calc_t68(blr_val) /1E6, 6)\n",
    "    banc_age = round_sig(calc_t68(banc_val) /1E6, 6)\n",
    "    bear_age = round_sig(calc_t68(bear_val) /1E6, 6)\n",
    "    \n",
    "    plt.xlim([0, endclip])\n",
    "    \n",
    "    \n",
    "    print('int/MKED-1 int: ',space,round_sig(blr_val,6),space,round_sig(banc_val,6),space,round_sig(bear_val,6) )\n",
    "    print('Ages [Ma]:           ',space,round_sig(blr_age,6),space,round_sig(banc_age,6),space,round_sig(bear_age,6) )\n",
    "    \n",
    "    #print('int2/MKED-1 int: ',model_dict3['intercept']/model_dict['intercept'], )\n",
    "    #print('int3/MKED-1 int: ',model_dict4['intercept']/model_dict['intercept'], )\n",
    "    print() \n",
    "    #print('int2/avg1 ', model_dict2['intercept']/model_dict['average'])\n",
    "    #print(std1, ' chopped at: ',clip1, '   ', model_dict2['intercept']/y_pred[clip1])\n",
    "    #print()\n",
    "    if choice:\n",
    "        return model_dict, model_dict2\n",
    "\n",
    "    \n",
    "def regress_compare3(tester, std1, std2,std3, std4, clip1 = 0, clip2  = 0, clip3 = 0, clip4 = 0, endclip = 35, choice = False):    \n",
    "    '''This function takes in the output from calc_CPS(), and plots 4 standards with std1 = primary, plus clip positions for start and end. It incorporates dead time adjustments on 206 and/or 238.'''\n",
    "     #Dead Time tests\n",
    "\n",
    "    dt = 0\n",
    "    #dt_206_fac = (1 / (1 - t1*(dt/1E9)))\n",
    "    ut = -70\n",
    "    \n",
    "    ###MKED###\n",
    "    #start_clip = 2\n",
    "    mked = group_samples2(tester)[std1]\n",
    "\n",
    "    tester_list = []\n",
    "    for entry in mked:\n",
    "        tester_list.append(tester[entry])\n",
    "\n",
    "    mked_full = pd.concat(tester_list)\n",
    "    regress_data = mked_full[['Elapsed Time','206_CPS','238_CPS', '206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "\n",
    "    x_orig = np.array(mked_full['Elapsed Time']).reshape((-1, 1))\n",
    "\n",
    "    ### Regression Math ###\n",
    "    #start_clip = 1.0\n",
    "    mked_full = regress_data[regress_data['Elapsed Time'] > clip1]\n",
    "\n",
    "    mked_full = mked_full[mked_full['Elapsed Time'] < endclip]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x = np.array(mked_full['Elapsed Time']).reshape((-1, 1))\n",
    "    y1 = mked_full['206/238']\n",
    "    t1 = mked_full['206_CPS']#DT\n",
    "    u1 = mked_full['238_CPS']#DT\n",
    "    y = y1 * (1 / (1 - t1*(dt/1E9))) * (1 /(1 / (1 - u1*(ut/1E9))) )   #DT\n",
    "    model = LinearRegression().fit(x, y)\n",
    "    model_dict = {}\n",
    "    model_dict['average'] = y.mean()\n",
    "    model_dict['intercept'] = model.intercept_\n",
    "    model_dict['slope'] = float(model.coef_)\n",
    "    model_dict['coefficient of determination'] = model.score(x, y)\n",
    "\n",
    "    fit = model.fit(x,y)\n",
    "    y_pred = model.predict(x_orig)\n",
    "\n",
    "    ### MKED ###\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, sharex = True, figsize = (28, 6))\n",
    "    ax[0].scatter(x,y, color=\"black\")\n",
    "    ax[0].plot(x_orig, y_pred, color='blue', linewidth = 3)\n",
    "    ax[0].axhline(y= y.mean(), color='green', linestyle='-')\n",
    "    \n",
    "    \n",
    "    # these are matplotlib.patch.Patch properties\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax[0].text(0.97, 0.97, std1, transform=ax[0].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    equation = 'y = ' + str(round_sig(model_dict['slope'], 3)) + 'x + ' + str(round_sig(model_dict['intercept']))\n",
    "    ax[0].text(0.97, 0.88, equation, transform=ax[0].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "\n",
    "    ### BLR-1 ###\n",
    "    #start_clip = 8\n",
    "    blr = group_samples2(tester)[std2]\n",
    "\n",
    "    tester_list = []\n",
    "    for entry in blr:\n",
    "        tester_list.append(tester[entry])\n",
    "\n",
    "    blr_full = pd.concat(tester_list)\n",
    "    regress_data = blr_full[['Elapsed Time','206_CPS','238_CPS', '206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "\n",
    "\n",
    "    x_orig2 = np.array(blr_full['Elapsed Time']).reshape((-1, 1))\n",
    "\n",
    "    ### Regression Math ###\n",
    "    #start_clip = 1.0\n",
    "    blr_full = regress_data[regress_data['Elapsed Time'] > clip2]\n",
    "    blr_full = blr_full[blr_full['Elapsed Time'] < endclip]\n",
    "    x2 = np.array(blr_full['Elapsed Time']).reshape((-1, 1))\n",
    "    \n",
    "  \n",
    "    \n",
    "    y2 = blr_full['206/238']\n",
    "    t2 = blr_full['206_CPS']      #DT\n",
    "    #y2 = y2 * (1 / (1 - t2*(dt/1E9)))    #DT\n",
    "    u2 = blr_full['238_CPS']#DT\n",
    "    y2 = y2 * (1 / (1 - t2*(dt/1E9))) * (1 /(1 / (1 - u2*(ut/1E9))) )   #DT\n",
    "    \n",
    "    \n",
    "    model = LinearRegression().fit(x2, y2)\n",
    "\n",
    "    model_dict2 = {}\n",
    "    model_dict2['average'] = y2.mean()\n",
    "    model_dict2['intercept'] = model.intercept_\n",
    "    model_dict2['slope'] = float(model.coef_)\n",
    "    model_dict2['coefficient of determination'] = model.score(x, y)\n",
    "\n",
    "\n",
    "    fit2 = model.fit(x2,y2)\n",
    "    y_pred2 = model.predict(x_orig2)\n",
    "\n",
    "    ### BLR-1 ###\n",
    "    ax[1].scatter(x2,y2, color=\"black\")\n",
    "    ax[1].plot(x_orig2, y_pred2, color='blue', linewidth = 3)\n",
    "    ax[1].axhline(y= y2.mean(), color='green', linestyle='-')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # place a text box in upper left in axes coords\n",
    "    ax[1].text(0.97, 0.97, std2, transform=ax[1].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    equation2 = 'y = ' + str(round_sig(model_dict2['slope'], 3)) + 'x + ' + str(round_sig(model_dict2['intercept']))\n",
    "    ax[1].text(0.97, 0.88, equation2, transform=ax[1].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    ### Bancroft ###\n",
    "    #start_clip = 8\n",
    "    banc = group_samples2(tester)[std3]\n",
    "\n",
    "    tester_list = []\n",
    "    for entry in banc:\n",
    "        tester_list.append(tester[entry])\n",
    "\n",
    "    banc_full = pd.concat(tester_list)\n",
    "    regress_data = banc_full[['Elapsed Time','206_CPS', '238_CPS','206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "\n",
    "\n",
    "    x_orig3 = np.array(banc_full['Elapsed Time']).reshape((-1, 1))\n",
    "\n",
    "    ### Regression Math ###\n",
    "    #start_clip = 1.0\n",
    "    banc_full = regress_data[regress_data['Elapsed Time'] > clip3]\n",
    "    banc_full = banc_full[banc_full['Elapsed Time'] < endclip]\n",
    "    x3 = np.array(banc_full['Elapsed Time']).reshape((-1, 1))\n",
    "    \n",
    "    \n",
    "    y3 = banc_full['206/238']\n",
    "    \n",
    "    t3 = banc_full['206_CPS']      #DT\n",
    "    \n",
    "    u3 = banc_full['238_CPS']#DT\n",
    "    y3 = y3 * (1 / (1 - t3*(dt/1E9))) * (1 /(1 / (1 - u3*(ut/1E9))) )   #DT\n",
    "    #y3 = y3 * (1 / (1 - t3*(dt/1E9)))    #DT\n",
    "    \n",
    "    model = LinearRegression().fit(x3, y3)\n",
    "\n",
    "    model_dict3 = {}\n",
    "    model_dict3['average'] = y3.mean()\n",
    "    model_dict3['intercept'] = model.intercept_\n",
    "    model_dict3['slope'] = float(model.coef_)\n",
    "    model_dict3['coefficient of determination'] = model.score(x3, y3)\n",
    "\n",
    "\n",
    "    fit3 = model.fit(x3,y3)\n",
    "    y_pred3 = model.predict(x_orig3)\n",
    "\n",
    "    ### Bancroft ###\n",
    "    ax[2].scatter(x3,y3, color=\"black\")\n",
    "    ax[2].plot(x_orig3, y_pred3, color='blue', linewidth = 3)\n",
    "    ax[2].axhline(y= y3.mean(), color='green', linestyle='-')\n",
    "    \n",
    "    # place a text box in upper left in axes coords\n",
    "    ax[2].text(0.97, 0.97, std3, transform=ax[2].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    equation2 = 'y = ' + str(round_sig(model_dict3['slope'], 3)) + 'x + ' + str(round_sig(model_dict3['intercept']))\n",
    "    ax[2].text(0.97, 0.88, equation2, transform=ax[2].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    ### Bear Lake ###\n",
    "    #start_clip = 8\n",
    "    bear = group_samples2(tester)[std4]\n",
    "\n",
    "    tester_list = []\n",
    "    for entry in bear:\n",
    "        tester_list.append(tester[entry])\n",
    "\n",
    "    bear_full = pd.concat(tester_list)\n",
    "    regress_data = bear_full[['Elapsed Time', '206_CPS','238_CPS','206/238', '208/232', '207/206','208/206', '206/204', '208/204', '207/204', '207/235']]\n",
    "\n",
    "    x_orig4 = np.array(bear_full['Elapsed Time']).reshape((-1, 1))\n",
    "\n",
    "    ### Regression Math ###\n",
    "    #start_clip = 1.0\n",
    "    bear_full = regress_data[regress_data['Elapsed Time'] > clip4]\n",
    "    bear_full = bear_full[bear_full['Elapsed Time'] < endclip]\n",
    "    x4 = np.array(bear_full['Elapsed Time']).reshape((-1, 1))\n",
    "    y4 = bear_full['206/238']\n",
    "    \n",
    "    t4 = bear_full['206_CPS']      #DT\n",
    "    u4 = bear_full['238_CPS']#DT\n",
    "    y4 = y4 * (1 / (1 - t4*(dt/1E9))) * (1 /(1 / (1 - u4*(ut/1E9))) )   #DT\n",
    "    #y4 = y4 * (1 / (1 - t4*(dt/1E9)))    #DT\n",
    "    \n",
    "    model = LinearRegression().fit(x4, y4)\n",
    "\n",
    "    model_dict4 = {}\n",
    "    model_dict4['average'] = y4.mean()\n",
    "    model_dict4['intercept'] = model.intercept_\n",
    "    model_dict4['slope'] = float(model.coef_)\n",
    "    model_dict4['coefficient of determination'] = model.score(x4, y4)\n",
    "\n",
    "\n",
    "    fit4 = model.fit(x4,y4)\n",
    "    y_pred4 = model.predict(x_orig4)\n",
    "\n",
    "    ### Bear Lake ###\n",
    "    ax[3].scatter(x4,y4, color=\"black\")\n",
    "    ax[3].plot(x_orig4, y_pred4, color='blue', linewidth = 3)\n",
    "    \n",
    "    \n",
    "    ax[3].axhline(y= y4.mean(), color='green', linestyle='-')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #ax[3].plot(y4.mean(), color='green', linewidth = 3)\n",
    "    \n",
    "    # place a text box in upper left in axes coords\n",
    "    ax[3].text(0.97, 0.97, std4, transform=ax[3].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    equation2 = 'y = ' + str(round_sig(model_dict4['slope'], 3)) + 'x + ' + str(round_sig(model_dict4['intercept']))\n",
    "    ax[3].text(0.97, 0.88, equation2, transform=ax[3].transAxes, fontsize=14,\n",
    "    verticalalignment='top',horizontalalignment = 'right', bbox=props)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    space = '                          '\n",
    "    blr_val = model_dict2['intercept']/model_dict['intercept'] * published('MKED-1', '206/238')\n",
    "    banc_val = model_dict3['intercept']/model_dict['intercept'] * published('MKED-1', '206/238')\n",
    "    bear_val = model_dict4['intercept']/model_dict['intercept'] * published('MKED-1', '206/238')\n",
    "    \n",
    "    blr_age = round_sig(calc_t68(blr_val) /1E6, 6)\n",
    "    banc_age = round_sig(calc_t68(banc_val) /1E6, 6)\n",
    "    bear_age = round_sig(calc_t68(bear_val) /1E6, 6)\n",
    "    \n",
    "    \n",
    "    plt.xlim([0, 50])\n",
    "    \n",
    "    print('int/MKED-1 int: ',space,round_sig(blr_val,6),space,round_sig(banc_val,6),space,round_sig(bear_val,6) )\n",
    "    \n",
    "    #print('int2/MKED-1 int: ',model_dict3['intercept']/model_dict['intercept'], )\n",
    "    #print('int3/MKED-1 int: ',model_dict4['intercept']/model_dict['intercept'], )\n",
    "    print() \n",
    "    #print('int2/avg1 ', model_dict2['intercept']/model_dict['average'])\n",
    "    #print(std1, ' chopped at: ',clip1, '   ', model_dict2['intercept']/y_pred[clip1])\n",
    "    #print()\n",
    "    if choice:\n",
    "        return model_dict, model_dict2\n",
    "\n",
    "# ### This is the normal regression comparison    \n",
    "# space = '                          '\n",
    "# print('         ','MKED-1 ',space,'BLR-1',space,'Bancroft',space,'Bear Lake')\n",
    "\n",
    "# print('       ','Expected:',space,'0.17921',space,'0.178523',space,'0.1870\\n')\n",
    "\n",
    "# #Nov16, Nov17, Nov22, Nov23\n",
    "\n",
    "\n",
    "\n",
    "# regress_compare2(tester, 'MKED-1', 'BLR-1','Bancroft', 'Bear Lake', clip1 = 2, clip2  =9,clip3 = 7, clip4 = 14, endclip = 34, choice = False)\n",
    "# regress_compare2(tester2, 'MKED-1', 'BLR-1','Bancroft', 'Bear Lake', clip1 = 2, clip2  = 9,clip3 = 7, clip4 = 14, endclip = 34, choice = False)\n",
    "# regress_compare2(tester3, 'MKED-1', 'BLR-1','Bancroft', 'Bear Lake', clip1 = 2, clip2  = 9,clip3 = 7, clip4 = 2, endclip = 34, choice = False)\n",
    "# regress_compare2(tester4, 'MKED-1', 'BLR-1','Bancroft', 'Bear Lake', clip1 = 2, clip2  =9,clip3 = 7, clip4 = 5, endclip = 34, choice = False)\n",
    "\n",
    "\n",
    "# ### This is the (unjustified) deadtime adjustment regression comparisons \n",
    "\n",
    "# space = '                          '\n",
    "# print('         ','MKED-1 ',space,'BLR-1',space,'Bancroft',space,'Bear Lake')\n",
    "\n",
    "# print('       ','Expected:',space,'0.17921',space,'0.178523',space,'0.1870\\n')\n",
    "\n",
    "# #Nov16, Nov17, Nov22, Nov23\n",
    "\n",
    "\n",
    "\n",
    "# regress_compare3(tester, 'MKED-1', 'BLR-1','Bancroft', 'Bear Lake', clip1 = 5, clip2  =8,clip3 = 8, clip4 = 2, endclip = 30, choice = False)\n",
    "# regress_compare3(tester2, 'MKED-1', 'BLR-1','Bancroft', 'Bear Lake', clip1 = 10, clip2  = 8,clip3 = 5, clip4 = 2, endclip = 37, choice = False)\n",
    "# regress_compare3(tester3, 'MKED-1', 'BLR-1','Bancroft', 'Bear Lake', clip1 = 2, clip2  = 10,clip3 = 5, clip4 = 2, endclip = 30, choice = False)\n",
    "# regress_compare3(tester4, 'MKED-1', 'BLR-1','Bancroft', 'Bear Lake', clip1 = 5, clip2  = 5,clip3 = 5, clip4 = 2, endclip = 30, choice = False)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Start Here****\n",
    "\n",
    "Input Filename of Iolite-baseline corrected Time-Series .xlsx, the date, and the order in which things were ablated.\n",
    "\n",
    "Potential work-flow to consider is doing a full-run average, identifying standards that are outliers, removing those, correcting the order, and re-running with block-brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "#Check order in Iolite\n",
    "\n",
    "chrono_order2 = [    #This order is currently manually input based on analysis order - KD 11/3/2021\n",
    " 'SRM NIST 612 1.1',\n",
    " 'SRM NIST 612 1.2',\n",
    " 'SRM NIST 612 1.3',\n",
    " 'SRM NIST 612 1.4',\n",
    " 'SRM NIST 612 1.5',\n",
    " 'MKED-1 1.1',\n",
    " 'MKED-1 1.2',\n",
    " 'MKED-1 1.3',\n",
    " 'MKED-1 1.4',\n",
    " 'MKED-1 1.5',  \n",
    " 'BLR-1 1.1',\n",
    " 'BLR-1 1.2', \n",
    " 'BLR-1 1.3',\n",
    " 'BLR-1 1.4', \n",
    " 'BLR-1 1.5',\n",
    " '95-65_17 1.1',  \n",
    " '95-65_17 1.2',\n",
    " '95-65_17 1.3',    \n",
    " '95-65_18 1.1',\n",
    " '95-65_18 1.2',  \n",
    " '95-65_18 1.3', \n",
    " '95-65_21 1.1',  \n",
    " '95-65_21 1.2',  \n",
    " 'Bancroft 1.1',\n",
    " 'Bancroft 1.2',\n",
    " 'Bancroft 1.3',\n",
    " 'Yates 1.1',\n",
    " 'Yates 1.2', \n",
    " 'Yates 1.3',\n",
    " 'Bear Lake 1.1',\n",
    " 'Bear Lake 1.2', \n",
    " 'Bear Lake 1.3',  \n",
    " 'SRM NIST 612 1.6',\n",
    " 'SRM NIST 612 1.7',\n",
    " 'SRM NIST 612 1.8',\n",
    " 'MKED-1 1.6',\n",
    " 'MKED-1 1.7',\n",
    " #'MKED-1 1.8',\n",
    " 'BLR-1 1.6',\n",
    " 'BLR-1 1.7', \n",
    " 'BLR-1 1.8',\n",
    " '95-65_29 1.1',  \n",
    " '95-65_29 1.2',   \n",
    " '95-65_29 1.3',  \n",
    " '95-65_29 1.4', \n",
    " '95-65_27 1.1',   \n",
    " '95-65_27 1.2',\n",
    " '95-65_27 1.3',     \n",
    " '95-65_28 1.1',\n",
    " '95-65_28 1.2',  \n",
    " '95-65_28 1.3',  \n",
    " '95-65_28 1.4', \n",
    " 'Bancroft 1.4',\n",
    " 'Bancroft 1.5',\n",
    " 'Bancroft 1.6',\n",
    " 'Yates 1.4',\n",
    " 'Yates 1.5', \n",
    " 'Yates 1.6',\n",
    " 'Bear Lake 1.4',\n",
    " 'Bear Lake 1.5', \n",
    " 'Bear Lake 1.6',    \n",
    " 'SRM NIST 612 1.9',\n",
    " 'SRM NIST 612 1.10',\n",
    " 'SRM NIST 612 1.11',\n",
    " 'MKED-1 1.9',\n",
    " #'MKED-1 1.10',\n",
    " 'MKED-1 1.11',\n",
    " 'BLR-1 1.9',\n",
    " 'BLR-1 1.10', \n",
    " 'BLR-1 1.11',\n",
    " '95-65_25 1.1',  \n",
    " '95-65_25 1.2',   \n",
    " '95-65_25 1.3',  \n",
    " '95-65_26 1.1',\n",
    " '95-65_26 1.2',  \n",
    " '95-65_26 1.3', \n",
    " 'SRM NIST 612 1.12',\n",
    " 'SRM NIST 612 1.13',\n",
    " 'SRM NIST 612 1.14',\n",
    " 'MKED-1 1.12',\n",
    " 'MKED-1 1.13',\n",
    " 'MKED-1 1.14',\n",
    " 'BLR-1 1.12',\n",
    " 'BLR-1 1.13', \n",
    " 'BLR-1 1.14',  \n",
    " 'Bancroft 1.7',\n",
    " 'Bancroft 1.8',\n",
    " 'Bancroft 1.9',\n",
    " 'Yates 1.7',\n",
    " 'Yates 1.8', \n",
    " 'Yates 1.9',\n",
    " 'Bear Lake 1.7',\n",
    " 'Bear Lake 1.8', \n",
    " 'Bear Lake 1.9',    \n",
    " '95-69_42 1.1',   \n",
    " '95-69_42 1.2',\n",
    " '95-69_41 1.1',   \n",
    " '95-69_41 1.2',   \n",
    " '95-69_41 1.3',\n",
    " '95-69_43 1.1',  \n",
    " '95-69_43 1.2',  \n",
    " 'BLR-1 1.15',\n",
    " 'BLR-1 1.16', \n",
    " 'BLR-1 1.17',   \n",
    " 'BLR-1 1.18',\n",
    " 'BLR-1 1.19',  \n",
    " 'MKED-1 1.15',\n",
    " 'MKED-1 1.16',\n",
    " 'MKED-1 1.17',\n",
    " 'MKED-1 1.18',\n",
    " 'MKED-1 1.19', \n",
    " 'SRM NIST 612 1.15',\n",
    " 'SRM NIST 612 1.16',\n",
    " 'SRM NIST 612 1.17',\n",
    " 'SRM NIST 612 1.18',\n",
    " 'SRM NIST 612 1.19',\n",
    " ]\n",
    "\n",
    "\n",
    "#Load in Baseline-subtracted file and set Date\n",
    "filename = 'NP2_20211117_baseline-subtracted.xlsx'\n",
    "date = '17Nov2021'\n",
    "#order = chrono_order2\n",
    "primary_std = 'MKED-1'\n",
    "openIsoplotR = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     3,
     107,
     225,
     386,
     554,
     744
    ]
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE ###\n",
    "#Order of analyses#\n",
    "\n",
    "oct22_order = [\n",
    " 'SRM NIST 612 1.1',\n",
    " 'SRM NIST 612 1.2',\n",
    " 'SRM NIST 612 1.3',\n",
    " 'SRM NIST 612 1.4',\n",
    " 'SRM NIST 612 1.5',\n",
    " 'MKED-1 1.1',\n",
    " 'MKED-1 1.2',\n",
    " 'MKED-1 1.3',\n",
    " 'MKED-1 1.4',\n",
    " 'MKED-1 1.5',  \n",
    " 'BLR-1 1.1',\n",
    " 'BLR-1 1.2', \n",
    " 'BLR-1 1.3',\n",
    " 'BLR-1 1.4', \n",
    " 'BLR-1 1.5', \n",
    " '95-69_t30 1.1',\n",
    " '95-69_t30 1.2',\n",
    " '95-69_t30 1.3',\n",
    " '95-69_t1 1.1',\n",
    " '95-69_t1 1.2',\n",
    " '95-69_t1 1.3', \n",
    " '95-69_t2 1.1',\n",
    " '95-69_t2 1.2',\n",
    " '95-69_t2 1.3',   \n",
    " '95-69_t6 1.1',\n",
    " '95-69_t6 1.2',\n",
    " 'SRM NIST 612 1.6',\n",
    " 'SRM NIST 612 1.7',\n",
    " 'SRM NIST 612 1.8',\n",
    " 'MKED-1 1.6',\n",
    " 'MKED-1 1.7',\n",
    " 'MKED-1 1.8',\n",
    " 'BLR-1 1.6',\n",
    " 'BLR-1 1.7', \n",
    " 'BLR-1 1.8',\n",
    " '95-69_t7 1.1',\n",
    " '95-69_t7 1.2',\n",
    " '95-69_t7 1.3',  \n",
    " '95-69_t8 1.1',\n",
    " '95-69_t8 1.2',\n",
    " '95-69_t8 1.3',    \n",
    " '95-69_t15 1.1',\n",
    " '95-69_t15 1.2',\n",
    " '95-69_t15 1.3', \n",
    " '95-69_t3 1.1',\n",
    " '95-69_t3 1.2',\n",
    " '95-69_t3 1.3',    \n",
    " 'Bancroft 1.1',\n",
    " 'Bancroft 1.2',\n",
    " 'Bancroft 1.3',   \n",
    " 'SRM NIST 612 1.9',\n",
    " 'SRM NIST 612 1.10',\n",
    " 'SRM NIST 612 1.11',\n",
    " 'MKED-1 1.9',\n",
    " 'MKED-1 1.10',\n",
    " #'MKED-1 1.11',\n",
    " 'BLR-1 1.9',\n",
    " 'BLR-1 1.10', \n",
    " 'BLR-1 1.11',\n",
    " '95-69_t4 1.1',\n",
    " '95-69_t4 1.2',\n",
    " '95-69_t4 1.3', \n",
    " '95-69_t5 1.1',\n",
    " '95-69_t5 1.2',\n",
    " '95-69_t5 1.3', \n",
    " '95-65_t1 1.1',\n",
    " '95-65_t1 1.2',\n",
    " '95-65_t1 1.3',  \n",
    " '95-65_t2 1.1',\n",
    " '95-65_t2 1.2',\n",
    " '95-65_t2 1.3',   \n",
    " 'Bancroft 1.4',\n",
    " 'Bancroft 1.5',\n",
    " 'Bancroft 1.6',  \n",
    " 'SRM NIST 612 1.12',\n",
    " 'SRM NIST 612 1.13',\n",
    " 'SRM NIST 612 1.14',\n",
    " 'MKED-1 1.12',\n",
    " 'MKED-1 1.13',\n",
    " 'MKED-1 1.14',\n",
    " 'BLR-1 1.12',\n",
    " 'BLR-1 1.13', \n",
    " 'BLR-1 1.14',\n",
    " '95-65_t3 1.1',\n",
    " '95-65_t3 1.2',\n",
    " '95-65_t3 1.3',   \n",
    " '95-65_t4 1.1',\n",
    " '95-65_t4 1.2',\n",
    " '95-65_t4 1.3',   \n",
    " '95-65_t15 1.1',\n",
    " '95-65_t15 1.2',\n",
    " '95-65_t15 1.3',   \n",
    " '95-65_t16 1.1',\n",
    " '95-65_t16 1.2',\n",
    " '95-65_t17 1.1',  \n",
    " 'SRM NIST 612 1.15',  # Might be bad\n",
    " 'SRM NIST 612 1.16',  # Might be bad\n",
    " #'SRM NIST 612 1.17',  # Might be bad\n",
    " #'MKED-1 1.15', # Might be bad\n",
    " #'MKED-1 1.16', # Might be bad\n",
    " #'MKED-1 1.17', # Might be bad\n",
    "    \n",
    " ]\n",
    "nov4_order = [\n",
    " 'SRM NIST 612 1.1',\n",
    " 'SRM NIST 612 1.2',\n",
    " 'SRM NIST 612 1.3',\n",
    " 'SRM NIST 612 1.4',\n",
    " 'SRM NIST 612 1.5',\n",
    " 'MKED-1 1.1',\n",
    " 'MKED-1 1.2',\n",
    " 'MKED-1 1.3',\n",
    " 'MKED-1 1.4',\n",
    " 'MKED-1 1.5',  \n",
    " 'BLR-1 1.1',\n",
    " 'BLR-1 1.2', \n",
    " 'BLR-1 1.3',\n",
    " 'BLR-1 1.4', \n",
    " 'BLR-1 1.5',\n",
    " '95-65_17 1.1',  \n",
    " '95-65_17 1.2',\n",
    " '95-65_17 1.3',    \n",
    " '95-65_18 1.1',\n",
    " '95-65_18 1.2',  \n",
    " '95-65_18 1.3', \n",
    " '95-65_21 1.1',  \n",
    " '95-65_21 1.2',  \n",
    " 'Bancroft 1.1',\n",
    " 'Bancroft 1.2',\n",
    " 'Bancroft 1.3',\n",
    " 'Yates 1.1',\n",
    " 'Yates 1.2', \n",
    " 'Yates 1.3',\n",
    " 'Bear Lake 1.1',\n",
    " 'Bear Lake 1.2', \n",
    " 'Bear Lake 1.3',  \n",
    " 'SRM NIST 612 1.6',\n",
    " 'SRM NIST 612 1.7',\n",
    " 'SRM NIST 612 1.8',\n",
    " 'MKED-1 1.6',\n",
    " 'MKED-1 1.7',\n",
    " #'MKED-1 1.8',\n",
    " 'BLR-1 1.6',\n",
    " 'BLR-1 1.7', \n",
    " 'BLR-1 1.8',\n",
    " '95-65_29 1.1',  \n",
    " '95-65_29 1.2',   \n",
    " '95-65_29 1.3',  \n",
    " '95-65_29 1.4', \n",
    " '95-65_27 1.1',   \n",
    " '95-65_27 1.2',\n",
    " '95-65_27 1.3',     \n",
    " '95-65_28 1.1',\n",
    " '95-65_28 1.2',  \n",
    " '95-65_28 1.3',  \n",
    " '95-65_28 1.4', \n",
    " 'Bancroft 1.4',\n",
    " 'Bancroft 1.5',\n",
    " 'Bancroft 1.6',\n",
    " 'Yates 1.4',\n",
    " 'Yates 1.5', \n",
    " 'Yates 1.6',\n",
    " 'Bear Lake 1.4',\n",
    " 'Bear Lake 1.5', \n",
    " 'Bear Lake 1.6',    \n",
    " 'SRM NIST 612 1.9',\n",
    " 'SRM NIST 612 1.10',\n",
    " 'SRM NIST 612 1.11',\n",
    " 'MKED-1 1.9',\n",
    " #'MKED-1 1.10',\n",
    " 'MKED-1 1.11',\n",
    " 'BLR-1 1.9',\n",
    " 'BLR-1 1.10', \n",
    " 'BLR-1 1.11',\n",
    " '95-65_25 1.1',  \n",
    " '95-65_25 1.2',   \n",
    " '95-65_25 1.3',  \n",
    " '95-65_26 1.1',\n",
    " '95-65_26 1.2',  \n",
    " '95-65_26 1.3', \n",
    " 'SRM NIST 612 1.12',\n",
    " 'SRM NIST 612 1.13',\n",
    " 'SRM NIST 612 1.14',\n",
    " 'MKED-1 1.12',\n",
    " 'MKED-1 1.13',\n",
    " 'MKED-1 1.14',\n",
    " 'BLR-1 1.12',\n",
    " 'BLR-1 1.13', \n",
    " 'BLR-1 1.14',  \n",
    " 'Bancroft 1.7',\n",
    " 'Bancroft 1.8',\n",
    " 'Bancroft 1.9',\n",
    " 'Yates 1.7',\n",
    " 'Yates 1.8', \n",
    " 'Yates 1.9',\n",
    " 'Bear Lake 1.7',\n",
    " 'Bear Lake 1.8', \n",
    " 'Bear Lake 1.9',    \n",
    " '95-69_42 1.1',   \n",
    " '95-69_42 1.2',\n",
    " '95-69_41 1.1',   \n",
    " '95-69_41 1.2',   \n",
    " '95-69_41 1.3',\n",
    " '95-69_43 1.1',  \n",
    " '95-69_43 1.2',  \n",
    " 'BLR-1 1.15',\n",
    " 'BLR-1 1.16', \n",
    " 'BLR-1 1.17',   \n",
    " 'BLR-1 1.18',\n",
    " 'BLR-1 1.19',  \n",
    " 'MKED-1 1.15',\n",
    " 'MKED-1 1.16',\n",
    " 'MKED-1 1.17',\n",
    " 'MKED-1 1.18',\n",
    " 'MKED-1 1.19', \n",
    " 'SRM NIST 612 1.15',\n",
    " 'SRM NIST 612 1.16',\n",
    " 'SRM NIST 612 1.17',\n",
    " 'SRM NIST 612 1.18',\n",
    " 'SRM NIST 612 1.19',\n",
    " ]\n",
    "nov16_order = [\n",
    "     'SRM NIST 612 1.1',\n",
    " 'SRM NIST 612 1.2',\n",
    " 'SRM NIST 612 1.3',\n",
    " 'SRM NIST 612 1.4',\n",
    " 'SRM NIST 612 1.5',\n",
    " 'MKED-1 1.1',\n",
    " 'MKED-1 1.2',\n",
    " 'MKED-1 1.3',\n",
    " 'MKED-1 1.4',\n",
    " 'MKED-1 1.5',  \n",
    " 'BLR-1 1.1',\n",
    " 'BLR-1 1.2', \n",
    " 'BLR-1 1.3',\n",
    " 'BLR-1 1.4', \n",
    " 'BLR-1 1.5',\n",
    " '94-77_1 1.1',  \n",
    " '94-77_1 1.2',\n",
    " '94-77_1 1.3',\n",
    " '94-77_1 1.4', \n",
    " '94-77_1 1.5',  \n",
    " '94-77_t2 1.1',  \n",
    " '94-77_t2 1.2',\n",
    " '94-77_t2 1.3',  \n",
    " '94-77_t2 1.4',  \n",
    " '94-77_t3 1.1',  \n",
    " '94-77_t3 1.2',\n",
    " '94-77_t3 1.3',  \n",
    " '94-77_t3 1.4',  \n",
    "    \n",
    " 'SRM NIST 612 1.6',\n",
    " 'SRM NIST 612 1.7',\n",
    " 'SRM NIST 612 1.8',\n",
    " 'MKED-1 1.6',\n",
    " 'MKED-1 1.7',\n",
    " 'MKED-1 1.8',\n",
    " 'BLR-1 1.6',\n",
    " 'BLR-1 1.7', \n",
    " 'BLR-1 1.8', \n",
    " '94-77_t4 1.1',  \n",
    " '94-77_t4 1.2',\n",
    " '94-77_t4 1.3',  \n",
    " '94-77_t5 1.1',  \n",
    " '94-77_t5 1.2',\n",
    " '94-77_t5 1.3',  \n",
    " '94-77_t5 1.4',  \n",
    " '94-77_t5 1.5',  \n",
    " '94-77_t6 1.1',  \n",
    " '94-77_t6 1.2',\n",
    " '94-77_t6 1.3',   \n",
    " '94-77_t7 1.1',  \n",
    " '94-77_t7 1.2',\n",
    " '94-77_t7 1.3', \n",
    "    \n",
    "  \n",
    " 'Yates 1.1',\n",
    " 'Yates 1.2', \n",
    " 'Yates 1.3',\n",
    " 'Bear Lake 1.1',\n",
    " 'Bear Lake 1.2', \n",
    " 'Bear Lake 1.3',    \n",
    " 'Bancroft 1.1',\n",
    " 'Bancroft 1.2',\n",
    " 'Bancroft 1.3',  \n",
    " 'SRM NIST 612 1.9',\n",
    " 'SRM NIST 612 1.10',\n",
    " 'SRM NIST 612 1.11',\n",
    " 'SRM NIST 612 1.12',  \n",
    " 'MKED-1 1.9',\n",
    " 'MKED-1 1.10',\n",
    " 'MKED-1 1.11',\n",
    " 'MKED-1 1.12',   \n",
    " 'BLR-1 1.9',\n",
    " 'BLR-1 1.10', \n",
    " 'BLR-1 1.11',  \n",
    " 'BLR-1 1.12',  \n",
    "    \n",
    " '94-77_t8 1.1',  \n",
    " '94-77_t8 1.2',\n",
    " '94-77_t8 1.3',   \n",
    " '94-77_t9 1.1',  \n",
    " '94-77_t9 1.2',\n",
    " '94-77_t9 1.3',  \n",
    " '94-77_t10 1.1',  \n",
    " '94-77_t10 1.2',\n",
    " '94-77_t10 1.3',   \n",
    " '94-77_t10 1.4',  \n",
    " '94-77_t11 1.1',  \n",
    " '94-77_t11 1.2',\n",
    " '94-77_t11 1.3',   \n",
    " '94-77_t11 1.4', \n",
    " '94-77_t12 1.1',  \n",
    " '94-77_t12 1.2',\n",
    " '94-77_t12 1.3',  \n",
    " '94-77_t13 1.1',  \n",
    " '94-77_t13 1.2', \n",
    "    \n",
    " 'Yates 1.4',\n",
    " 'Yates 1.5', \n",
    " 'Yates 1.6',\n",
    " 'Bear Lake 1.4',\n",
    " 'Bear Lake 1.5', \n",
    " 'Bear Lake 1.6',    \n",
    " 'Bancroft 1.4',\n",
    " 'Bancroft 1.5',\n",
    " 'Bancroft 1.6',  \n",
    " 'SRM NIST 612 1.13',\n",
    " 'SRM NIST 612 1.14',\n",
    " 'SRM NIST 612 1.15',\n",
    " 'SRM NIST 612 1.16',  \n",
    " 'MKED-1 1.13',\n",
    " 'MKED-1 1.14',\n",
    " 'MKED-1 1.15',\n",
    " 'MKED-1 1.16',   \n",
    " 'BLR-1 1.13',\n",
    " 'BLR-1 1.14', \n",
    " 'BLR-1 1.15',  \n",
    " 'BLR-1 1.16',     \n",
    "    \n",
    " '91-39_t7 1.1',  \n",
    " '91-39_t7 1.2',\n",
    " '91-39_t7 1.3',  \n",
    " '91-39_t8 1.1',  \n",
    " '91-39_t8 1.2',\n",
    " '91-39_t8 1.3',  \n",
    " '91-39_t4 1.1',  \n",
    " '91-39_t4 1.2',  \n",
    " '91-39_t3 1.1',  \n",
    " '91-39_t3 1.2',\n",
    " '91-39_t3 1.3',  \n",
    " '91-39_t3 1.4',  \n",
    " '91-39_t2 1.1',  \n",
    " '91-39_t2 1.2',\n",
    " '91-39_t2 1.3',    \n",
    " '91-39_t1 1.1',   \n",
    " '91-39_t6 1.1',  \n",
    "  \n",
    " 'Yates 1.7',\n",
    " 'Yates 1.8', \n",
    " 'Yates 1.9',\n",
    " 'Bear Lake 1.7',\n",
    " 'Bear Lake 1.8', \n",
    " 'Bear Lake 1.9',    \n",
    " 'Bancroft 1.7',\n",
    " 'Bancroft 1.8',\n",
    " 'Bancroft 1.9',\n",
    " 'BLR-1 1.17',\n",
    " 'BLR-1 1.18', \n",
    " 'BLR-1 1.19',  \n",
    " 'BLR-1 1.20',     \n",
    " 'BLR-1 1.21',  \n",
    " 'MKED-1 1.17',\n",
    " 'MKED-1 1.18', \n",
    " 'MKED-1 1.19',  \n",
    " 'MKED-1 1.20',     \n",
    " 'MKED-1 1.21',   \n",
    " 'SRM NIST 612 1.17',\n",
    " 'SRM NIST 612 1.18',\n",
    " 'SRM NIST 612 1.19',\n",
    " 'SRM NIST 612 1.20',   \n",
    " 'SRM NIST 612 1.21',   ]\n",
    "nov17_order = [\n",
    " 'SRM NIST 612 1.1',\n",
    " 'SRM NIST 612 1.2',\n",
    " 'SRM NIST 612 1.3',\n",
    " 'SRM NIST 612 1.4',\n",
    " 'SRM NIST 612 1.5',\n",
    " 'MKED-1 1.1',\n",
    " 'MKED-1 1.2',\n",
    " 'MKED-1 1.3',\n",
    " 'MKED-1 1.4',\n",
    " 'MKED-1 1.5',  \n",
    " 'BLR-1 1.1',\n",
    " 'BLR-1 1.2', \n",
    " 'BLR-1 1.3',\n",
    " 'BLR-1 1.4', \n",
    " 'BLR-1 1.5',\n",
    "    \n",
    " 'W15-1_t5 1.1',\n",
    " 'W15-1_t5 1.2',\n",
    " 'W15-1_t5 1.3',\n",
    " 'W15-1_t5 1.4', \n",
    " 'W15-1_t4 1.1',\n",
    " 'W15-1_t4 1.2',\n",
    " 'W15-1_t4 1.3',\n",
    " 'W15-1_t4 1.4',      \n",
    " 'W15-1_t2 1.1',\n",
    " 'W15-1_t2 1.2',\n",
    " 'W15-1_t2 1.3',\n",
    " 'W15-1_t3a 1.1',\n",
    " 'W15-1_t3a 1.2',\n",
    " 'W15-1_t3a 1.3',\n",
    " 'W15-1_t3b 1.1',\n",
    " 'W15-1_t3b 1.2',\n",
    " 'W15-1_t3b 1.3',    \n",
    " \n",
    " 'SRM NIST 612 1.6',\n",
    " 'SRM NIST 612 1.7',\n",
    " 'SRM NIST 612 1.8',\n",
    " 'MKED-1 1.6',\n",
    " 'MKED-1 1.7',\n",
    " 'MKED-1 1.8',\n",
    " 'BLR-1 1.6',\n",
    " 'BLR-1 1.7', \n",
    " 'BLR-1 1.8',\n",
    "  \n",
    " 'W15-1_t1 1.1',\n",
    " 'W15-1_t1 1.2',\n",
    " 'W15-1_t1 1.3',\n",
    " 'W15-1_t1 1.4',   \n",
    " 'W15-1_t11 1.1',\n",
    " 'W15-1_t11 1.2',\n",
    " 'W15-1_t11 1.3',\n",
    " 'W15-1_t12a 1.1',\n",
    " 'W15-1_t12a 1.2',\n",
    " 'W15-1_t12a 1.3',  \n",
    " 'W15-1_t12b 1.1',\n",
    " 'W15-1_t8 1.1',\n",
    " 'W15-1_t8 1.2',\n",
    " 'W15-1_t8 1.3',\n",
    " 'W15-1_t7 1.1',\n",
    " 'W15-1_t7 1.2',\n",
    " 'W15-1_t6 1.1',\n",
    " 'W15-1_t6 1.2', \n",
    "    \n",
    " 'Yates 1.1',\n",
    " 'Yates 1.2', \n",
    " 'Yates 1.3',\n",
    " 'Bear Lake 1.1',\n",
    " 'Bear Lake 1.2', \n",
    " 'Bear Lake 1.3',    \n",
    " 'Bancroft 1.1',\n",
    " 'Bancroft 1.2',\n",
    " 'Bancroft 1.3',  \n",
    " 'SRM NIST 612 1.9',\n",
    " 'SRM NIST 612 1.10',\n",
    " 'SRM NIST 612 1.11',\n",
    " 'SRM NIST 612 1.12',  \n",
    " 'MKED-1 1.9',\n",
    " 'MKED-1 1.10',\n",
    " 'MKED-1 1.11',\n",
    " 'MKED-1 1.12',   \n",
    " 'BLR-1 1.9',\n",
    " 'BLR-1 1.10', \n",
    " 'BLR-1 1.11',  \n",
    " 'BLR-1 1.12',    \n",
    "    \n",
    " #'W15-1_t9 1.1',\n",
    " 'W15-1_t9 1.2',\n",
    " #'W15-1_t9 1.3',   \n",
    " 'W15-1_t10 1.1',\n",
    " 'W15-1_t10 1.2',   \n",
    " 'W15-3_t11 1.1',\n",
    " 'W15-3_t11 1.2',\n",
    " 'W15-3_t11 1.3',\n",
    " 'W15-3_t10 1.1',\n",
    " 'W15-3_t10 1.2',\n",
    " 'W15-3_t10 1.3',    \n",
    " 'W15-3_t8 1.1',\n",
    " 'W15-3_t8 1.2',  \n",
    " 'W15-3_t9 1.1',\n",
    " 'W15-3_t9 1.2',\n",
    " 'W15-3_t9 1.3',\n",
    " 'W15-3_t5 1.1',\n",
    " 'W15-3_t5 1.2',   \n",
    "    \n",
    " 'Yates 1.4',\n",
    " 'Yates 1.5', \n",
    " 'Yates 1.6',\n",
    " 'Bear Lake 1.4',\n",
    " 'Bear Lake 1.5', \n",
    " 'Bear Lake 1.6',    \n",
    " 'Bancroft 1.4',\n",
    " 'Bancroft 1.5',\n",
    " 'Bancroft 1.6',  \n",
    " 'SRM NIST 612 1.13',\n",
    " 'SRM NIST 612 1.14',\n",
    " 'SRM NIST 612 1.15',\n",
    " 'SRM NIST 612 1.16',  \n",
    " 'MKED-1 1.13',\n",
    " 'MKED-1 1.14',\n",
    " 'MKED-1 1.15',\n",
    " 'MKED-1 1.16',   \n",
    " 'BLR-1 1.13',\n",
    " 'BLR-1 1.14', \n",
    " 'BLR-1 1.15',  \n",
    " 'BLR-1 1.16',   \n",
    "    \n",
    " 'W15-3_t4 1.1',\n",
    " 'W15-3_t4 1.2',\n",
    " 'W15-3_t4 1.3', \n",
    " 'W15-3_t4 1.4',   \n",
    " 'W15-3_t1 1.1',\n",
    " 'W15-3_t1 1.2',\n",
    " 'W15-3_t1 1.3', \n",
    " 'W15-3_t1 1.4',\n",
    " 'W15-3_t2 1.1',\n",
    " 'W15-3_t2 1.2',\n",
    " 'W15-3_t2 1.3', \n",
    " 'W15-3_t2 1.4',\n",
    " 'W15-3_t3 1.1',\n",
    " 'W15-3_t3 1.2',\n",
    " 'W15-3_t3 1.3', \n",
    " 'W15-3_t3 1.4', \n",
    "    \n",
    " 'Yates 1.7',\n",
    " 'Yates 1.8', \n",
    " 'Yates 1.9',\n",
    " 'Bear Lake 1.7',\n",
    " 'Bear Lake 1.8', \n",
    " 'Bear Lake 1.9',    \n",
    " 'Bancroft 1.7',\n",
    " 'Bancroft 1.8',\n",
    " 'Bancroft 1.9',\n",
    " 'BLR-1 1.17',\n",
    " 'BLR-1 1.18', \n",
    " 'BLR-1 1.19',  \n",
    " #'BLR-1 1.20',     \n",
    " #'BLR-1 1.21',  \n",
    " 'MKED-1 1.17',\n",
    " 'MKED-1 1.18', \n",
    " 'MKED-1 1.19',  \n",
    " 'MKED-1 1.20',     \n",
    " 'MKED-1 1.21',   \n",
    " 'SRM NIST 612 1.17',\n",
    " 'SRM NIST 612 1.18',\n",
    " 'SRM NIST 612 1.19',\n",
    " 'SRM NIST 612 1.20',   \n",
    " 'SRM NIST 612 1.21',   ]\n",
    "nov22_order = [\n",
    "    'SRM NIST 612 1.1',\n",
    " 'SRM NIST 612 1.2',\n",
    " 'SRM NIST 612 1.3',\n",
    " 'SRM NIST 612 1.4',\n",
    " 'SRM NIST 612 1.5',\n",
    " 'MKED-1 1.1',\n",
    " 'MKED-1 1.2',\n",
    " 'MKED-1 1.3',\n",
    " 'MKED-1 1.4',\n",
    " 'MKED-1 1.5',  \n",
    " 'BLR-1 1.1',\n",
    " 'BLR-1 1.2', \n",
    " 'BLR-1 1.3',\n",
    " 'BLR-1 1.4', \n",
    " 'BLR-1 1.5',\n",
    " 'SAB-94-134_t8 1.1',   \n",
    " 'SAB-94-134_t8 1.2',\n",
    " 'SAB-94-134_t8 1.3',\n",
    " 'SAB-94-134_t10 1.1',   \n",
    " 'SAB-94-134_t10 1.2',\n",
    " 'SAB-94-134_t10 1.3',   \n",
    " 'SAB-94-134_t10 1.4',  \n",
    " 'SAB-94-134_t10 1.5',  \n",
    " 'SAB-94-134_t7 1.1',  \n",
    " 'SAB-94-134_t7 1.2',  \n",
    " 'SAB-94-134_t7 1.3',  \n",
    " 'SAB-94-134_t6 1.1',  \n",
    " 'SAB-94-134_t6 1.2',  \n",
    " 'SAB-94-134_t6 1.3',   \n",
    " 'SAB-94-134_t5 1.1',  \n",
    " 'SAB-94-134_t5 1.2',  \n",
    " 'SAB-94-134_t5 1.3',   \n",
    " \n",
    " 'SRM NIST 612 1.6',\n",
    " 'SRM NIST 612 1.7',\n",
    " 'SRM NIST 612 1.8',\n",
    " 'MKED-1 1.6',\n",
    " 'MKED-1 1.7',\n",
    " 'MKED-1 1.8',\n",
    " 'BLR-1 1.6',\n",
    " 'BLR-1 1.7', \n",
    " 'BLR-1 1.8',  \n",
    "  \n",
    " 'SAB-94-134_t3 1.1',  \n",
    " 'SAB-94-134_t3 1.2',  \n",
    " 'SAB-94-134_t3 1.3',   \n",
    " 'SAB-94-134_t3 1.4',   \n",
    " 'SAB-94-134_t4 1.1',  #ln46\n",
    " 'SAB-94-134_t4 1.2',  #ln47\n",
    " 'SAB-94-134_t4 1.3',  \n",
    " 'SAB-94-134_t4 1.4', \n",
    " 'SAB-94-134_t2 1.1',  #ln50\n",
    " 'SAB-94-134_t2 1.2',  #ln51\n",
    " 'SAB-94-134_t2 1.3',   \n",
    " 'SAB-94-134_t2 1.4',  \n",
    " 'SAB-94-134_t1 1.1',  \n",
    " 'SAB-94-134_t1 1.2',  \n",
    " 'SAB-94-134_t1 1.3',   \n",
    " 'SAB-94-134_t1 1.4',  \n",
    " 'SAB-94-134_t1 1.5',  \n",
    "  \n",
    " 'Bear Lake 1.1',\n",
    " 'Bear Lake 1.2', \n",
    " 'Bear Lake 1.3',    \n",
    " 'Bancroft 1.1',\n",
    " 'Bancroft 1.2',\n",
    " 'Bancroft 1.3',\n",
    " 'Yates 1.1',\n",
    " 'Yates 1.2', \n",
    " 'Yates 1.3',   \n",
    " 'SRM NIST 612 1.9',\n",
    " 'SRM NIST 612 1.10',\n",
    " 'SRM NIST 612 1.11',\n",
    " 'SRM NIST 612 1.12',  \n",
    " 'MKED-1 1.9',\n",
    " 'MKED-1 1.10',\n",
    " 'MKED-1 1.11',\n",
    " 'MKED-1 1.12',   \n",
    " 'BLR-1 1.9',\n",
    " 'BLR-1 1.10', \n",
    " 'BLR-1 1.11',  \n",
    " 'BLR-1 1.12', \n",
    "  \n",
    " 'SAB-95-65c_t5 1.1',   \n",
    " 'SAB-95-65c_t5 1.2',    \n",
    " 'SAB-95-65c_t6 1.1',    \n",
    " 'SAB-95-65c_t7 1.1',   \n",
    " 'SAB-95-65c_t7 1.2',   \n",
    " 'SAB-95-65c_t8 1.1',   \n",
    " 'SAB-95-65c_t9 1.1',   \n",
    " 'SAB-95-65c_t9 1.2',   \n",
    " 'SAB-95-65c_t9 1.3',  \n",
    " 'SAB-95-65c_t10 1.1',   \n",
    " 'SAB-95-65c_t10 1.2',   \n",
    " 'SAB-95-65c_t10 1.3',   \n",
    "  \n",
    " 'Bear Lake 1.4',\n",
    " 'Bear Lake 1.5', \n",
    " 'Bear Lake 1.6',    \n",
    " 'Bancroft 1.4',\n",
    " 'Bancroft 1.5',\n",
    " 'Bancroft 1.6',   \n",
    " 'Yates 1.4',\n",
    " 'Yates 1.5', \n",
    " 'Yates 1.6',  \n",
    " 'SRM NIST 612 1.13',\n",
    " 'SRM NIST 612 1.14',\n",
    " 'SRM NIST 612 1.15',\n",
    " 'SRM NIST 612 1.16',  \n",
    " 'MKED-1 1.13',\n",
    " 'MKED-1 1.14',\n",
    " 'MKED-1 1.15',\n",
    " 'MKED-1 1.16',   \n",
    " 'BLR-1 1.13',\n",
    " 'BLR-1 1.14', \n",
    " 'BLR-1 1.15',  \n",
    " 'BLR-1 1.16',   \n",
    " \n",
    " 'SAB-95-65c_t11 1.1',   \n",
    " 'SAB-95-65c_t11 1.2',   \n",
    " 'SAB-95-65c_t14 1.1',   \n",
    " 'SAB-95-65c_t14 1.2', \n",
    " 'SAB-95-65c_t12 1.1',   \n",
    " 'SAB-95-65c_t12 1.2',  \n",
    " 'SAB-95-65c_t13 1.1',   \n",
    " 'SAB-95-65c_t13 1.2',   \n",
    " 'SAB-95-65c_t13 1.3',   \n",
    " 'SAB-95-65b_t1 1.1',   \n",
    " 'SAB-95-65b_t2 1.1',   \n",
    " 'SAB-95-65b_t2 1.2', \n",
    " #'SAB-95-65b_t3 1.1',   \n",
    " 'SAB-95-65b_t4 1.1',   \n",
    " 'SAB-95-65b_t4 1.2',   \n",
    " 'SAB-95-65b_t5 1.1',   \n",
    " 'SAB-95-65b_t5 1.2',  \n",
    " 'SAB-95-65b_t5 1.3',  \n",
    "    \n",
    " 'Bear Lake 1.7',\n",
    " 'Bear Lake 1.8', \n",
    " 'Bear Lake 1.9',    \n",
    " 'Bancroft 1.7',\n",
    " 'Bancroft 1.8',\n",
    " 'Bancroft 1.9',\n",
    " 'Yates 1.7',\n",
    " 'Yates 1.8', \n",
    " 'Yates 1.9',   \n",
    " 'SRM NIST 612 1.17',\n",
    " 'SRM NIST 612 1.18',\n",
    " 'SRM NIST 612 1.19',\n",
    " 'SRM NIST 612 1.20',  \n",
    " 'MKED-1 1.17',\n",
    " 'MKED-1 1.18',\n",
    " 'MKED-1 1.19',\n",
    " 'MKED-1 1.20',   \n",
    " 'BLR-1 1.17',\n",
    " 'BLR-1 1.18', \n",
    " 'BLR-1 1.19',  \n",
    " 'BLR-1 1.20',   \n",
    "  \n",
    " 'SAB-95-65b_t11 1.1',   \n",
    " 'SAB-95-65b_t11 1.2',  \n",
    " 'SAB-95-65b_t9 1.1',\n",
    " 'SAB-95-65b_t9 1.2', \n",
    " 'SAB-95-65b_t9 1.3',  \n",
    " 'SAB-95-65b_t10 1.1',\n",
    " 'SAB-95-65b_t10 1.2',  \n",
    " 'SAB-95-65b_t10 1.3',  \n",
    " 'SAB-95-65b_t12 1.1',\n",
    " 'SAB-95-65b_t12 1.2',  \n",
    " 'SAB-95-65b_t12 1.3',  \n",
    " 'SAB-95-65b_t13 1.1',\n",
    " 'SAB-95-65b_t13 1.2',  \n",
    " 'SAB-95-65b_t13 1.3',\n",
    "  \n",
    " 'BLR-1 1.21',\n",
    " 'BLR-1 1.22', \n",
    " 'BLR-1 1.23',  \n",
    " 'BLR-1 1.24',\n",
    " 'BLR-1 1.25',  \n",
    " 'MKED-1 1.21',\n",
    " 'MKED-1 1.22',\n",
    " 'MKED-1 1.23',\n",
    " 'MKED-1 1.24',   \n",
    " 'MKED-1 1.25',   \n",
    " 'SRM NIST 612 1.21',\n",
    " 'SRM NIST 612 1.22',\n",
    " 'SRM NIST 612 1.23',\n",
    " 'SRM NIST 612 1.24',  \n",
    " 'SRM NIST 612 1.25',    ]\n",
    "nov23_order = [\n",
    "    'SRM NIST 612 1.1',\n",
    " 'SRM NIST 612 1.2',\n",
    " 'SRM NIST 612 1.3',\n",
    " 'SRM NIST 612 1.4',\n",
    " 'SRM NIST 612 1.5',\n",
    " 'MKED-1 1.1',\n",
    " 'MKED-1 1.2',\n",
    " 'MKED-1 1.3',\n",
    " 'MKED-1 1.4',\n",
    " 'MKED-1 1.5',  \n",
    " 'BLR-1 1.1',\n",
    " 'BLR-1 1.2', \n",
    " 'BLR-1 1.3',\n",
    " 'BLR-1 1.4', \n",
    " 'BLR-1 1.5',\n",
    "    \n",
    " 'SB-91-53_t6 1.1',\n",
    " 'SB-91-53_t6 1.2',\n",
    " 'SB-91-53_t6 1.3',   \n",
    " 'SB-91-53_t4 1.1',\n",
    " 'SB-91-53_t4 1.2',\n",
    " 'SB-91-53_t4 1.3',   \n",
    " 'SB-91-53_t4 1.4',   \n",
    " 'SB-91-53_t5 1.1',\n",
    " 'SB-91-53_t5 1.2',  \n",
    " 'SB-91-53_t3 1.1',\n",
    " 'SB-91-53_t3 1.2',\n",
    "    \n",
    " 'Bear Lake 1.1',\n",
    " 'Bear Lake 1.2', \n",
    " 'Bear Lake 1.3',    \n",
    " 'Bancroft 1.1',\n",
    " 'Bancroft 1.2',\n",
    " 'Bancroft 1.3',\n",
    " 'Yates 1.1',\n",
    " 'Yates 1.2', \n",
    " 'Yates 1.3',   \n",
    " 'SRM NIST 612 1.6',\n",
    " 'SRM NIST 612 1.7',\n",
    " 'SRM NIST 612 1.8',\n",
    " 'SRM NIST 612 1.8',  \n",
    " 'MKED-1 1.6',\n",
    " 'MKED-1 1.7',\n",
    " 'MKED-1 1.8',\n",
    " 'MKED-1 1.9',   \n",
    " 'BLR-1 1.6',\n",
    " 'BLR-1 1.7', \n",
    " 'BLR-1 1.8',  \n",
    " 'BLR-1 1.9',\n",
    "    \n",
    " 'SB-91-53_t2 1.1',\n",
    " 'SB-91-53_t2 1.2',   \n",
    " 'SB-91-53_t1 1.1',\n",
    " 'SB-91-53_t1 1.2',\n",
    " 'SB-91-53_t1 1.3',  \n",
    " 'SB-91-53_t12 1.1',\n",
    " 'SB-91-53_t12 1.2',\n",
    " 'SB-91-53_t13 1.1',   \n",
    " 'SB-91-53_t7 1.1',\n",
    " 'SB-91-53_t7 1.2',\n",
    " 'SB-91-53_t7 1.3',   \n",
    " 'SB-91-53_t14 1.1',   \n",
    " 'SB-91-53_t10 1.1',\n",
    " 'SB-91-53_t10 1.2',\n",
    " 'SB-91-53_t10 1.3',   \n",
    "    \n",
    " 'Bear Lake 1.4',\n",
    " 'Bear Lake 1.5', \n",
    " 'Bear Lake 1.6',    \n",
    " 'Bancroft 1.4',\n",
    " 'Bancroft 1.5',\n",
    " 'Bancroft 1.6',   \n",
    " 'Yates 1.4',\n",
    " 'Yates 1.5', \n",
    " 'Yates 1.6', \n",
    " 'SRM NIST 612 1.10',\n",
    " 'SRM NIST 612 1.11',\n",
    " 'SRM NIST 612 1.12',\n",
    " 'SRM NIST 612 1.13',  \n",
    " 'MKED-1 1.10',\n",
    " 'MKED-1 1.11',\n",
    " 'MKED-1 1.12',\n",
    " 'MKED-1 1.13',   \n",
    " 'BLR-1 1.10',\n",
    " 'BLR-1 1.11', \n",
    " 'BLR-1 1.12',  \n",
    " 'BLR-1 1.13',\n",
    "      \n",
    " 'SB-91-53_t8 1.1',\n",
    " 'SB-91-53_t8 1.2',   \n",
    " 'SB-91-53_t8 1.3',\n",
    " 'SB-91-53_t8 1.4',  \n",
    " 'SB-91-53_t8 1.5',\n",
    " 'SB-91-53_t11 1.1',\n",
    " 'SB-91-53_t11 1.2',   \n",
    " 'SB-91-53_t11 1.3',  \n",
    " 'SB-91-53_t15 1.1',\n",
    " 'SB-91-53_t15 1.2',   \n",
    " 'SB-91-53_t15 1.3', \n",
    " 'SB-91-53_t16 1.1',\n",
    " 'SB-91-53_t16 1.2',   \n",
    " 'SB-91-53_t16 1.3',  \n",
    "    \n",
    " 'Bear Lake 1.7',\n",
    " 'Bear Lake 1.8', \n",
    " 'Bear Lake 1.9',    \n",
    " 'Bancroft 1.7',\n",
    " 'Bancroft 1.8',\n",
    " 'Bancroft 1.9',\n",
    " 'Yates 1.7',\n",
    " 'Yates 1.8', \n",
    " 'Yates 1.9',  \n",
    " 'BLR-1 1.14',\n",
    " 'BLR-1 1.15', \n",
    " 'BLR-1 1.16',  \n",
    " 'BLR-1 1.17',\n",
    " 'BLR-1 1.18',  \n",
    " 'MKED-1 1.14',\n",
    " #'MKED-1 1.15',\n",
    " 'MKED-1 1.16',\n",
    " #'MKED-1 1.17',   \n",
    "# 'MKED-1 1.18',   \n",
    " 'SRM NIST 612 1.14',\n",
    " 'SRM NIST 612 1.15',\n",
    " 'SRM NIST 612 1.16',\n",
    " 'SRM NIST 612 1.17',  \n",
    " 'SRM NIST 612 1.18',     ]\n",
    "\n",
    "dec09_order = [\n",
    " #'SRM NIST 612 1.1',\n",
    " 'SRM NIST 612 1.2',\n",
    " 'SRM NIST 612 1.3',\n",
    " 'SRM NIST 612 1.4',\n",
    " 'SRM NIST 612 1.5',\n",
    " 'MKED-1 1.1',\n",
    " 'MKED-1 1.2',\n",
    " 'MKED-1 1.3',\n",
    " 'MKED-1 1.4',\n",
    " 'MKED-1 1.5',  \n",
    " 'BLR-1 1.1',\n",
    " 'BLR-1 1.2', \n",
    " 'BLR-1 1.3',\n",
    " 'BLR-1 1.4', \n",
    " 'BLR-1 1.5', \n",
    "    \n",
    " 'Bear Lake 1.1',   \n",
    " 'Bear Lake 1.2',\n",
    " 'Bear Lake 1.3',   \n",
    " '95-69_t1 1.1',  \n",
    " '95-69_t1 1.2',  \n",
    " '95-69_t1 1.3',    \n",
    " '95-69_t8 1.1',  \n",
    " '95-69_t8 1.2',  \n",
    " '95-69_t8 1.3',     \n",
    " #'Yates 1.1', \n",
    " #'Yates 1.2', \n",
    " #'Yates 1.3',    \n",
    " 'Bancroft 1.1',\n",
    " 'Bancroft 1.2',\n",
    " 'Bancroft 1.3',  \n",
    "    \n",
    " 'SRM NIST 612 1.6',\n",
    " 'SRM NIST 612 1.7',\n",
    " 'SRM NIST 612 1.8',\n",
    " 'SRM NIST 612 1.9',   \n",
    " 'MKED-1 1.6',\n",
    " 'MKED-1 1.7',\n",
    " 'MKED-1 1.8',\n",
    " 'MKED-1 1.9',\n",
    " 'BLR-1 1.6',\n",
    " 'BLR-1 1.7', \n",
    " 'BLR-1 1.8',\n",
    " 'BLR-1 1.9',\n",
    "    \n",
    " 'Bear Lake 1.4',   \n",
    " 'Bear Lake 1.5',\n",
    " 'Bear Lake 1.6',     \n",
    " '95-65_t3 1.1',  \n",
    " '95-65_t3 1.2',  \n",
    " '95-65_t3 1.3',   \n",
    " '95-65_t4 1.1',  \n",
    " '95-65_t4 1.2',  \n",
    " '95-65_t4 1.3',     \n",
    " #'Yates 1.4', \n",
    " #'Yates 1.5', \n",
    " #'Yates 1.6',    \n",
    " 'Bancroft 1.4',\n",
    " 'Bancroft 1.5',\n",
    " #'Bancroft 1.6',    \n",
    "   \n",
    " 'BLR-1 1.10',\n",
    " 'BLR-1 1.11', \n",
    " 'BLR-1 1.12',\n",
    " 'BLR-1 1.13', \n",
    " 'BLR-1 1.14',   \n",
    " 'MKED-1 1.10',\n",
    " 'MKED-1 1.11',\n",
    " 'MKED-1 1.12',\n",
    " 'MKED-1 1.13',\n",
    " 'MKED-1 1.14',     \n",
    " 'SRM NIST 612 1.10',\n",
    " 'SRM NIST 612 1.11',\n",
    " 'SRM NIST 612 1.12',\n",
    " 'SRM NIST 612 1.13',\n",
    " 'SRM NIST 612 1.14',\n",
    "\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to fix how the rows are ordered...\n",
    "#date = '29Nov2021'\n",
    "filename = 'NP2_20211022_baseline-subtracted_mod1.xlsx'\n",
    "filename2 = 'NP2_20211104_baseline-subtracted_mod1.xlsx'\n",
    "filename3 = 'NP2_20211116_baseline-subtracted.xlsx'\n",
    "filename4 = 'NP2_20211122_baseline-subtracted.xlsx'\n",
    "filename5 = 'NP2_20211123_baseline-subtracted_mod1.xlsx'\n",
    "\n",
    "filename6 = 'NP2_20211208_baseline-subtracted.xlsx'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File chosen = NP2_20211208_baseline-subtracted.xlsx \n",
      "\n",
      "Date chosen = 09Dec \n",
      "\n",
      "Chronologic order defined as: \n",
      "SRM NIST 612 1.2, SRM NIST 612 1.3, SRM NIST 612 1.4\n",
      "SRM NIST 612 1.5, MKED-1 1.1, MKED-1 1.2\n",
      "MKED-1 1.3, MKED-1 1.4, MKED-1 1.5\n",
      "BLR-1 1.1, BLR-1 1.2, BLR-1 1.3\n",
      "BLR-1 1.4, BLR-1 1.5, Bear Lake 1.1\n",
      "Bear Lake 1.2, Bear Lake 1.3, 95-69_t1 1.1\n",
      "95-69_t1 1.2, 95-69_t1 1.3, 95-69_t8 1.1\n",
      "95-69_t8 1.2, 95-69_t8 1.3, Bancroft 1.1\n",
      "Bancroft 1.2, Bancroft 1.3, SRM NIST 612 1.6\n",
      "SRM NIST 612 1.7, SRM NIST 612 1.8, SRM NIST 612 1.9\n",
      "MKED-1 1.6, MKED-1 1.7, MKED-1 1.8\n",
      "MKED-1 1.9, BLR-1 1.6, BLR-1 1.7\n",
      "BLR-1 1.8, BLR-1 1.9, Bear Lake 1.4\n",
      "Bear Lake 1.5, Bear Lake 1.6, 95-65_t3 1.1\n",
      "95-65_t3 1.2, 95-65_t3 1.3, 95-65_t4 1.1\n",
      "95-65_t4 1.2, 95-65_t4 1.3, Bancroft 1.4\n",
      "Bancroft 1.5, BLR-1 1.10, BLR-1 1.11\n",
      "BLR-1 1.12, BLR-1 1.13, BLR-1 1.14\n",
      "MKED-1 1.10, MKED-1 1.11, MKED-1 1.12\n",
      "MKED-1 1.13, MKED-1 1.14, SRM NIST 612 1.10\n",
      "SRM NIST 612 1.11, SRM NIST 612 1.12, SRM NIST 612 1.13\n",
      "SRM NIST 612 1.14\n",
      "\n",
      "\n",
      "full bracket =  [4, 58]\n",
      "Primary Standard=  MKED-1\n",
      "Error minimization rejection percentage=  0\n",
      "Calculating excess variance for:  MKED-1 \n",
      "\n",
      "206/238\n",
      "Initial MSWD= 24.648502549809056\n",
      "excess variance= 3.0602 MSWD= 1.0 \n",
      "\n",
      "208/232\n",
      "Initial MSWD= 29.590510361027476\n",
      "excess variance= 3.9213 MSWD= 1.0 \n",
      "\n",
      "207/206\n",
      "Initial MSWD= 35.06128460033153\n",
      "excess variance= 2.2999 MSWD= 1.0 \n",
      "\n",
      "208/206\n",
      "Initial MSWD= 44.912575271115394\n",
      "excess variance= 1.2105 MSWD= 1.0 \n",
      "\n",
      "206/204\n",
      "Initial MSWD= 2.2005669727276542\n",
      "excess variance= 56.0848 MSWD= 1.0 \n",
      "\n",
      "207/204\n",
      "Initial MSWD= 2.1577167180446617\n",
      "excess variance= 54.3657 MSWD= 1.0 \n",
      "\n",
      "208/204\n",
      "Initial MSWD= 2.1809107540809385\n",
      "excess variance= 55.2963 MSWD= 1.0 \n",
      "\n",
      "\n",
      "STEP 1. stat_to_excV() is complete.\n",
      "\n",
      "STEP 2. file_to_blockBrackets() is complete.\n",
      "\n",
      "\n",
      "STEP 3. calc_CPS() is complete for bracket_1.\n",
      "\n",
      "\n",
      "\n",
      "STEP 3. calc_CPS() is complete for bracket_2.\n",
      "\n",
      "\n",
      "STEP 4. Separate brackets have been concatenated.\n",
      "\n",
      "STEP 5. export_corr_full() is complete.\n",
      "\n",
      "STEP 6. export_corr_plot() is complete.\n",
      "\n",
      "STEP 7. export_isoplot() is complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_Bracket_to_output(filename6, '09Dec', dec09_order, 'MKED-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_Bracket_to_output(filename, '22Oct', oct22_order, 'BLR-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_Bracket_to_output(filename2, '4Nov', nov4_order, 'MKED-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Bracket_to_output(filename2, '4Nov', nov4_order, 'BLR-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Bracket_to_output(filename3, '16Nov', nov16_order, 'MKED-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Bracket_to_output(filename4, '22Nov', nov22_order, 'MKED-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Bracket_to_output(filename5, '23Nov', nov23_order, 'MKED-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Currently will open IsoplotR in Safari Browser.\n",
    "if openIsoplotR:\n",
    "    webbrowser.get(\"safari\").open_new(\"http://isoplotr.geol.ucsb.edu/isoplotr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Code to run everything in a FULL GROUP right now ###\n",
    "\n",
    "file_to_output(filename, date, order, primary_std, True)\n",
    "\n",
    "#Currently will open IsoplotR in Safari Browser.\n",
    "if openIsoplotR:\n",
    "    webbrowser.get(\"safari\").open_new(\"http://isoplotr.geol.ucsb.edu/isoplotr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# #### DEPRECATED FUNCTIONS as of 3 November 2021 #####\n",
    "\n",
    "# #These might be useful later, but are currently not being used.\n",
    "\n",
    "#ratio_report(plot_dict, 'fake.pdf', False, 'ratio_output.pdf')\n",
    "\n",
    "# def files_ranked_toEXCEL(calc_dict, excel_name):\n",
    "#     stats = statistics_ranktest(calc_dict)\n",
    "#     with pd.ExcelWriter(excel_name) as writer:\n",
    "#         for sheet in calc_dict:\n",
    "#             calc_dict[sheet].to_excel(writer, sheet_name = sheet, index = False)\n",
    "        \n",
    "#         stats.to_excel(writer, sheet_name = 'Statistics', index = True)\n",
    "#     new_filename = str(excel_name.split('.')[0]) + '_statistics.xlsx'\n",
    "#     with pd.ExcelWriter(new_filename) as writer:\n",
    "#         stats.to_excel(writer, sheet_name = 'Statistics', index = True) \n",
    "\n",
    "\n",
    "\n",
    "# def ranked_minimization(sheet, ratio, reject_percentage = 20):\n",
    "\n",
    "#     mytest = tester[sheet].copy(deep=True)\n",
    "\n",
    "#     df_mean_before = mytest[ratio].mean()\n",
    "#     df_1std_before = mytest[ratio].std()\n",
    "#     df_count_before = mytest[ratio].count()\n",
    "#     df_2se_perc_before = (2 * mytest[ratio].sem()) / df_mean_before * 100\n",
    "\n",
    "#     dif_mean = ratio + '_dif_from_mean'\n",
    "#     dif_1SD = ratio + '_dif_from_1SD'\n",
    "#     mytest[dif_mean] = mytest.apply(lambda x: abs(x[ratio] - df_mean_before), axis=1)\n",
    "#     mytest[dif_1SD] = mytest.apply(lambda x: x[dif_mean] - df_1std_before, axis=1)\n",
    "\n",
    "\n",
    "#     mytest2 = mytest.sort_values(by = dif_1SD, ascending = False)\n",
    "#     #mytest2.head()\n",
    "\n",
    "#     ratios_to_reject = int(mytest[ratio].count() * reject_percentage / 100)\n",
    "#     #print(ratios_to_reject)\n",
    "\n",
    "#     after_rejection = mytest2[ratios_to_reject:]\n",
    "\n",
    "#     df_mean_after = after_rejection[ratio].mean()\n",
    "#     df_1std_after = after_rejection[ratio].std()\n",
    "#     df_count_after = after_rejection[ratio].count()\n",
    "    \n",
    "#     #This is not 2SE%, should probably fix labels... KD 14 June 2021\n",
    "#     df_2se_perc_after = (2 * after_rejection[ratio].std()) / df_mean_after * 100\n",
    "\n",
    "#     # print(df_mean_after)\n",
    "#     # print(df_1std_after)\n",
    "#     # print(df_2se_perc_after)\n",
    "\n",
    "#     results_dict = {}\n",
    "    \n",
    "#     results_dict['avg_before'] = df_mean_before\n",
    "#     results_dict['1sd_before'] = df_1std_before\n",
    "#     results_dict['2se%_before'] = df_2se_perc_before\n",
    "#     results_dict['avg_after'] = df_mean_after\n",
    "#     results_dict['1sd_after'] = df_1std_after\n",
    "#     results_dict['2σ%_after'] = df_2se_perc_after\n",
    "    \n",
    "#     return results_dict\n",
    "\n",
    "# def statistics_ranktest(calc_dict, reject_percentage = 20):\n",
    "#     calc_list = ['238_CPS', '232_CPS',\n",
    "#            '208_CPS', '207_CPS', '206_CPS', '204_CPS', '202_CPS', '206/238',\n",
    "#            '208/232', '207/206', '208/206', '206/204','208/204','207/204', '207/235'  ]\n",
    "#     mega_dict = {}\n",
    "\n",
    "#     for sheet in calc_dict:\n",
    "#         tester = calc_dict[sheet]\n",
    "#         stats_dict = {}\n",
    "#         for col in tester:\n",
    "\n",
    "#             if col in calc_list:\n",
    "#                 #print(col)\n",
    "#                 if '/' in col:\n",
    "#                     key_bf = col + '_before rejection'\n",
    "#                     key_af = col + '_after rejection'\n",
    "#                     key_bf_se = col + '_before rejection 2se%'\n",
    "#                     key_af_se = col + '_after rejection 2σ%'\n",
    "                    \n",
    "#                     ranked_dict = ranked_minimization(sheet, col, reject_percentage)\n",
    "#                     stats_dict[key_bf] = ranked_dict['avg_before']\n",
    "#                     stats_dict[key_bf_se] = ranked_dict['2se%_before']\n",
    "#                     stats_dict[key_af] = ranked_dict['avg_after']\n",
    "#                     #This is not 2SE%, should probably fix labels... KD 14 June 2021\n",
    "#                     stats_dict[key_af_se] = ranked_dict['2σ%_after']\n",
    "#                 else:\n",
    "#                     key = col + '_mean'\n",
    "#                     df_mean = tester[col].mean()\n",
    "#                     stats_dict[key] = df_mean\n",
    "#                     #This is not 2SE%, should probably fix labels... KD 14 June 2021\n",
    "#                     df_precision = (2 * tester[col].std()) / df_mean * 100\n",
    "#                     stats_dict[col + '_2σ%'] = df_precision\n",
    "#             if 'OPZ' in col:\n",
    "#                  stats_dict[col + '_mean'] = tester[col].mean()\n",
    "#             if 'SNR' in col:\n",
    "#                  stats_dict[col + '_mean'] = tester[col].mean()\n",
    "                    \n",
    "#         stats_dict['Time (s)'] = tester['Elapsed Time'].max()\n",
    "        \n",
    "#         #new_string = sheet.replace('time series data', '')\n",
    "#         new_string = sheet.split('time')[0].rstrip()\n",
    "#         mega_dict[new_string] = stats_dict\n",
    "\n",
    "#     df_1 = pd.DataFrame(mega_dict)\n",
    "#     df_flip = pd.DataFrame.transpose(df_1)\n",
    "#     return df_flip\n",
    "\n",
    "\n",
    "# def stat_grouper(stats_df):\n",
    "#     ''' Input dataframe that was output from statistics_ranktest2 function.'''\n",
    "#     headers_to_keep = [\n",
    "#         '206/238_after rejection',\n",
    "#         '206/238_after rejection 2se%',\n",
    "#         '208/232_after rejection',\n",
    "#         '208/232_after rejection 2se%',\n",
    "#         '207/206_after rejection',\n",
    "#         '207/206_after rejection 2se%',\n",
    "#         '208/206_after rejection',\n",
    "#         '208/206_after rejection 2se%',\n",
    "#         '206/204_after rejection',\n",
    "#         '206/204_after rejection 2se%',\n",
    "#         '208/204_after rejection',\n",
    "#         '208/204_after rejection 2se%',\n",
    "#         '207/204_after rejection',\n",
    "#         '207/204_after rejection 2se%',\n",
    "#         '207/235_after rejection',\n",
    "#         '207/235_after rejection 2se%' \n",
    "#     ]\n",
    "\n",
    "#     headers_to_math = [\n",
    "#         '206/238_after rejection',   \n",
    "#         '208/232_after rejection',  \n",
    "#         '207/206_after rejection',    \n",
    "#         '208/206_after rejection',    \n",
    "#         '206/204_after rejection',    \n",
    "#         '208/204_after rejection',   \n",
    "#         '207/204_after rejection',\n",
    "#         '207/235_after rejection',\n",
    "#     ]\n",
    "\n",
    "\n",
    "#     short_tester = stats_df[headers_to_keep]\n",
    "\n",
    "#     samples = short_tester.index.values.tolist()\n",
    "#     group = None\n",
    "#     group_names = []\n",
    "#     start = [0]\n",
    "#     end = []\n",
    "#     for idx in range(len(samples)):\n",
    "#         if group == None:\n",
    "#             group = samples[idx].split(' 1')[0]\n",
    "#             group_names.append(group)\n",
    "#         #print(samples[idx])\n",
    "#         if group in samples[idx]:\n",
    "#             pass\n",
    "#         else:\n",
    "#             end.append(idx)\n",
    "#             start.append(idx + 1)\n",
    "#             name = samples[idx].split(' 1')[0]\n",
    "#             group = name\n",
    "#             group_names.append(name)\n",
    "#     end.append(len(samples))\n",
    "\n",
    "#     # print('start:', start)\n",
    "#     # print('end:', end)\n",
    "#     # print('group names:', group_names)\n",
    "\n",
    "#     stats_dict = {}\n",
    "#     for idx in range(len(group_names)):\n",
    "#         group_dict = {}\n",
    "#         #print(idx)\n",
    "#         group_df = short_tester.iloc[start[idx]:end[idx]]\n",
    "\n",
    "#         for col in headers_to_math:\n",
    "#             #print(group_df[col])\n",
    "#             name = col.split('_')[0]\n",
    "#             df_mean = group_df[col].mean()\n",
    "#             #print(df_mean)\n",
    "#             group_dict[name + '_mean'] = df_mean\n",
    "#             df_2se_perc = (2 * group_df[col].std())/df_mean * 100\n",
    "#             group_dict[name + '_2SE%'] = df_2se_perc \n",
    "\n",
    "#         stats_dict[group_names[idx]] = group_dict\n",
    "\n",
    "#     return pd.DataFrame(stats_dict)\n",
    "\n",
    "# #Functions for ratio plots\n",
    "# ###Still some hardcoded stuff in this...\n",
    "\n",
    "# def ratio_plot(group_namer, primary_std, choice, plot_dict, published_df):\n",
    "\n",
    "#     # Making groups of the standards for plotting\n",
    "#     worksheets = list(plot_dict.keys())\n",
    "#     group = None\n",
    "#     std_names = []\n",
    "#     start = [0]\n",
    "#     end = []\n",
    "#     for idx in range(len(worksheets)):\n",
    "#         if group == None:\n",
    "#             group = worksheets[idx].split('_')[0]\n",
    "#             std_names.append(group)\n",
    "#         #print(samples[idx])\n",
    "#         if worksheets[idx].split('_')[0] in group:\n",
    "#             pass\n",
    "#         else:\n",
    "#             end.append(idx)\n",
    "#             start.append(idx) \n",
    "#             name = worksheets[idx].split('_')[0]\n",
    "#             group = name\n",
    "#             std_names.append(name)\n",
    "#     end.append(len(worksheets))\n",
    "\n",
    "#     #print('start:', start)\n",
    "#     #print('end:', end)\n",
    "#     #print('std names:', std_names)\n",
    "\n",
    "#     std_dict = {}\n",
    "\n",
    "#     for idx in range(len(std_names)):\n",
    "#         group_dict = {}\n",
    "#         std_group = worksheets[start[idx]:end[idx]]\n",
    "#         #print('std_group', std_group)\n",
    "#         group_dict['0%'] = std_group[0]\n",
    "#         group_dict['20%'] = std_group[2]\n",
    "#         group_dict['40%'] = std_group[4]\n",
    "\n",
    "#         group_dict['0% group'] = std_group[1]\n",
    "#         group_dict['20% group'] = std_group[3]\n",
    "#         group_dict['40% group'] = std_group[5]\n",
    "\n",
    "#         std_dict[std_names[idx]] = group_dict\n",
    "\n",
    "\n",
    "#     #Creating loop for all entries in plot_dict\n",
    "\n",
    "#     plot_dict_grouped_dict = {}\n",
    "\n",
    "#     for key in plot_dict:\n",
    "\n",
    "#         if 'full' in key:\n",
    "\n",
    "#         #Create groups for plotting\n",
    "\n",
    "#             samples = plot_dict[key].index.values.tolist()\n",
    "#             group = None\n",
    "#             group_names = []\n",
    "#             start = [0]\n",
    "#             end = []\n",
    "#             for idx in range(len(samples)):\n",
    "#                 if group == None:\n",
    "#                     group = samples[idx].split(' 1')[0]\n",
    "#                     group_names.append(group)\n",
    "#                 #print(samples[idx])\n",
    "#                 if group in samples[idx]:\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     end.append(idx)\n",
    "#                     start.append(idx) # + 1 was original\n",
    "#                     name = samples[idx].split(' 1')[0]\n",
    "#                     group = name\n",
    "#                     group_names.append(name)\n",
    "#             end.append(len(samples))\n",
    "\n",
    "#             #print('start:', start)\n",
    "#             #print('end:', end)\n",
    "#             #print('group names:', group_names)\n",
    "\n",
    "#             group_df_list = []\n",
    "#             group_df_dict = {}\n",
    "\n",
    "#             for idx in range(len(group_names)):\n",
    "#                 group_dict = {}\n",
    "#                 #print(idx)\n",
    "#                 group_df = plot_dict[key].iloc[start[idx]:end[idx]]\n",
    "#                 #print(group_df.index.values.tolist())\n",
    "\n",
    "#                 group_df_list.append(group_df)\n",
    "#                 group_df_dict[group_names[idx]] = group_df\n",
    "\n",
    "#             plot_dict_grouped_dict[key] = group_df_dict\n",
    "\n",
    "#             if 'group' in key:\n",
    "#                  plot_dict_grouped_dict[key] = plot_dict[key]\n",
    "\n",
    "\n",
    "#     #Data to be plotted\n",
    "\n",
    "\n",
    "# #     group_namer = 'Mudtank'\n",
    "# #     primary_std = 'ttn'\n",
    "\n",
    "#     primary_std_name = {\n",
    "#         'glass': \"SRM NIST 610\",\n",
    "#         'glass612': \"SRM NIST 612\",\n",
    "#         'glassBHVO': \"BHVO-2G\",\n",
    "#         'ttn': \"MKED-1\",\n",
    "#     }\n",
    "\n",
    "#     test_zero = plot_dict_grouped_dict[ std_dict[primary_std]['0%']][group_namer]\n",
    "#     test_20 = plot_dict_grouped_dict[ std_dict[primary_std]['20%']][group_namer]\n",
    "#     test_40 = plot_dict_grouped_dict[ std_dict[primary_std]['40%']][group_namer]\n",
    "\n",
    "#     test_group_zero = plot_dict[std_dict[primary_std]['0% group']].loc[group_namer]\n",
    "#     test_group_20 = plot_dict[std_dict[primary_std]['20% group']].loc[group_namer]\n",
    "#     test_group_40 = plot_dict[std_dict[primary_std]['40% group']].loc[group_namer]\n",
    "\n",
    "\n",
    "#     if (group_namer == 'Bancroft') or (group_namer =='Yates'):\n",
    "#         #print('debug')\n",
    "#         pub_ratio = published_df.loc[['OLT-1', 'OLT-2', 'TCB']]\n",
    "#     else:\n",
    "#         pub_ratio = published_df.loc[group_namer]\n",
    "\n",
    "\n",
    "#     color_dict = {\n",
    "#         '0%': 'red',\n",
    "#         '20%': 'blue',\n",
    "#         '40%': 'green',\n",
    "#         'pub': 'purple'\n",
    "#     }\n",
    "\n",
    "\n",
    "#     #Initialize plots\n",
    "#     fig, axs = plt.subplots(2, 2, sharex = False, figsize = (24, 12))\n",
    "#     fig.suptitle(group_namer + ', Primary Std: ' + primary_std_name[primary_std], fontsize=28)\n",
    "\n",
    "#     #207/204 vs 206/204 plot\n",
    "#         #Results for each line\n",
    "#     x1_entry = '206/204 corrected'\n",
    "#     y1_entry = '207/204 corrected'\n",
    "#     ax = axs[0,0]\n",
    "#     plt.setp(ax.get_xticklabels(), fontsize=16)\n",
    "#     plt.setp(ax.get_yticklabels(), fontsize=16)\n",
    "\n",
    "#     ax.plot(test_zero[x1_entry], test_zero[y1_entry], 'o', color= color_dict['0%']) #0% reject\n",
    "#     ax.plot(test_20[x1_entry], test_20[y1_entry], 'o', color= color_dict['20%']) #20% reject\n",
    "#     ax.plot(test_40[x1_entry], test_40[y1_entry], 'o', color= color_dict['40%']) #40% reject\n",
    "#         #Group averages\n",
    "#     x1_entry = '206/204 corrected_mean'\n",
    "#     y1_entry = '207/204 corrected_mean'\n",
    "#     ax.plot(test_group_zero[x1_entry], test_group_zero[y1_entry], '^', color= color_dict['0%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "#     ax.plot(test_group_20[x1_entry], test_group_20[y1_entry], '^', color= color_dict['20%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "#     ax.plot(test_group_40[x1_entry], test_group_40[y1_entry], '^', color= color_dict['40%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "#         #Group 2sigma\n",
    "#     x_error = '206/204 corrected_2σ'\n",
    "#     y_error = '206/204 corrected_2σ'\n",
    "#     ax.errorbar(test_group_zero[x1_entry], test_group_zero[y1_entry],test_group_zero[x_error], test_group_zero[y_error],color = 'black')\n",
    "#     ax.errorbar(test_group_20[x1_entry], test_group_20[y1_entry],test_group_20[x_error], test_group_20[y_error],color = 'black')\n",
    "#     ax.errorbar(test_group_40[x1_entry], test_group_40[y1_entry],test_group_40[x_error], test_group_40[y_error],color = 'black')\n",
    "#         #Published value\n",
    "#     x1_entry = '206/204'\n",
    "#     y1_entry = '207/204'\n",
    "#     ax.plot(pub_ratio[x1_entry], pub_ratio[y1_entry], 's', color= color_dict['pub'], markersize=18, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "\n",
    "#         #Formatting\n",
    "#     ax.set(xlabel = '206/204', ylabel = '207/204',)\n",
    "#     ax.yaxis.label.set_size(20)\n",
    "#     ax.xaxis.label.set_size(20)\n",
    "\n",
    "\n",
    "#     #208/204 vs 206/204 plot\n",
    "\n",
    "#     x2_entry = '206/204 corrected'\n",
    "#     y2_entry = '208/204 corrected'\n",
    "#     ax = axs[0,1]\n",
    "#     plt.setp(ax.get_xticklabels(), fontsize=16)\n",
    "#     plt.setp(ax.get_yticklabels(), fontsize=16)\n",
    "\n",
    "#     ax.plot(test_zero[x2_entry], test_zero[y2_entry], 'o', color= color_dict['0%']) #0% reject\n",
    "#     ax.plot(test_20[x2_entry], test_20[y2_entry], 'o', color= color_dict['20%']) #20% reject\n",
    "#     ax.plot(test_40[x2_entry], test_40[y2_entry], 'o', color= color_dict['40%']) #40% reject\n",
    "\n",
    "#     x1_entry = '206/204 corrected_mean'\n",
    "#     y1_entry = '208/204 corrected_mean'\n",
    "#     ax.plot(test_group_zero[x1_entry], test_group_zero[y1_entry], '^', color= color_dict['0%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "#     ax.plot(test_group_20[x1_entry], test_group_20[y1_entry], '^', color= color_dict['20%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "#     ax.plot(test_group_40[x1_entry], test_group_40[y1_entry], '^', color= color_dict['40%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "\n",
    "#      #Group 2sigma\n",
    "#     x_error = '206/204 corrected_2σ'\n",
    "#     y_error = '208/204 corrected_2σ'\n",
    "#     ax.errorbar(test_group_zero[x1_entry], test_group_zero[y1_entry],test_group_zero[x_error], test_group_zero[y_error],color = 'black')\n",
    "#     ax.errorbar(test_group_20[x1_entry], test_group_20[y1_entry],test_group_20[x_error], test_group_20[y_error],color = 'black')\n",
    "#     ax.errorbar(test_group_40[x1_entry], test_group_40[y1_entry],test_group_40[x_error], test_group_40[y_error],color = 'black')\n",
    "\n",
    "#      #Published value\n",
    "#     x1_entry = '206/204'\n",
    "#     y1_entry = '208/204'\n",
    "#     ax.plot(pub_ratio[x1_entry], pub_ratio[y1_entry], 's', color= color_dict['pub'], markersize=18, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "\n",
    "#     #Formatting\n",
    "#     ax.set(xlabel = '206/204', ylabel = '208/204',)\n",
    "#     ax.yaxis.label.set_size(20)\n",
    "#     ax.xaxis.label.set_size(20)\n",
    "\n",
    "\n",
    "\n",
    "#     #207/206 vs 208/206 plot\n",
    "\n",
    "#     x3_entry = '208/206 corrected'\n",
    "#     y3_entry = '207/206 corrected'\n",
    "#     ax = axs[1,0]\n",
    "#     plt.setp(ax.get_xticklabels(), fontsize=16)\n",
    "#     plt.setp(ax.get_yticklabels(), fontsize=16)\n",
    "\n",
    "\n",
    "#     custom_lines = ax.plot(test_zero[x3_entry], test_zero[y3_entry], 'o', color= color_dict['0%']) #0% reject\n",
    "#     ax.plot(test_20[x3_entry], test_20[y3_entry], 'o', color= color_dict['20%']) #20% reject\n",
    "#     ax.plot(test_40[x3_entry], test_40[y3_entry], 'o', color= color_dict['40%']) #40% reject\n",
    "\n",
    "#     x1_entry = '208/206 corrected_mean'\n",
    "#     y1_entry = '207/206 corrected_mean'\n",
    "#     ax.plot(test_group_zero[x1_entry], test_group_zero[y1_entry], '^', color= color_dict['0%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "#     ax.plot(test_group_20[x1_entry], test_group_20[y1_entry], '^', color= color_dict['20%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "#     ax.plot(test_group_40[x1_entry], test_group_40[y1_entry], '^', color= color_dict['40%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "\n",
    "#      #Group 2sigma\n",
    "#     x_error = '208/206 corrected_2σ'\n",
    "#     y_error = '207/206 corrected_2σ'\n",
    "#     ax.errorbar(test_group_zero[x1_entry], test_group_zero[y1_entry],test_group_zero[x_error], test_group_zero[y_error],color = 'black')\n",
    "#     ax.errorbar(test_group_20[x1_entry], test_group_20[y1_entry],test_group_20[x_error], test_group_20[y_error],color = 'black')\n",
    "#     ax.errorbar(test_group_40[x1_entry], test_group_40[y1_entry],test_group_40[x_error], test_group_40[y_error],color = 'black')\n",
    "\n",
    "#      #Published value\n",
    "#     x1_entry = '208/206'\n",
    "#     y1_entry = '207/206'\n",
    "#     ax.plot(pub_ratio[x1_entry], pub_ratio[y1_entry], 's', color= color_dict['pub'], markersize=18, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "\n",
    "#     #Formatting\n",
    "#     ax.set(xlabel = '208/206', ylabel = '207/206',)\n",
    "#     ax.yaxis.label.set_size(20)\n",
    "#     ax.xaxis.label.set_size(20)\n",
    "\n",
    "\n",
    "#     #206/238 vs 208/232 plot\n",
    "\n",
    "#     x4_entry = '208/232 corrected'\n",
    "#     y4_entry = '206/238 corrected'\n",
    "#     ax = axs[1,1]\n",
    "#     plt.setp(ax.get_xticklabels(), fontsize=16)\n",
    "#     plt.setp(ax.get_yticklabels(), fontsize=16)\n",
    "\n",
    "#     ax.plot(test_zero[x4_entry], test_zero[y4_entry], 'o', color= color_dict['0%']) #0% reject\n",
    "#     ax.plot(test_20[x4_entry], test_20[y4_entry], 'o', color= color_dict['20%']) #20% reject\n",
    "#     ax.plot(test_40[x4_entry], test_40[y4_entry], 'o', color= color_dict['40%']) #40% reject\n",
    "\n",
    "#     x1_entry = '208/232 corrected_mean'\n",
    "#     y1_entry = '206/238 corrected_mean'\n",
    "#     ax.plot(test_group_zero[x1_entry], test_group_zero[y1_entry], '^', color= color_dict['0%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "#     ax.plot(test_group_20[x1_entry], test_group_20[y1_entry], '^', color= color_dict['20%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "#     ax.plot(test_group_40[x1_entry], test_group_40[y1_entry], '^', color= color_dict['40%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "\n",
    "#      #Group 2sigma\n",
    "#     x_error = '208/232 corrected_2σ'\n",
    "#     y_error = '206/238 corrected_2σ'\n",
    "#     ax.errorbar(test_group_zero[x1_entry], test_group_zero[y1_entry],test_group_zero[x_error], test_group_zero[y_error],color = 'black')\n",
    "#     ax.errorbar(test_group_20[x1_entry], test_group_20[y1_entry],test_group_20[x_error], test_group_20[y_error],color = 'black')\n",
    "#     ax.errorbar(test_group_40[x1_entry], test_group_40[y1_entry],test_group_40[x_error], test_group_40[y_error],color = 'black')\n",
    "\n",
    "#      #Published value\n",
    "#     x1_entry = '208/232'\n",
    "#     y1_entry = '206/238'\n",
    "#     ax.plot(pub_ratio[x1_entry], pub_ratio[y1_entry], 's', color= color_dict['pub'], markersize=18, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "\n",
    "#     #Formatting\n",
    "#     ax.set(xlabel = '208/232', ylabel = '206/238',)\n",
    "#     ax.yaxis.label.set_size(20)\n",
    "#     ax.xaxis.label.set_size(20)\n",
    "\n",
    "#     # Make a legend\n",
    "\n",
    "#     patch_0 = mpatches.Patch(color= color_dict['0%'], label= '0% Rejection')\n",
    "#     patch_20 = mpatches.Patch(color=color_dict['20%'], label= '20% Rejection')\n",
    "#     patch_40 = mpatches.Patch(color=color_dict['40%'], label= '40% Rejection')\n",
    "#     patch_pub = mpatches.Patch(color=color_dict['pub'], label= 'Published Data')\n",
    "#     plt.legend(handles=[patch_0, patch_20, patch_40, patch_pub])\n",
    "\n",
    "\n",
    "#     MYDIR = (\"Ratio_Figures\")\n",
    "#     CHECK_FOLDER = os.path.isdir(MYDIR)\n",
    "\n",
    "#     # If folder doesn't exist, then create it.\n",
    "#     if not CHECK_FOLDER:\n",
    "#         os.makedirs(MYDIR)\n",
    "#         #print(\"created folder : \", MYDIR)\n",
    "    \n",
    "#     new_string = group_namer + '_PrimaryStd_' + primary_std_name[primary_std]\n",
    "    \n",
    "#     filename = os.path.join(MYDIR, new_string + '.pdf')\n",
    "#     plt.savefig(filename)\n",
    "#     print('Plot for ', new_string, \" is complete.\")\n",
    "    \n",
    "#     if choice == False:\n",
    "#         plt.close()\n",
    "\n",
    "        \n",
    "        \n",
    "# #ratio_plot(group_namer, primary_std, choice = True, plot_dict = plot_dict, published_df = published_df)\n",
    "# #ratio_report(plot_dict = plot_dict, intro_filename = 'fake.pdf', intro = False, output_name = 'ratio_output.pdf'):\n",
    "    \n",
    "# def ratio_report(plot_dict, intro_filename, intro, output_name):\n",
    "    \n",
    "#     MYDIR = (\"Ratio_Figures\")\n",
    "#     mergedObject = PdfFileMerger()\n",
    "    \n",
    "#     if intro:\n",
    "#         mergedObject.append(PdfFileReader(intro_filename, 'rb'))\n",
    "#         print(f'Succesfully incorporated {intro_filename} into PDF.')\n",
    "    \n",
    "    \n",
    "#     #Actual loop of plotting\n",
    "    \n",
    "#     # Making groups of the standards for plotting\n",
    "#     worksheets = list(plot_dict.keys())\n",
    "#     group = None\n",
    "#     std_names = []\n",
    "#     start = [0]\n",
    "#     end = []\n",
    "#     for idx in range(len(worksheets)):\n",
    "#         if group == None:\n",
    "#             group = worksheets[idx].split('_')[0]\n",
    "#             std_names.append(group)\n",
    "#         #print(samples[idx])\n",
    "#         if worksheets[idx].split('_')[0] in group:\n",
    "#             pass\n",
    "#         else:\n",
    "#             end.append(idx)\n",
    "#             start.append(idx) \n",
    "#             name = worksheets[idx].split('_')[0]\n",
    "#             group = name\n",
    "#             std_names.append(name)\n",
    "#     end.append(len(worksheets))\n",
    "\n",
    "#     #print('start:', start)\n",
    "#     #print('end:', end)\n",
    "#     #print('std names:', std_names)\n",
    "\n",
    "#     group_list = None\n",
    "\n",
    "#     for key in plot_dict:\n",
    "\n",
    "#         if 'full' in key:\n",
    "\n",
    "#         #Create groups for plotting\n",
    "\n",
    "#             samples = plot_dict[key].index.values.tolist()\n",
    "#             group = None\n",
    "#             group_names = []\n",
    "#             start = [0]\n",
    "#             end = []\n",
    "#             for idx in range(len(samples)):\n",
    "#                 if group == None:\n",
    "#                     group = samples[idx].split(' 1')[0]\n",
    "#                     group_names.append(group)\n",
    "#                 #print(samples[idx])\n",
    "#                 if group in samples[idx]:\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     end.append(idx)\n",
    "#                     start.append(idx) # + 1 was original\n",
    "#                     name = samples[idx].split(' 1')[0]\n",
    "#                     group = name\n",
    "#                     group_names.append(name)\n",
    "#             end.append(len(samples))\n",
    "\n",
    "#             #print('start:', start)\n",
    "#             #print('end:', end)\n",
    "#             #print('group names:', group_names)\n",
    "#             group_list = group_names\n",
    "\n",
    "#     #print('group names:', group_list)\n",
    "\n",
    "#     primary_std_name = {\n",
    "#         'glass': \"SRM NIST 610\",\n",
    "#         'glass612': \"SRM NIST 612\",\n",
    "#         'glassBHVO': \"BHVO-2G\",\n",
    "#         'ttn': \"MKED-1\",\n",
    "#     }\n",
    "    \n",
    "# #ratio_plot(group_namer, primary_std, choice = True, plot_dict = plot_dict, published_df = published_df)    \n",
    "#     for std in std_names:\n",
    "#         #print(std)\n",
    "\n",
    "#         for group in group_list:\n",
    "#             if 'apa' in group:\n",
    "#                 print(group +' was skipped.')\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 #print(group)\n",
    "#                 ratio_plot(group, std, False, plot_dict, published_df)\n",
    "#                 new_string = group + '_PrimaryStd_' + primary_std_name[std]\n",
    "\n",
    "#                 filename = os.path.join(MYDIR, new_string + '.pdf')\n",
    "#                 mergedObject.append(PdfFileReader(filename, 'rb'))\n",
    "    \n",
    "#     if '.pdf' in output_name:\n",
    "#         pass\n",
    "#     else:\n",
    "#         output_name = output_name + '.pdf'\n",
    "    \n",
    "     \n",
    "#     mergedObject.write(output_name)\n",
    "\n",
    "#     print(f'PDF file named: {output_name} is complete.')   \n",
    "\n",
    "\n",
    "# def AB_error2(result_df):\n",
    "#     ''' Unfinished (and maybe broken) function toward calculating the Constant Ext Error from Isoplot?'''\n",
    "#     pd.set_option('mode.chained_assignment',None)\n",
    "    \n",
    "#     AB_err_tester = result_df\n",
    "#     result_headers = [\n",
    "#         '206/238 corrected',\n",
    "#         '208/232 corrected',\n",
    "#         '207/206 corrected',\n",
    "#         '208/206 corrected',\n",
    "#         '206/204 corrected',\n",
    "#         '207/204 corrected',\n",
    "#         '208/204 corrected'    \n",
    "#     ]\n",
    "\n",
    "#     for ratio in result_headers:\n",
    "#         #print(ratio)\n",
    "#         error_string = str(ratio.split()[0]) + '_after rejection 2σ%'\n",
    "#         new_string1 = ratio + ' 2σ'\n",
    "#         #print(error_string)\n",
    "#         new_string2 = ratio + ' x/(σ^2)'\n",
    "#         new_string3 = ratio + ' 1/(σ^2)'\n",
    "\n",
    "#         AB_err_tester[new_string1] = AB_err_tester.apply(lambda x: (x[error_string] / 100 ) * (x[ratio]), axis=1)\n",
    "\n",
    "#         AB_err_tester[new_string2] = AB_err_tester.apply(lambda x: x[ratio] / ((x[new_string1] / 2)**2), axis=1)\n",
    "#         AB_err_tester[new_string3] = AB_err_tester.apply(lambda x: (x[new_string2] / x[ratio]), axis=1)\n",
    "\n",
    "\n",
    "#     samples = AB_err_tester.index.values.tolist()\n",
    "#     group = None\n",
    "#     group_names = []\n",
    "#     start = [0]\n",
    "#     end = []\n",
    "#     for idx in range(len(samples)):\n",
    "#         if group == None:\n",
    "#             group = samples[idx].split(' 1')[0]\n",
    "#             group_names.append(group)\n",
    "#         #print(samples[idx])\n",
    "#         if group in samples[idx]:\n",
    "#             pass\n",
    "#         else:\n",
    "#             end.append(idx)\n",
    "#             start.append(idx) # + 1 was original\n",
    "#             name = samples[idx].split(' 1')[0]\n",
    "#             group = name\n",
    "#             group_names.append(name)\n",
    "#     end.append(len(samples))\n",
    "\n",
    "#     # print('start:', start)\n",
    "#     # print('end:', end)\n",
    "#     # print('group names:', group_names)\n",
    "\n",
    "#     stats_dict = {}\n",
    "#     corrected_stats_dict = {}\n",
    "#     group_df_list = []\n",
    "    \n",
    "#     for idx in range(len(group_names)):\n",
    "#         group_dict = {}\n",
    "#         #print(idx)\n",
    "#         group_df = AB_err_tester.iloc[start[idx]:end[idx]]\n",
    "#         #print(group_df.index.values.tolist())\n",
    "#         for ratio in result_headers:\n",
    "#             new_string2 = ratio + ' x/(σ^2)'\n",
    "#             new_string3 = ratio + ' 1/(σ^2)'\n",
    "#             new_string4 = ratio + ' Weighted Mean'\n",
    "#             new_string5 = ratio + ' σ^2'\n",
    "#             new_string6 = ratio + ' σ'\n",
    "#             wtd_mean = group_df[new_string2].sum() / group_df[new_string3].sum()\n",
    "#             sigma_sqrd = 1 / group_df[new_string3].sum()\n",
    "#             sigma = sigma_sqrd ** 0.5\n",
    "\n",
    "#             group_dict[new_string4] = wtd_mean\n",
    "#             group_dict[new_string5] = sigma_sqrd\n",
    "#             group_dict[new_string6] = sigma\n",
    "            \n",
    "#             z_string = str(ratio.split()[0]) + '_Z'\n",
    "#             group_df[z_string] = group_df.apply(lambda x: (((x[ratio] - group_dict[new_string4])**2) * x[new_string3] ), axis=1)\n",
    "            \n",
    "#             new_string5b = ratio +' S-factor'\n",
    "#             #new_string6 = ratio + ' count'\n",
    "#             new_string7b = ratio + ' MSWD'\n",
    "#             #new_string8 = ratio + ' MSWD_difference_from_1'\n",
    "\n",
    "#             S_factor = group_df[z_string].sum()\n",
    "#             MSWD = S_factor / (group_df[ratio].count() - 1)\n",
    "#             #MSWD_dif = 1 - MSWD\n",
    "\n",
    "#             group_dict[new_string5b] = S_factor\n",
    "#             group_dict[new_string7b] = MSWD\n",
    "        \n",
    "#         group_df_list.append(group_df)\n",
    "\n",
    "#         stats_dict[group_names[idx]] = group_dict\n",
    "\n",
    "#     grouper = pd.DataFrame(stats_dict) ### Will need to flip this at some point.\n",
    "#     new_AB_err_tester = pd.concat(group_df_list)\n",
    "    \n",
    "#     grouper_flip = pd.DataFrame.transpose(grouper)\n",
    "\n",
    "#     return new_AB_err_tester, grouper_flip \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
