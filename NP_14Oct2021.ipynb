{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Made by Khalil Droubi\n",
    "###To process Iolite baseline-subtracted NP-II files for BB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mod 9-1-2021:\n",
    "\n",
    "-Make sure that your sample names for MKED-1 match the code!\n",
    "\n",
    "\n",
    "\n",
    "To Do:\n",
    "\n",
    "-Do a Final Check if MSWD calculations are correct in AB_err function\n",
    "\n",
    "-Write function to chop lines up into smaller intervals, to test how long of a line we really need.\n",
    "\n",
    "-Make Age calculation work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress\n",
    "\n",
    "#Graphing stuff\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "\n",
    "# %pip install seaborn\n",
    "# import seaborn as sns\n",
    "\n",
    "#%pip install PyPDF2\n",
    "from PyPDF2 import PdfFileMerger, PdfFileReader\n",
    "\n",
    "#%pip install pdfkit\n",
    "import pdfkit\n",
    "\n",
    "#%pip install xlsxwriter\n",
    "import xlsxwriter\n",
    "\n",
    "#pd.set_option(\"display.precision\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc207_206(work_age):\n",
    "    \n",
    "    decay_238 = 1.55125E-10\n",
    "    decay_235 = 9.8485E-10\n",
    "    decay_232 = 4.9475E-11\n",
    "    \n",
    "    working_207_206 = (1/137.818)*(math.exp(decay_238*work_age)-1) / (math.exp(decay_235*work_age) -1)\n",
    "    return working_207_206\n",
    "\n",
    "def calc206_238(work_age):\n",
    "    \n",
    "    decay_238 = 1.55125E-10\n",
    "    \n",
    "    ratio = (math.exp(decay_238 * work_age)) -1\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "def age207vs206_calc(input207_206, age_guess = 1*(10**9), years_multiplier = 2):\n",
    "    work_age = age_guess\n",
    "    \n",
    "    \n",
    "    working_207_206 = calc207_206(work_age)\n",
    "\n",
    "    diff = input207_206 - working_207_206\n",
    "\n",
    "    if diff > 0:\n",
    "        #print('debug')\n",
    "        while abs(diff) > 0.00000000000000001:\n",
    "            #print('debug')\n",
    "            work_age = work_age + 1*(10**years_multiplier)\n",
    "            working_207_206 = calc207_206(work_age)\n",
    "            diff = input207_206 - working_207_206\n",
    "    if diff < 0:\n",
    "        while abs(diff) > 0.00000000000000001:\n",
    "            work_age = work_age - 1*(10**years_multiplier)\n",
    "            working_207_206 = calc207_206(work_age)\n",
    "            diff = input207_206 - working_207_206\n",
    "            \n",
    "    print('Successful calculation!?')        \n",
    "    return work_age\n",
    "\n",
    "def age206vs238_calc(input206_238, age_guess = 1*(10**9), years_multiplier = 2):\n",
    "    work_age = age_guess\n",
    "    \n",
    "    \n",
    "    working_206_238 = calc206_238(work_age)\n",
    "\n",
    "    diff = input206_238 - working_206_238\n",
    "\n",
    "    if diff > 0:\n",
    "        #print('debug')\n",
    "        while abs(diff) > 0.00000000000000001:\n",
    "          \n",
    "            work_age = work_age + 1*(10**years_multiplier)\n",
    "            working_206_238 = calc206_238(work_age)\n",
    "            diff = input206_238 - working_206_238\n",
    "    if diff < 0:\n",
    "        while abs(diff) > 0.00000000000000001:\n",
    "            \n",
    "            work_age = work_age - 1*(10**years_multiplier)\n",
    "            working_207_206 = calc206_238(work_age)\n",
    "            diff = input206_238 - working_206_238\n",
    "            \n",
    "            \n",
    "    return work_age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for calculation and EXCEL export\n",
    "\n",
    "def read_np2_timeseries(excel_file):\n",
    "    ''' Excel input file is your baseline corrected time series export from Iolite for the NP-II.'''\n",
    "    df = pd.read_excel(excel_file, sheet_name = None)\n",
    "    keys = df.keys()\n",
    "    header_row = 0\n",
    "    new_dict = {}\n",
    "    for key in keys:\n",
    "        if 'time' in key: #Kind of hard-coded right now, so if names get weird may need to change\n",
    "            \n",
    "            df_test = df[key]\n",
    "\n",
    "            df_test.columns = df_test.iloc[header_row]\n",
    "            df_test = df_test.drop(header_row)\n",
    "            df_test = df_test.reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "            new_string = key.split('time')[0].rstrip()\n",
    "            new_dict[new_string] = df_test #test1_new\n",
    "    return new_dict\n",
    "\n",
    "def calc_CPS(np2_dict):\n",
    "    columns = ['Absolute Time',\n",
    "     'Elapsed Time',\n",
    "     'm238_CPS',\n",
    "     'm232_CPS',\n",
    "     'm208_CPS',\n",
    "     'm207_CPS',\n",
    "     'm206_CPS',\n",
    "     'm204_CPS',\n",
    "     'm202_CPS']\n",
    "\n",
    "    new_col = ['Absolute Time',\n",
    "     'Elapsed Time',\n",
    "     '238_CPS',\n",
    "     '232_CPS',\n",
    "     '208_CPS',\n",
    "     '207_CPS',\n",
    "     '206_CPS',\n",
    "     '204_CPS',\n",
    "     '202_CPS']\n",
    "\n",
    "    cut_col = ['238_CPS',\n",
    "     '232_CPS',\n",
    "     '208_CPS',\n",
    "     '207_CPS',\n",
    "     '206_CPS',\n",
    "     '204_CPS',\n",
    "     '202_CPS']\n",
    "\n",
    "    calc_dict = {}\n",
    "    for key in np2_dict:\n",
    "        #print(key)\n",
    "        test_df1 = np2_dict[key]\n",
    "\n",
    "        for col in columns:\n",
    "                test_df2 = test_df1.apply(lambda x: x * 62500000 if 'CPS' in x.name else x)\n",
    "                test_df2 = test_df2[['Absolute Time',\n",
    "             'Elapsed Time',\n",
    "                 'm238_CPS',\n",
    "                 'm232_CPS',\n",
    "                 'm208_CPS',\n",
    "                 'm207_CPS',\n",
    "                 'm206_CPS',\n",
    "                 'm204_CPS',\n",
    "                 'm202_CPS',]]\n",
    "        test_df2.columns = new_col\n",
    "        test_df2 = test_df2[cut_col]\n",
    "        result = pd.concat([test_df1, test_df2], axis=1)\n",
    "        \n",
    "         #Calculating OPZ\n",
    "        result['OPZ_238'] = result.apply(lambda x: x['m238'] - x['m238_CPS'], axis=1)\n",
    "        result['OPZ_232'] = result.apply(lambda x: x['m232'] - x['m232_CPS'], axis=1)\n",
    "        result['OPZ_208'] = result.apply(lambda x: x['m208'] - x['m208_CPS'], axis=1)\n",
    "        result['OPZ_207'] = result.apply(lambda x: x['m207'] - x['m207_CPS'], axis=1)\n",
    "        result['OPZ_206'] = result.apply(lambda x: x['m206'] - x['m206_CPS'], axis=1)\n",
    "        result['OPZ_204'] = result.apply(lambda x: x['m204'] - x['m204_CPS'], axis=1)\n",
    "        result['OPZ_202'] = result.apply(lambda x: x['m202'] - x['m202_CPS'], axis=1)\n",
    "        result['OPZ_208/206'] = result.apply(lambda x: x['OPZ_208'] / x['OPZ_206'], axis=1)\n",
    "        result['OPZ_207/206'] = result.apply(lambda x: x['OPZ_207'] / x['OPZ_206'], axis=1)\n",
    "        result['OPZ_206/204_Hg-corrected'] = result.apply(lambda x: x['OPZ_206'] / (x['OPZ_204'] - (x['OPZ_202'] * 6.87/29.86)) , axis=1)\n",
    "       \n",
    "        #Calculating Signal-to-Noise Ratios [V]/[V]\n",
    "        result['SNR_238'] = result.apply(lambda x: x['m238_CPS']/ x['OPZ_238'], axis=1)\n",
    "        result['SNR_232'] = result.apply(lambda x: x['m232_CPS']/ x['OPZ_232'], axis=1)\n",
    "        result['SNR_208'] = result.apply(lambda x: x['m208_CPS']/ x['OPZ_208'], axis=1)\n",
    "        result['SNR_207'] = result.apply(lambda x: x['m207_CPS']/ x['OPZ_207'], axis=1)\n",
    "        result['SNR_206'] = result.apply(lambda x: x['m206_CPS']/ x['OPZ_206'], axis=1)\n",
    "        result['SNR_204'] = result.apply(lambda x: x['m204_CPS']/ x['OPZ_204'], axis=1)\n",
    "        result['SNR_202'] = result.apply(lambda x: x['m202_CPS']/ x['OPZ_202'], axis=1)\n",
    "        \n",
    "        #Calculating Ratios\n",
    "        result['206/238'] = result.apply(lambda x: x['206_CPS']/x['238_CPS'], axis=1)\n",
    "        result['208/232'] = result.apply(lambda x: x['208_CPS']/x['232_CPS'], axis=1)\n",
    "        result['207/206'] = result.apply(lambda x: x['207_CPS']/x['206_CPS'], axis=1)\n",
    "        result['208/206'] = result.apply(lambda x: x['208_CPS']/x['206_CPS'], axis=1)\n",
    "        result['206/204'] = result.apply(lambda x: x['206_CPS']/x['204_CPS'], axis=1)\n",
    "        result['208/204'] = result.apply(lambda x: x['208_CPS']/x['204_CPS'], axis=1)\n",
    "        result['207/204'] = result.apply(lambda x: x['207_CPS']/x['204_CPS'], axis=1)\n",
    "        \n",
    "        #Calculating OPZ CPS\n",
    "        result['OPZ_238_CPS'] = result.apply(lambda x: x['OPZ_238'] * 62500000, axis=1)\n",
    "        result['OPZ_232_CPS'] = result.apply(lambda x: x['OPZ_232'] * 62500000, axis=1)\n",
    "        result['OPZ_208_CPS'] = result.apply(lambda x: x['OPZ_208'] * 62500000, axis=1)\n",
    "        result['OPZ_207_CPS'] = result.apply(lambda x: x['OPZ_207'] * 62500000, axis=1)\n",
    "        result['OPZ_206_CPS'] = result.apply(lambda x: x['OPZ_206'] * 62500000, axis=1)\n",
    "        result['OPZ_204_CPS'] = result.apply(lambda x: x['OPZ_204'] * 62500000, axis=1)\n",
    "        result['OPZ_202_CPS'] = result.apply(lambda x: x['OPZ_202'] * 62500000, axis=1)\n",
    "        \n",
    "        calc_dict[key] = result\n",
    "    \n",
    "    return calc_dict\n",
    "\n",
    "def ranked_minimization(sheet, ratio, reject_percentage = 20):\n",
    "\n",
    "    mytest = tester[sheet].copy(deep=True)\n",
    "\n",
    "    df_mean_before = mytest[ratio].mean()\n",
    "    df_1std_before = mytest[ratio].std()\n",
    "    df_count_before = mytest[ratio].count()\n",
    "    df_2se_perc_before = (2 * mytest[ratio].sem()) / df_mean_before * 100\n",
    "\n",
    "    dif_mean = ratio + '_dif_from_mean'\n",
    "    dif_1SD = ratio + '_dif_from_1SD'\n",
    "    mytest[dif_mean] = mytest.apply(lambda x: abs(x[ratio] - df_mean_before), axis=1)\n",
    "    mytest[dif_1SD] = mytest.apply(lambda x: x[dif_mean] - df_1std_before, axis=1)\n",
    "\n",
    "\n",
    "    mytest2 = mytest.sort_values(by = dif_1SD, ascending = False)\n",
    "    #mytest2.head()\n",
    "\n",
    "    ratios_to_reject = int(mytest[ratio].count() * reject_percentage / 100)\n",
    "    #print(ratios_to_reject)\n",
    "\n",
    "    after_rejection = mytest2[ratios_to_reject:]\n",
    "\n",
    "    df_mean_after = after_rejection[ratio].mean()\n",
    "    df_1std_after = after_rejection[ratio].std()\n",
    "    df_count_after = after_rejection[ratio].count()\n",
    "    \n",
    "    #This is not 2SE%, should probably fix labels... KD 14 June 2021\n",
    "    df_2se_perc_after = (2 * after_rejection[ratio].std()) / df_mean_after * 100\n",
    "\n",
    "    # print(df_mean_after)\n",
    "    # print(df_1std_after)\n",
    "    # print(df_2se_perc_after)\n",
    "\n",
    "    results_dict = {}\n",
    "    \n",
    "    results_dict['avg_before'] = df_mean_before\n",
    "    results_dict['1sd_before'] = df_1std_before\n",
    "    results_dict['2se%_before'] = df_2se_perc_before\n",
    "    results_dict['avg_after'] = df_mean_after\n",
    "    results_dict['1sd_after'] = df_1std_after\n",
    "    results_dict['2σ%_after'] = df_2se_perc_after\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "def statistics_NP2(calc_dict):\n",
    "    calc_list = ['238_CPS', '232_CPS',\n",
    "           '208_CPS', '207_CPS', '206_CPS', '204_CPS', '202_CPS', '206/238',\n",
    "           '208/232', '207/206', '208/206', '206/204','208/204','207/204'  ]\n",
    "    mega_dict = {}\n",
    "\n",
    "    for sheet in calc_dict:\n",
    "        tester = calc_dict[sheet]\n",
    "        stats_dict = {}\n",
    "        for col in tester:\n",
    "\n",
    "            if col in calc_list:\n",
    "                #print(col)\n",
    "                if '/' in col:\n",
    "                    key = col + '_before rejection'\n",
    "                else:\n",
    "                    key = col + '_mean'\n",
    "                df_mean = tester[col].mean()\n",
    "                stats_dict[key] = df_mean\n",
    "                #This is not 2SE%, should probably fix labels... KD 14 June 2021\n",
    "                df_precision = (2 * tester[col].std()) / df_mean * 100\n",
    "                stats_dict[col + '_2σ%'] = df_precision\n",
    "            if 'OPZ' in col:\n",
    "                 stats_dict[col + '_mean'] = tester[col].mean()\n",
    "            if 'SNR' in col:\n",
    "                 stats_dict[col + '_mean'] = tester[col].mean() \n",
    "                    \n",
    "        stats_dict['Time (s)'] = tester['Elapsed Time'].max()\n",
    "        \n",
    "        #new_string = sheet.replace('time series data', '')\n",
    "        new_string = sheet.split('time')[0].rstrip()\n",
    "        mega_dict[new_string] = stats_dict\n",
    "\n",
    "    df_1 = pd.DataFrame(mega_dict)\n",
    "    df_flip = pd.DataFrame.transpose(df_1)\n",
    "    return df_flip\n",
    "\n",
    "def statistics_ranktest(calc_dict, reject_percentage = 20):\n",
    "    calc_list = ['238_CPS', '232_CPS',\n",
    "           '208_CPS', '207_CPS', '206_CPS', '204_CPS', '202_CPS', '206/238',\n",
    "           '208/232', '207/206', '208/206', '206/204','208/204','207/204'  ]\n",
    "    mega_dict = {}\n",
    "\n",
    "    for sheet in calc_dict:\n",
    "        tester = calc_dict[sheet]\n",
    "        stats_dict = {}\n",
    "        for col in tester:\n",
    "\n",
    "            if col in calc_list:\n",
    "                #print(col)\n",
    "                if '/' in col:\n",
    "                    key_bf = col + '_before rejection'\n",
    "                    key_af = col + '_after rejection'\n",
    "                    key_bf_se = col + '_before rejection 2se%'\n",
    "                    key_af_se = col + '_after rejection 2σ%'\n",
    "                    \n",
    "                    ranked_dict = ranked_minimization(sheet, col, reject_percentage)\n",
    "                    stats_dict[key_bf] = ranked_dict['avg_before']\n",
    "                    stats_dict[key_bf_se] = ranked_dict['2se%_before']\n",
    "                    stats_dict[key_af] = ranked_dict['avg_after']\n",
    "                    #This is not 2SE%, should probably fix labels... KD 14 June 2021\n",
    "                    stats_dict[key_af_se] = ranked_dict['2σ%_after']\n",
    "                else:\n",
    "                    key = col + '_mean'\n",
    "                    df_mean = tester[col].mean()\n",
    "                    stats_dict[key] = df_mean\n",
    "                    #This is not 2SE%, should probably fix labels... KD 14 June 2021\n",
    "                    df_precision = (2 * tester[col].std()) / df_mean * 100\n",
    "                    stats_dict[col + '_2σ%'] = df_precision\n",
    "            if 'OPZ' in col:\n",
    "                 stats_dict[col + '_mean'] = tester[col].mean()\n",
    "            if 'SNR' in col:\n",
    "                 stats_dict[col + '_mean'] = tester[col].mean()\n",
    "                    \n",
    "        stats_dict['Time (s)'] = tester['Elapsed Time'].max()\n",
    "        \n",
    "        #new_string = sheet.replace('time series data', '')\n",
    "        new_string = sheet.split('time')[0].rstrip()\n",
    "        mega_dict[new_string] = stats_dict\n",
    "\n",
    "    df_1 = pd.DataFrame(mega_dict)\n",
    "    df_flip = pd.DataFrame.transpose(df_1)\n",
    "    return df_flip\n",
    "\n",
    "### Getting rid of negatives\n",
    "\n",
    "def ranked_minimization2(sheet, ratio, reject_percentage = 20):\n",
    "    ''' Modified to exclude negative ratios BEFORE rejection.'''\n",
    "    \n",
    "    mytest = tester[sheet].copy(deep=True)\n",
    "\n",
    "    df_mean_before = mytest[ratio].mean()\n",
    "    df_1std_before = mytest[ratio].std()\n",
    "    df_count_before = mytest[ratio].count()\n",
    "    df_2se_perc_before = (2 * mytest[ratio].sem()) / df_mean_before * 100\n",
    "\n",
    "    dif_mean = ratio + '_dif_from_mean'\n",
    "    dif_1SD = ratio + '_dif_from_1SD'\n",
    "    mytest[dif_mean] = mytest.apply(lambda x: abs(x[ratio] - df_mean_before), axis=1)\n",
    "    mytest[dif_1SD] = mytest.apply(lambda x: x[dif_mean] - df_1std_before, axis=1)\n",
    "\n",
    "    mytest_noNeg = mytest[mytest[ratio] > 0]\n",
    "    \n",
    "    mytest2 = mytest_noNeg.sort_values(by = dif_1SD, ascending = False)\n",
    "    #mytest2.head()\n",
    "\n",
    "    ratios_to_reject = int(mytest_noNeg[ratio].count() * reject_percentage / 100)\n",
    "    #print(ratios_to_reject)\n",
    "\n",
    "    after_rejection = mytest2[ratios_to_reject:]\n",
    "    \n",
    "    \n",
    "\n",
    "    df_mean_after = after_rejection[ratio].mean()\n",
    "    df_1std_after = after_rejection[ratio].std()\n",
    "    df_count_after = after_rejection[ratio].count()\n",
    "   \n",
    "    df_2se_perc_after = (2 * after_rejection[ratio].sem()) / df_mean_after * 100\n",
    "    df_2sd_perc_after = (2 * after_rejection[ratio].std()) / df_mean_after * 100\n",
    "    \n",
    "\n",
    "    # print(df_mean_after)\n",
    "    # print(df_1std_after)\n",
    "    # print(df_2se_perc_after)\n",
    "\n",
    "    results_dict = {}\n",
    "    \n",
    "    results_dict['avg_before'] = df_mean_before\n",
    "    results_dict['1sd_before'] = df_1std_before\n",
    "    results_dict['2se%_before'] = df_2se_perc_before\n",
    "    results_dict['avg_after'] = df_mean_after\n",
    "    results_dict['1sd_after'] = df_1std_after\n",
    "    results_dict['2se%_after'] = df_2se_perc_after\n",
    "    results_dict['2σ%_after'] = df_2sd_perc_after\n",
    "    \n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "def statistics_ranktest2(calc_dict, reject_percentage = 20):\n",
    "    '''Incorporates the new ranked minimization.'''\n",
    "    calc_list = ['238_CPS', '232_CPS',\n",
    "           '208_CPS', '207_CPS', '206_CPS', '204_CPS', '202_CPS', '206/238',\n",
    "           '208/232', '207/206', '208/206', '206/204','208/204','207/204'  ]\n",
    "    mega_dict = {}\n",
    "\n",
    "    for sheet in calc_dict:\n",
    "        tester = calc_dict[sheet]\n",
    "        stats_dict = {}\n",
    "        for col in tester:\n",
    "\n",
    "            if col in calc_list:\n",
    "                #print(col)\n",
    "                if '/' in col:\n",
    "                    key_bf = col + '_before rejection'\n",
    "                    key_af = col + '_after rejection'\n",
    "                    key_bf_se = col + '_before rejection 2se%'\n",
    "                    key_af_se = col + '_after rejection 2se%'\n",
    "                    key_af_2sd = col + '_after rejection 2σ%'\n",
    "                    \n",
    "                    ranked_dict = ranked_minimization2(sheet, col, reject_percentage)\n",
    "                    stats_dict[key_bf] = ranked_dict['avg_before']\n",
    "                    stats_dict[key_bf_se] = ranked_dict['2se%_before']\n",
    "                    stats_dict[key_af] = ranked_dict['avg_after']\n",
    "                    \n",
    "                    stats_dict[key_af_se] = ranked_dict['2se%_after']\n",
    "                    stats_dict[key_af_2sd] = ranked_dict['2σ%_after']\n",
    "                else:\n",
    "                    key = col + '_mean'\n",
    "                    df_mean = tester[col].mean()\n",
    "                    stats_dict[key] = df_mean\n",
    "                    #This is not 2SE%, should probably fix labels... KD 14 June 2021\n",
    "                    df_precision = (2 * tester[col].std()) / df_mean * 100\n",
    "                    stats_dict[col + '_2σ%'] = df_precision\n",
    "            if 'OPZ' in col:\n",
    "                 stats_dict[col + '_mean'] = tester[col].mean()\n",
    "            if 'SNR' in col:\n",
    "                 stats_dict[col + '_mean'] = tester[col].mean()\n",
    "                \n",
    "        stats_dict['Time (s)'] = tester['Elapsed Time'].max()\n",
    "        \n",
    "        #new_string = sheet.replace('time series data', '')\n",
    "        new_string = sheet.split('time')[0].rstrip()\n",
    "        mega_dict[new_string] = stats_dict\n",
    "\n",
    "    df_1 = pd.DataFrame(mega_dict)\n",
    "    df_flip = pd.DataFrame.transpose(df_1)\n",
    "    return df_flip\n",
    "\n",
    "def stat_grouper(stats_df):\n",
    "    ''' Input dataframe that was output from statistics_ranktest2 function.'''\n",
    "    headers_to_keep = [\n",
    "        '206/238_after rejection',\n",
    "        '206/238_after rejection 2se%',\n",
    "        '208/232_after rejection',\n",
    "        '208/232_after rejection 2se%',\n",
    "        '207/206_after rejection',\n",
    "        '207/206_after rejection 2se%',\n",
    "        '208/206_after rejection',\n",
    "        '208/206_after rejection 2se%',\n",
    "        '206/204_after rejection',\n",
    "        '206/204_after rejection 2se%',\n",
    "        '208/204_after rejection',\n",
    "        '208/204_after rejection 2se%',\n",
    "        '207/204_after rejection',\n",
    "        '207/204_after rejection 2se%'   \n",
    "    ]\n",
    "\n",
    "    headers_to_math = [\n",
    "        '206/238_after rejection',   \n",
    "        '208/232_after rejection',  \n",
    "        '207/206_after rejection',    \n",
    "        '208/206_after rejection',    \n",
    "        '206/204_after rejection',    \n",
    "        '208/204_after rejection',   \n",
    "        '207/204_after rejection',\n",
    "    ]\n",
    "\n",
    "\n",
    "    short_tester = stats_df[headers_to_keep]\n",
    "\n",
    "    samples = short_tester.index.values.tolist()\n",
    "    group = None\n",
    "    group_names = []\n",
    "    start = [0]\n",
    "    end = []\n",
    "    for idx in range(len(samples)):\n",
    "        if group == None:\n",
    "            group = samples[idx].split(' 1')[0]\n",
    "            group_names.append(group)\n",
    "        #print(samples[idx])\n",
    "        if group in samples[idx]:\n",
    "            pass\n",
    "        else:\n",
    "            end.append(idx)\n",
    "            start.append(idx + 1)\n",
    "            name = samples[idx].split(' 1')[0]\n",
    "            group = name\n",
    "            group_names.append(name)\n",
    "    end.append(len(samples))\n",
    "\n",
    "    # print('start:', start)\n",
    "    # print('end:', end)\n",
    "    # print('group names:', group_names)\n",
    "\n",
    "    stats_dict = {}\n",
    "    for idx in range(len(group_names)):\n",
    "        group_dict = {}\n",
    "        #print(idx)\n",
    "        group_df = short_tester.iloc[start[idx]:end[idx]]\n",
    "\n",
    "        for col in headers_to_math:\n",
    "            #print(group_df[col])\n",
    "            name = col.split('_')[0]\n",
    "            df_mean = group_df[col].mean()\n",
    "            #print(df_mean)\n",
    "            group_dict[name + '_mean'] = df_mean\n",
    "            df_2se_perc = (2 * group_df[col].std())/df_mean * 100\n",
    "            group_dict[name + '_2SE%'] = df_2se_perc \n",
    "\n",
    "        stats_dict[group_names[idx]] = group_dict\n",
    "\n",
    "    return pd.DataFrame(stats_dict)\n",
    "\n",
    "def stat_correcter2(stats_df, std = 'glass'):\n",
    "    ''' Input dataframe that was output from statistics_ranktest2\n",
    "    function.'''\n",
    "    headers_to_keep = [\n",
    "        '206/238_after rejection',\n",
    "        '206/238_after rejection 2se%',\n",
    "        '206/238_after rejection 2σ%',\n",
    "        '208/232_after rejection',\n",
    "        '208/232_after rejection 2se%',\n",
    "        '208/232_after rejection 2σ%',\n",
    "        '207/206_after rejection',\n",
    "        '207/206_after rejection 2se%',\n",
    "        '207/206_after rejection 2σ%',\n",
    "        '208/206_after rejection',\n",
    "        '208/206_after rejection 2se%',\n",
    "        '208/206_after rejection 2σ%',\n",
    "        '206/204_after rejection',\n",
    "        '206/204_after rejection 2se%',\n",
    "        '206/204_after rejection 2σ%',\n",
    "        '208/204_after rejection',\n",
    "        '208/204_after rejection 2se%',\n",
    "        '208/204_after rejection 2σ%',\n",
    "        '207/204_after rejection',\n",
    "        '207/204_after rejection 2se%',\n",
    "        '207/204_after rejection 2σ%',\n",
    "    ]\n",
    "\n",
    "    headers_to_math = [\n",
    "        '206/238_after rejection',   \n",
    "        '208/232_after rejection',  \n",
    "        '207/206_after rejection',    \n",
    "        '208/206_after rejection',    \n",
    "        '206/204_after rejection',    \n",
    "        '208/204_after rejection',   \n",
    "        '207/204_after rejection',\n",
    "    ]\n",
    "    \n",
    "    ### Correction Math\n",
    "    \n",
    "    #Currently hard-coded, will need to define in function at some point...\n",
    "    \n",
    "    published_ratios_610 = {\n",
    "        '206/238': 0.258174418887103,\n",
    "        '208/232': 0.546910763260703,\n",
    "        '207/206': 0.909778846717897,\n",
    "        '208/206': 2.16900334369684,\n",
    "        '206/204': 17.047,\n",
    "        '207/204': 15.509,\n",
    "        '208/204': 36.975,\n",
    "        \n",
    "    }\n",
    "    #Mod 9-13-2021: Change these to measured values.\n",
    "    published_ratios_MKED = {\n",
    "        #'206/238': 0.265752478, #Total Sample from Spandler et al., 2015\n",
    "        '206/238': 0.2634133, #ID-ICPMS\n",
    "        '208/232': 0.079191124, #ID-ICPMS\n",
    "        #'207/206': 0.096, #Total Sample from Spandler et al., 2015\n",
    "        '207/206': 0.095909435, #ID-ICPMS\n",
    "        '208/206': 0.852420222, #ID-ICPMS\n",
    "        #'206/204': 9090.909, #Total Sample from Spandler et al., 2015\n",
    "        '206/204': 10474.84592, #ID-ICPMS\n",
    "        '207/204': 1004.661031, #ID-ICPMS\n",
    "        '208/204': 8928.948295, #ID-ICPMS     \n",
    "    }\n",
    "    published_ratios_612 = {\n",
    "        '206/238': 0.289090155580635,\n",
    "        '208/232': 0.598868250924868,\n",
    "        '207/206': 0.907335907335907,\n",
    "        '208/206': 2.16450216450216,\n",
    "        '206/204': 17.094,\n",
    "        '207/204': 15.51,\n",
    "        '208/204': 37,      \n",
    "    }\n",
    "    published_ratios_BHVO = {\n",
    "        '206/238': 1.24298582359808,\n",
    "        '208/232': 0.811596334965975,\n",
    "        '207/206': 0.831206960977953,\n",
    "        '208/206': 2.042918913147926,\n",
    "        '206/204': 18.733,\n",
    "        '207/204': 15.571,\n",
    "        '208/204': 38.27,      \n",
    "    }   \n",
    "    \n",
    "    \n",
    "    if std == 'glass':\n",
    "        #print('610 primary std')\n",
    "        published_ratios = published_ratios_610\n",
    "    elif std == '612':\n",
    "        #print('612 primary std')\n",
    "        published_ratios = published_ratios_612\n",
    "    elif std == 'BHVO':\n",
    "        #print('BHVO primary std')\n",
    "        published_ratios = published_ratios_BHVO    \n",
    "    else:\n",
    "        published_ratios = published_ratios_MKED \n",
    "    \n",
    "    short_tester = stats_df[headers_to_keep]\n",
    "    result = short_tester.copy(deep=True)\n",
    "\n",
    "    samples = short_tester.index.values.tolist()\n",
    "    group = None\n",
    "    group_names = []\n",
    "    start = [0]\n",
    "    end = []\n",
    "    for idx in range(len(samples)):\n",
    "        if group == None:\n",
    "            group = samples[idx].split(' 1')[0]\n",
    "            group_names.append(group)\n",
    "        #print(samples[idx])\n",
    "        if group in samples[idx]:\n",
    "            pass\n",
    "        else:\n",
    "            end.append(idx)\n",
    "           \n",
    "            start.append(idx)\n",
    "            name = samples[idx].split(' 1')[0]\n",
    "            group = name\n",
    "            group_names.append(name)\n",
    "    end.append(len(samples))\n",
    "\n",
    "    # print('start:', start)\n",
    "    # print('end:', end)\n",
    "    # print('group names:', group_names)\n",
    "\n",
    "    stats_dict = {}\n",
    "    corrected_stats_dict = {}\n",
    "    for idx in range(len(group_names)):\n",
    "        group_dict = {}\n",
    "        #print(idx)\n",
    "        group_df = short_tester.iloc[start[idx]:end[idx]]\n",
    "        #print(group_df.index.values.tolist())\n",
    "        for col in headers_to_math:\n",
    "            #print(group_df[col])\n",
    "            name = col.split('_')[0]\n",
    "            df_mean = group_df[col].mean()\n",
    "            #print(df_mean)\n",
    "            group_dict[name + '_mean'] = df_mean\n",
    "            #External error\n",
    "            df_2sd_perc = (2 * group_df[col].std())/df_mean * 100\n",
    "            \n",
    "            group_dict[name + '_2\\u03C3%'] = df_2sd_perc\n",
    "            \n",
    "            df_2se_perc = (2 * group_df[col].sem())/df_mean * 100\n",
    "            \n",
    "            group_dict[name + '_2se%'] = df_2se_perc\n",
    "            \n",
    "   \n",
    "\n",
    "        stats_dict[group_names[idx]] = group_dict\n",
    "    \n",
    "    grouper = pd.DataFrame(stats_dict)\n",
    "    #grouper['SRM NIST 610']['206/238_mean']\n",
    "    if std == 'glass':\n",
    "        #print('610 primary std')\n",
    "        primary_std = grouper['SRM NIST 610']\n",
    "    elif std == '612':\n",
    "        #print('612 primary std')\n",
    "        primary_std = grouper['SRM NIST 612']\n",
    "    elif std == 'BHVO':\n",
    "        #print('BHVO primary std')\n",
    "        primary_std = grouper['BHVO-2G']         \n",
    "    else:\n",
    "        primary_std = grouper['MKED-1']\n",
    "        \n",
    "    \n",
    "\n",
    "        #Applying correction Hard coding the primary standard\n",
    "    result['206/238 corrected'] = short_tester.apply(lambda x: x['206/238_after rejection']/ (primary_std['206/238_mean'] / published_ratios['206/238']), axis=1)\n",
    "    result['208/232 corrected'] = short_tester.apply(lambda x: x['208/232_after rejection']/ (primary_std['208/232_mean'] / published_ratios['208/232']), axis=1)\n",
    "    result['207/206 corrected'] = short_tester.apply(lambda x: x['207/206_after rejection']/ (primary_std['207/206_mean'] / published_ratios['207/206']), axis=1)\n",
    "    result['208/206 corrected'] = short_tester.apply(lambda x: x['208/206_after rejection']/(primary_std['208/206_mean'] / published_ratios['208/206']), axis=1)\n",
    "    result['206/204 corrected'] = short_tester.apply(lambda x: x['206/204_after rejection']/(primary_std['206/204_mean'] / published_ratios['206/204']), axis=1)\n",
    "    result['207/204 corrected'] = short_tester.apply(lambda x: x['207/204_after rejection']/(primary_std['207/204_mean'] / published_ratios['207/204']), axis=1)\n",
    "    result['208/204 corrected'] = short_tester.apply(lambda x: x['208/204_after rejection']/(primary_std['208/204_mean'] / published_ratios['208/204']), axis=1)\n",
    "\n",
    "#     #Making 2se for plotting in EXCEL\n",
    "    \n",
    "    result['206/238_after rejection 2se'] = short_tester.apply(lambda x: x['206/238_after rejection'] * x['206/238_after rejection 2se%'] / 100, axis=1)\n",
    "    result['208/232_after rejection 2se'] = short_tester.apply(lambda x: x['208/232_after rejection'] * x['208/232_after rejection 2se%'] / 100, axis=1)\n",
    "    result['207/206_after rejection 2se'] = short_tester.apply(lambda x: x['207/206_after rejection'] * x['207/206_after rejection 2se%'] / 100, axis=1)\n",
    "    result['208/206_after rejection 2se'] = short_tester.apply(lambda x: x['208/206_after rejection'] * x['208/206_after rejection 2se%'] / 100, axis=1)\n",
    "    result['206/204_after rejection 2se'] = short_tester.apply(lambda x: x['206/204_after rejection'] * x['206/204_after rejection 2se%'] / 100, axis=1)\n",
    "    result['207/204_after rejection 2se'] = short_tester.apply(lambda x: x['207/204_after rejection'] * x['207/204_after rejection 2se%'] / 100, axis=1)\n",
    "    result['208/204_after rejection 2se'] = short_tester.apply(lambda x: x['208/204_after rejection'] * x['208/204_after rejection 2se%'] / 100, axis=1)\n",
    "    \n",
    "    \n",
    "    #Age calculation\n",
    "    \n",
    "#     result['206/238 Age [Ma]'] = result.apply(lambda x: age206vs238_calc(x['206/238 corrected'], 500*(10**9), 6) / 1000000, axis=1)\n",
    "#     result['207/206 Age [Ma]'] = result.apply(lambda x: age207vs206_calc(x['207/206 corrected'], 500*(10**9), 6) / 1000000, axis=1)\n",
    "    \n",
    "    \n",
    "    result_headers = [\n",
    "        '206/238 corrected',\n",
    "        '208/232 corrected',\n",
    "        '207/206 corrected',\n",
    "        '208/206 corrected',\n",
    "        '206/204 corrected',\n",
    "        '207/204 corrected',\n",
    "        '208/204 corrected',\n",
    "#         '206/238 Age [Ma]',\n",
    "#         '207/206 Age [Ma]',\n",
    "    ]\n",
    "       \n",
    "    \n",
    "    for idx in range(len(group_names)):\n",
    "        group_dict = {}\n",
    "        #print(idx)\n",
    "        group_df = result.iloc[start[idx]:end[idx]]\n",
    "        #print(group_df.index.values.tolist())\n",
    "        #print(group_df)\n",
    "        for col in result_headers:\n",
    "            #print(group_df[col])\n",
    "            name = col.split('_')[0]\n",
    "            df_mean = group_df[col].mean()\n",
    "            #print(df_mean)\n",
    "            group_dict[name + '_mean'] = df_mean\n",
    "            df_2sd_perc = (2 * group_df[col].std())/df_mean * 100\n",
    "            group_dict[name + '_2\\u03C3%'] = df_2sd_perc \n",
    "            df_2sd = 2 * group_df[col].std()\n",
    "            group_dict[name + '_2\\u03C3'] = df_2sd\n",
    "            \n",
    "            df_2se_perc = (2 * group_df[col].sem())/df_mean * 100\n",
    "            group_dict[name + '_2se%'] = df_2se_perc\n",
    "            df_2se = 2 * group_df[col].sem()\n",
    "            group_dict[name + '_2se'] = df_2se\n",
    "        \n",
    "        \n",
    "\n",
    "        corrected_stats_dict[group_names[idx]] = group_dict\n",
    "  \n",
    "   \n",
    "    corrected_grouper = pd.DataFrame(corrected_stats_dict)\n",
    "   \n",
    "   \n",
    "    \n",
    "    grouper_flip = pd.DataFrame.transpose(grouper)\n",
    "    corrected_flip = pd.DataFrame.transpose(corrected_grouper)\n",
    "    grouper_list = [grouper_flip, corrected_flip]\n",
    "    \n",
    "    grouper_comb = pd.concat(grouper_list, axis=1)\n",
    "    \n",
    "    #### Propogating Errors\n",
    "    \n",
    "    if std == 'glass':\n",
    "        print('610 primary std')\n",
    "        external_err_std = grouper_comb.loc['SRM NIST 610']\n",
    "    elif std == '612':\n",
    "        print('612 primary std')\n",
    "        external_err_std = grouper_comb.loc['SRM NIST 612']\n",
    "    elif std == 'BHVO':\n",
    "        print('BHVO primary std')\n",
    "        external_err_std = grouper_comb.loc['BHVO-2G']    \n",
    "    else:\n",
    "        print('MKED primary std')\n",
    "        external_err_std = grouper_comb.loc['MKED-1']\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    result['206/238 corrected BB error%'] = short_tester.apply(lambda x: ((x['206/238_after rejection 2se%'])**2 + (external_err_std['206/238 corrected_2\\u03C3%'])**2)**0.5, axis=1)\n",
    "    result['208/232 corrected BB error%'] = short_tester.apply(lambda x: ((x['208/232_after rejection 2se%'])**2 + (external_err_std['208/232 corrected_2\\u03C3%'])**2)**0.5, axis=1)\n",
    "    result['207/206 corrected BB error%'] = short_tester.apply(lambda x: ((x['207/206_after rejection 2se%'])**2 + (external_err_std['207/206 corrected_2\\u03C3%'])**2)**0.5, axis=1)\n",
    "    result['208/206 corrected BB error%'] = short_tester.apply(lambda x: ((x['208/206_after rejection 2se%'])**2 + (external_err_std['208/206 corrected_2\\u03C3%'])**2)**0.5, axis=1)\n",
    "    result['206/204 corrected BB error%'] = short_tester.apply(lambda x: ((x['206/204_after rejection 2se%'])**2 + (external_err_std['206/204 corrected_2\\u03C3%'])**2)**0.5, axis=1)\n",
    "    result['207/204 corrected BB error%'] = short_tester.apply(lambda x: ((x['207/204_after rejection 2se%'])**2 + (external_err_std['207/204 corrected_2\\u03C3%'])**2)**0.5, axis=1)\n",
    "    result['208/204 corrected BB error%'] = short_tester.apply(lambda x: ((x['208/204_after rejection 2se%'])**2 + (external_err_std['208/204 corrected_2\\u03C3%'])**2)**0.5, axis=1)\n",
    " \n",
    "    result['206/238 corrected BB error'] = result.apply(lambda x: (x['206/238 corrected BB error%'])* x['206/238 corrected']/100, axis=1)\n",
    "    result['208/232 corrected BB error'] = result.apply(lambda x: (x['208/232 corrected BB error%'])* x['208/232 corrected']/100, axis=1)\n",
    "    result['207/206 corrected BB error'] = result.apply(lambda x: (x['207/206 corrected BB error%'])* x['207/206 corrected']/100, axis=1) \n",
    "    result['208/206 corrected BB error'] = result.apply(lambda x: (x['208/206 corrected BB error%'])* x['208/206 corrected']/100, axis=1)\n",
    "    result['206/204 corrected BB error'] = result.apply(lambda x: (x['206/204 corrected BB error%'])* x['206/204 corrected']/100, axis=1)   \n",
    "    result['207/204 corrected BB error'] = result.apply(lambda x: (x['207/204 corrected BB error%'])* x['207/204 corrected']/100, axis=1)\n",
    "    result['208/204 corrected BB error'] = result.apply(lambda x: (x['208/204 corrected BB error%'])* x['208/204 corrected']/100, axis=1)\n",
    "    \n",
    "   \n",
    "    \n",
    "    new_headers = [\n",
    "        '206/238 corrected BB error%',\n",
    "        '208/232 corrected BB error%',\n",
    "        '207/206 corrected BB error%',\n",
    "        '208/206 corrected BB error%',\n",
    "        '206/204 corrected BB error%',\n",
    "        '207/204 corrected BB error%',\n",
    "        '208/204 corrected BB error%',\n",
    "        '206/238 corrected BB error',\n",
    "        '208/232 corrected BB error',\n",
    "        '207/206 corrected BB error',\n",
    "        '208/206 corrected BB error',\n",
    "        '206/204 corrected BB error',\n",
    "        '207/204 corrected BB error',\n",
    "        '208/204 corrected BB error'    \n",
    "    ]\n",
    "    \n",
    "    new_stats_dict = {}\n",
    "    for idx in range(len(group_names)):\n",
    "        group_dict = {}\n",
    "        #print(idx)\n",
    "        group_df = result.iloc[start[idx]:end[idx]]\n",
    "        #print(group_df)\n",
    "        for col in new_headers:\n",
    "            #print(group_df[col])\n",
    "            #name = col.split('_')[0]\n",
    "            \n",
    "            df_mean = group_df[col].mean()\n",
    "            \n",
    "            #print(df_mean)\n",
    "            group_dict[col + '_mean'] = df_mean\n",
    "        \n",
    "        new_stats_dict[group_names[idx]] = group_dict \n",
    "            \n",
    "    extra_grouper = pd.DataFrame(new_stats_dict)\n",
    "    \n",
    "   \n",
    "    corrected_flip2 = pd.DataFrame.transpose(extra_grouper)\n",
    "    grouper_list2 = [grouper_comb, corrected_flip2]\n",
    "    \n",
    "    grouper_comb_final = pd.concat(grouper_list2, axis=1)\n",
    "    #print('GROUPER',grouper_comb_final.keys())\n",
    "    #print('RESULT', result.keys())\n",
    "    ############\n",
    "\n",
    "    \n",
    "    grouper_order = ['206/238_mean', '206/238_2σ%','206/238_2se%', '208/232_mean', '208/232_2σ%','208/232_2se%',\n",
    "       '207/206_mean', '207/206_2σ%','207/206_2se%', '208/206_mean', '208/206_2σ%', '208/206_2se%',\n",
    "       '206/204_mean', '206/204_2σ%','206/204_2se%', '208/204_mean', '208/204_2σ%','208/204_2se%',\n",
    "       '207/204_mean', '207/204_2σ%', '207/204_2se%',\n",
    "        '206/238 corrected_mean','206/238 corrected_2σ','206/238 corrected_2se%',\n",
    "       '206/238 corrected_2se',\n",
    "       '206/238 corrected_2σ%', '206/238 corrected BB error_mean', '206/238 corrected BB error%_mean',\n",
    "       '208/232 corrected_mean','208/232 corrected_2σ', '208/232 corrected_2σ%', '208/232 corrected_2se%', '208/232 corrected_2se','208/232 corrected BB error_mean', '208/232 corrected BB error%_mean',\n",
    "       '207/206 corrected_mean', '207/206 corrected_2σ', '207/206 corrected_2σ%','207/206 corrected_2se%',\n",
    "       '207/206 corrected_2se', '207/206 corrected BB error_mean','207/206 corrected BB error%_mean',\n",
    "        '208/206 corrected_mean', '208/206 corrected_2σ','208/206 corrected_2σ%', '208/206 corrected_2se%', '208/206 corrected_2se','208/206 corrected BB error_mean', '208/206 corrected BB error%_mean',\n",
    "        '206/204 corrected_mean','206/204 corrected_2σ','206/204 corrected_2σ%', '206/204 corrected_2se%',\n",
    "       '206/204 corrected_2se', '206/204 corrected BB error_mean', '206/204 corrected BB error%_mean',\n",
    "      '207/204 corrected_mean','207/204 corrected_2σ', '207/204 corrected_2σ%', '207/204 corrected_2se%', '207/204 corrected_2se', '207/204 corrected BB error_mean', '207/204 corrected BB error%_mean',\n",
    "      '208/204 corrected_mean', '208/204 corrected_2σ','208/204 corrected_2σ%','208/204 corrected_2se%',\n",
    "       '208/204 corrected_2se', '208/204 corrected BB error_mean', '208/204 corrected BB error%_mean', \n",
    "        #'206/238 Age [Ma]_mean','207/206 Age [Ma]_mean', \n",
    "        ]\n",
    "    \n",
    "    result_order = ['206/238_after rejection','206/238_after rejection 2σ%', '206/238_after rejection 2se%','206/238_after rejection 2se',\n",
    "       '208/232_after rejection','208/232_after rejection 2σ%', '208/232_after rejection 2se%','208/232_after rejection 2se',\n",
    "       '207/206_after rejection','207/206_after rejection 2σ%', '207/206_after rejection 2se%','207/206_after rejection 2se',\n",
    "       '208/206_after rejection','208/206_after rejection 2σ%', '208/206_after rejection 2se%','208/206_after rejection 2se',\n",
    "       '206/204_after rejection','206/204_after rejection 2σ%', '206/204_after rejection 2se%','206/204_after rejection 2se',\n",
    "       '208/204_after rejection','208/204_after rejection 2σ%', '208/204_after rejection 2se%','208/204_after rejection 2se',\n",
    "       '207/204_after rejection','207/204_after rejection 2σ%', '207/204_after rejection 2se%','207/204_after rejection 2se',\n",
    "       '206/238 corrected', '206/238 corrected BB error', '206/238 corrected BB error%',\n",
    "       '208/232 corrected', '208/232 corrected BB error','208/232 corrected BB error%',\n",
    "        '207/206 corrected', '207/206 corrected BB error', '207/206 corrected BB error%',\n",
    "       '208/206 corrected', '208/206 corrected BB error', '208/206 corrected BB error%',\n",
    "        '206/204 corrected', '206/204 corrected BB error', '206/204 corrected BB error%',\n",
    "        '207/204 corrected', '207/204 corrected BB error', '207/204 corrected BB error%',\n",
    "       '208/204 corrected',  '208/204 corrected BB error', '208/204 corrected BB error%',\n",
    "       # '206/238 Age [Ma]','207/206 Age [Ma]',\n",
    "                   ]\n",
    "        \n",
    "        \n",
    "    result_plotter = ['206/238 corrected', '206/238 corrected BB error', '206/238 corrected BB error%',\n",
    "       '208/232 corrected', '208/232 corrected BB error','208/232 corrected BB error%',\n",
    "        '207/206 corrected', '207/206 corrected BB error', '207/206 corrected BB error%',\n",
    "       '208/206 corrected', '208/206 corrected BB error', '208/206 corrected BB error%',\n",
    "        '206/204 corrected', '206/204 corrected BB error', '206/204 corrected BB error%',\n",
    "        '207/204 corrected', '207/204 corrected BB error', '207/204 corrected BB error%',\n",
    "       '208/204 corrected',  '208/204 corrected BB error', '208/204 corrected BB error%']\n",
    "           \n",
    "    grouper_plotter = ['206/238 corrected_mean','206/238 corrected_2σ',\n",
    "       '206/238 corrected BB error_mean',\n",
    "       '208/232 corrected_mean','208/232 corrected_2σ','208/232 corrected BB error_mean', \n",
    "       '207/206 corrected_mean', '207/206 corrected_2σ', '207/206 corrected BB error_mean',\n",
    "        '208/206 corrected_mean', '208/206 corrected_2σ','208/206 corrected BB error_mean', \n",
    "        '206/204 corrected_mean','206/204 corrected_2σ', '206/204 corrected BB error_mean', \n",
    "      '207/204 corrected_mean','207/204 corrected_2σ', '207/204 corrected BB error_mean', \n",
    "      '208/204 corrected_mean', '208/204 corrected_2σ', '208/204 corrected BB error_mean', \n",
    "        ]\n",
    "    \n",
    "    result = result[result_order]\n",
    "    grouper_comb_final = grouper_comb_final[grouper_order]\n",
    "                                                        \n",
    "    result_plot = result[result_plotter]\n",
    "    grouper_plot = grouper_comb_final[grouper_plotter]                                                    \n",
    "                                                                                                           \n",
    "    \n",
    "    return result, grouper_comb_final, result_plot, grouper_plot\n",
    "\n",
    "def AB_error2(result_df):\n",
    "    ''' Progress toward calculating the Constant Ext Error from Isoplot?'''\n",
    "    pd.set_option('mode.chained_assignment',None)\n",
    "    \n",
    "    AB_err_tester = result_df\n",
    "    result_headers = [\n",
    "        '206/238 corrected',\n",
    "        '208/232 corrected',\n",
    "        '207/206 corrected',\n",
    "        '208/206 corrected',\n",
    "        '206/204 corrected',\n",
    "        '207/204 corrected',\n",
    "        '208/204 corrected'    \n",
    "    ]\n",
    "\n",
    "    for ratio in result_headers:\n",
    "        #print(ratio)\n",
    "        error_string = str(ratio.split()[0]) + '_after rejection 2σ%'\n",
    "        new_string1 = ratio + ' 2σ'\n",
    "        #print(error_string)\n",
    "        new_string2 = ratio + ' x/(σ^2)'\n",
    "        new_string3 = ratio + ' 1/(σ^2)'\n",
    "\n",
    "        AB_err_tester[new_string1] = AB_err_tester.apply(lambda x: (x[error_string] / 100 ) * (x[ratio]), axis=1)\n",
    "\n",
    "        AB_err_tester[new_string2] = AB_err_tester.apply(lambda x: x[ratio] / ((x[new_string1] / 2)**2), axis=1)\n",
    "        AB_err_tester[new_string3] = AB_err_tester.apply(lambda x: (x[new_string2] / x[ratio]), axis=1)\n",
    "\n",
    "\n",
    "    samples = AB_err_tester.index.values.tolist()\n",
    "    group = None\n",
    "    group_names = []\n",
    "    start = [0]\n",
    "    end = []\n",
    "    for idx in range(len(samples)):\n",
    "        if group == None:\n",
    "            group = samples[idx].split(' 1')[0]\n",
    "            group_names.append(group)\n",
    "        #print(samples[idx])\n",
    "        if group in samples[idx]:\n",
    "            pass\n",
    "        else:\n",
    "            end.append(idx)\n",
    "            start.append(idx) # + 1 was original\n",
    "            name = samples[idx].split(' 1')[0]\n",
    "            group = name\n",
    "            group_names.append(name)\n",
    "    end.append(len(samples))\n",
    "\n",
    "    # print('start:', start)\n",
    "    # print('end:', end)\n",
    "    # print('group names:', group_names)\n",
    "\n",
    "    stats_dict = {}\n",
    "    corrected_stats_dict = {}\n",
    "    group_df_list = []\n",
    "    \n",
    "    for idx in range(len(group_names)):\n",
    "        group_dict = {}\n",
    "        #print(idx)\n",
    "        group_df = AB_err_tester.iloc[start[idx]:end[idx]]\n",
    "        #print(group_df.index.values.tolist())\n",
    "        for ratio in result_headers:\n",
    "            new_string2 = ratio + ' x/(σ^2)'\n",
    "            new_string3 = ratio + ' 1/(σ^2)'\n",
    "            new_string4 = ratio + ' Weighted Mean'\n",
    "            new_string5 = ratio + ' σ^2'\n",
    "            new_string6 = ratio + ' σ'\n",
    "            wtd_mean = group_df[new_string2].sum() / group_df[new_string3].sum()\n",
    "            sigma_sqrd = 1 / group_df[new_string3].sum()\n",
    "            sigma = sigma_sqrd ** 0.5\n",
    "\n",
    "            group_dict[new_string4] = wtd_mean\n",
    "            group_dict[new_string5] = sigma_sqrd\n",
    "            group_dict[new_string6] = sigma\n",
    "            \n",
    "            z_string = str(ratio.split()[0]) + '_Z'\n",
    "            group_df[z_string] = group_df.apply(lambda x: (((x[ratio] - group_dict[new_string4])**2) * x[new_string3] ), axis=1)\n",
    "            \n",
    "            new_string5b = ratio +' S-factor'\n",
    "            #new_string6 = ratio + ' count'\n",
    "            new_string7b = ratio + ' MSWD'\n",
    "            #new_string8 = ratio + ' MSWD_difference_from_1'\n",
    "\n",
    "            S_factor = group_df[z_string].sum()\n",
    "            MSWD = S_factor / (group_df[ratio].count() - 1)\n",
    "            #MSWD_dif = 1 - MSWD\n",
    "\n",
    "            group_dict[new_string5b] = S_factor\n",
    "            group_dict[new_string7b] = MSWD\n",
    "        \n",
    "        group_df_list.append(group_df)\n",
    "\n",
    "        stats_dict[group_names[idx]] = group_dict\n",
    "\n",
    "    grouper = pd.DataFrame(stats_dict) ### Will need to flip this at some point.\n",
    "    new_AB_err_tester = pd.concat(group_df_list)\n",
    "    \n",
    "    grouper_flip = pd.DataFrame.transpose(grouper)\n",
    "\n",
    "    return new_AB_err_tester, grouper_flip \n",
    "\n",
    "\n",
    "def stat_rank_and_correct(calc_dict, rank_perc = 20, std = 'glass'):\n",
    "    \n",
    "    stat_tester = statistics_ranktest2(calc_dict, rank_perc)\n",
    "    result, grouper_comb_final, result_plot, grouper_plot = stat_correcter2(stat_tester, std)\n",
    "    \n",
    "    #print('Debug')\n",
    "    stat1_order = ['238_CPS_mean', '238_CPS_2σ%', '232_CPS_mean', '232_CPS_2σ%',\n",
    "       '208_CPS_mean', '208_CPS_2σ%', '207_CPS_mean', '207_CPS_2σ%',\n",
    "       '206_CPS_mean', '206_CPS_2σ%', '204_CPS_mean', '204_CPS_2σ%',\n",
    "       '202_CPS_mean', '202_CPS_2σ%', 'OPZ_238_mean', 'OPZ_232_mean',\n",
    "       'OPZ_208_mean', 'OPZ_207_mean', 'OPZ_206_mean', 'OPZ_204_mean',\n",
    "       'OPZ_202_mean','OPZ_208/206_mean','OPZ_207/206_mean','OPZ_206/204_Hg-corrected_mean',\n",
    "        'SNR_238_mean', 'SNR_232_mean',\n",
    "       'SNR_208_mean', 'SNR_207_mean', 'SNR_206_mean', 'SNR_204_mean',\n",
    "       'SNR_202_mean', '206/238_before rejection',\n",
    "       '206/238_before rejection 2se%',  '208/232_before rejection',\n",
    "       '208/232_before rejection 2se%',  '207/206_before rejection',\n",
    "       '207/206_before rejection 2se%',  '208/206_before rejection',\n",
    "       '208/206_before rejection 2se%',  '206/204_before rejection',\n",
    "       '206/204_before rejection 2se%',  '208/204_before rejection',\n",
    "       '208/204_before rejection 2se%',  '207/204_before rejection',\n",
    "       '207/204_before rejection 2se%',  'Time (s)']\n",
    "    stat1 = stat_tester[stat1_order]\n",
    "    \n",
    "    stat2 = pd.concat([stat1, result], axis=1)\n",
    "    \n",
    "    stat_new, group2 = AB_error2(stat2)\n",
    "    \n",
    "    stat2_order = ['Time (s)','238_CPS_mean', '238_CPS_2σ%', '232_CPS_mean', '232_CPS_2σ%',\n",
    "    '208_CPS_mean', '208_CPS_2σ%', '207_CPS_mean', '207_CPS_2σ%',\n",
    "       '206_CPS_mean', '206_CPS_2σ%', '204_CPS_mean', '204_CPS_2σ%',\n",
    "       '202_CPS_mean', '202_CPS_2σ%', 'OPZ_238_mean', 'OPZ_232_mean',\n",
    "       'OPZ_208_mean', 'OPZ_207_mean', 'OPZ_206_mean', 'OPZ_204_mean',\n",
    "       'OPZ_202_mean', 'OPZ_208/206_mean','OPZ_207/206_mean','OPZ_206/204_Hg-corrected_mean',\n",
    "        'SNR_238_mean', 'SNR_232_mean',\n",
    "       'SNR_208_mean', 'SNR_207_mean', 'SNR_206_mean', 'SNR_204_mean',\n",
    "       'SNR_202_mean', '206/238_before rejection',\n",
    "       '206/238_before rejection 2se%',  '208/232_before rejection',\n",
    "       '208/232_before rejection 2se%',  '207/206_before rejection',\n",
    "       '207/206_before rejection 2se%',  '208/206_before rejection',\n",
    "       '208/206_before rejection 2se%',  '206/204_before rejection',\n",
    "       '206/204_before rejection 2se%',  '208/204_before rejection',\n",
    "       '208/204_before rejection 2se%',  '207/204_before rejection', '207/204_before rejection 2se%',  \n",
    "       '206/238_after rejection', '206/238_after rejection 2se%','206/238_after rejection 2se',\n",
    "       '208/232_after rejection', '208/232_after rejection 2se%','208/232_after rejection 2se',\n",
    "       '207/206_after rejection', '207/206_after rejection 2se%','207/206_after rejection 2se',\n",
    "       '208/206_after rejection', '208/206_after rejection 2se%','208/206_after rejection 2se',\n",
    "       '206/204_after rejection', '206/204_after rejection 2se%','206/204_after rejection 2se',\n",
    "       '208/204_after rejection', '208/204_after rejection 2se%','208/204_after rejection 2se',\n",
    "       '207/204_after rejection', '207/204_after rejection 2se%','207/204_after rejection 2se',\n",
    "       '206/238 corrected', '206/238 corrected 2σ','206/238 corrected x/(σ^2)', '206/238 corrected 1/(σ^2)', '206/238 corrected BB error', '206/238 corrected BB error%',\n",
    "       '208/232 corrected', '208/232 corrected 2σ', '208/232 corrected x/(σ^2)','208/232 corrected 1/(σ^2)', '208/232 corrected BB error','208/232 corrected BB error%',\n",
    "       '207/206 corrected', '207/206 corrected 2σ','207/206 corrected x/(σ^2)', '207/206 corrected 1/(σ^2)','207/206 corrected BB error', '207/206 corrected BB error%',\n",
    "       '208/206 corrected', '208/206 corrected 2σ', '208/206 corrected x/(σ^2)','208/206 corrected 1/(σ^2)', '208/206 corrected BB error', '208/206 corrected BB error%',\n",
    "       '206/204 corrected', '206/204 corrected 2σ', '206/204 corrected x/(σ^2)', '206/204 corrected 1/(σ^2)', '206/204 corrected BB error', '206/204 corrected BB error%',\n",
    "       '207/204 corrected','207/204 corrected 2σ', '207/204 corrected x/(σ^2)', '207/204 corrected 1/(σ^2)', '207/204 corrected BB error', '207/204 corrected BB error%',\n",
    "       '208/204 corrected', '208/204 corrected 2σ','208/204 corrected x/(σ^2)', '208/204 corrected 1/(σ^2)',  '208/204 corrected BB error', '208/204 corrected BB error%',          \n",
    "       '206/238_Z','208/232_Z', '207/206_Z', '208/206_Z', '206/204_Z', '207/204_Z','208/204_Z', \n",
    "                  # '206/238 Age [Ma]','207/206 Age [Ma]',\n",
    "        ]\n",
    "    \n",
    "    new_grouper_plot2 = group2[[\n",
    "        '206/238 corrected Weighted Mean','206/238 corrected MSWD',\n",
    "        '208/232 corrected Weighted Mean','208/232 corrected MSWD',\n",
    "        '207/206 corrected Weighted Mean','207/206 corrected MSWD',\n",
    "        '208/206 corrected Weighted Mean','208/206 corrected MSWD',\n",
    "        '206/204 corrected Weighted Mean','206/204 corrected MSWD',\n",
    "        '207/204 corrected Weighted Mean','207/204 corrected MSWD',\n",
    "        '208/204 corrected Weighted Mean','208/204 corrected MSWD',   \n",
    "    ]]\n",
    "    \n",
    "    new_grouper_plot = pd.concat([grouper_plot,new_grouper_plot2], axis = 1)\n",
    "    #print('DEBUG')\n",
    "    \n",
    "    stat2 = stat_new[stat2_order]\n",
    "    \n",
    "    group_new = pd.concat([grouper_comb_final, group2], axis = 1)\n",
    "    \n",
    "    return stat2, group_new, result_plot, new_grouper_plot\n",
    "\n",
    "\n",
    "def files_ranked_toEXCEL(calc_dict, excel_name):\n",
    "    stats = statistics_ranktest(calc_dict)\n",
    "    with pd.ExcelWriter(excel_name) as writer:\n",
    "        for sheet in calc_dict:\n",
    "            calc_dict[sheet].to_excel(writer, sheet_name = sheet, index = False)\n",
    "        \n",
    "        stats.to_excel(writer, sheet_name = 'Statistics', index = True)\n",
    "    new_filename = str(excel_name.split('.')[0]) + '_statistics.xlsx'\n",
    "    with pd.ExcelWriter(new_filename) as writer:\n",
    "        stats.to_excel(writer, sheet_name = 'Statistics', index = True)\n",
    "        \n",
    "        \n",
    "\n",
    "def files_process_toEXCEL(calc_dict, excel_name):\n",
    "    with pd.ExcelWriter(excel_name) as writer:\n",
    "        for sheet in calc_dict:\n",
    "            calc_dict[sheet].to_excel(writer, sheet_name = sheet, index = False)\n",
    "        \n",
    "        statistics_NP2(calc_dict).to_excel(writer, sheet_name = 'Statistics', index = True)\n",
    "        \n",
    "        chronologic(statistics_NP2(calc_dict)).to_excel(writer, sheet_name = 'Chrono_Statistics', index = True)\n",
    "        \n",
    "        worksheet = writer.sheets['Statistics']  # pull worksheet object\n",
    "        worksheet.set_column(0, 0, 24)\n",
    "        worksheet.freeze_panes(1,1)\n",
    "        \n",
    "        worksheet = writer.sheets['Chrono_Statistics']  # pull worksheet object\n",
    "        worksheet.set_column(0, 0, 24)\n",
    "        worksheet.freeze_panes(1,1)\n",
    "        \n",
    "def file_process_combine(filename):\n",
    "    calc_dict = calc_CPS(read_np2_timeseries(filename))\n",
    "    new_filename = str(filename.split('.')[0]) + '_processed.xlsx'\n",
    "    files_process_toEXCEL(calc_dict, new_filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for fractionation output .xlsx\n",
    "\n",
    "def frac_np2_timeseries(excel_file):\n",
    "    ''' Excel input file is your baseline corrected time series export from Iolite for the NP-II.'''\n",
    "    df = pd.read_excel(excel_file, sheet_name = None)\n",
    "    keys = df.keys()\n",
    "    header_row = 0\n",
    "    new_dict = {}\n",
    "    frac_dict = []\n",
    "    for key in keys:\n",
    "        if 'time' in key: #Kind of hard-coded right now, so if names get weird may need to change\n",
    "            \n",
    "            df_test = df[key]\n",
    "\n",
    "            df_test.columns = df_test.iloc[header_row]\n",
    "            df_test = df_test.drop(header_row)\n",
    "            df_test = df_test.reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "            new_string = key.split('time')[0].rstrip()\n",
    "            \n",
    "            #Adding column with SAMPLE ID\n",
    "            cols = list(df_test.columns.values)\n",
    "            df_test['SAMPLE ID'] = new_string\n",
    "            new_cols = ['SAMPLE ID'] + cols\n",
    "            df_test = df_test[new_cols]\n",
    "            \n",
    "            new_dict[new_string] = df_test #test1_new\n",
    "            frac_dict.append(new_string)\n",
    "\n",
    "    combo = new_dict[frac_dict[0]]\n",
    "    for idx in enumerate(frac_dict):\n",
    "        #print(idx[0])\n",
    "        if idx[0] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            combo = pd.concat([combo, new_dict[idx[1]]])\n",
    "    #print(frac_dict)\n",
    "    #print(combo)\n",
    "    return new_dict, combo\n",
    "\n",
    "def frac_calc_CPS(combo):\n",
    "    columns = ['Absolute Time',\n",
    "     'Elapsed Time',\n",
    "     'm238_CPS',\n",
    "     'm232_CPS',\n",
    "     'm208_CPS',\n",
    "     'm207_CPS',\n",
    "     'm206_CPS',\n",
    "     'm204_CPS',\n",
    "     'm202_CPS']\n",
    "\n",
    "    new_col = ['Absolute Time',\n",
    "     'Elapsed Time',\n",
    "     '238_CPS',\n",
    "     '232_CPS',\n",
    "     '208_CPS',\n",
    "     '207_CPS',\n",
    "     '206_CPS',\n",
    "     '204_CPS',\n",
    "     '202_CPS']\n",
    "\n",
    "    cut_col = ['238_CPS',\n",
    "     '232_CPS',\n",
    "     '208_CPS',\n",
    "     '207_CPS',\n",
    "     '206_CPS',\n",
    "     '204_CPS',\n",
    "     '202_CPS']\n",
    "\n",
    "#     calc_dict = {}\n",
    "#     for key in np2_dict:\n",
    "#         #print(key)\n",
    "#         test_df1 = np2_dict[key]\n",
    "    test_df1 = combo\n",
    "    for col in columns:\n",
    "            test_df2 = test_df1.apply(lambda x: x * 62500000 if 'CPS' in x.name else x)\n",
    "            test_df2 = test_df2[['Absolute Time',\n",
    "         'Elapsed Time',\n",
    "             'm238_CPS',\n",
    "             'm232_CPS',\n",
    "             'm208_CPS',\n",
    "             'm207_CPS',\n",
    "             'm206_CPS',\n",
    "             'm204_CPS',\n",
    "             'm202_CPS',]]\n",
    "    test_df2.columns = new_col\n",
    "    test_df2 = test_df2[cut_col]\n",
    "    result = pd.concat([test_df1, test_df2], axis=1)\n",
    "\n",
    "     #Calculating OPZ\n",
    "    result['OPZ_238'] = result.apply(lambda x: x['m238'] - x['m238_CPS'], axis=1)\n",
    "    result['OPZ_232'] = result.apply(lambda x: x['m232'] - x['m232_CPS'], axis=1)\n",
    "    result['OPZ_208'] = result.apply(lambda x: x['m208'] - x['m208_CPS'], axis=1)\n",
    "    result['OPZ_207'] = result.apply(lambda x: x['m207'] - x['m207_CPS'], axis=1)\n",
    "    result['OPZ_206'] = result.apply(lambda x: x['m206'] - x['m206_CPS'], axis=1)\n",
    "    result['OPZ_204'] = result.apply(lambda x: x['m204'] - x['m204_CPS'], axis=1)\n",
    "    result['OPZ_202'] = result.apply(lambda x: x['m202'] - x['m202_CPS'], axis=1)\n",
    "    result['OPZ_208/206'] = result.apply(lambda x: x['OPZ_208'] / x['OPZ_206'], axis=1)\n",
    "    result['OPZ_207/206'] = result.apply(lambda x: x['OPZ_207'] / x['OPZ_206'], axis=1)\n",
    "    result['OPZ_206/204_Hg-corrected'] = result.apply(lambda x: x['OPZ_206'] / (x['OPZ_204'] - (x['OPZ_202'] * 6.87/29.86)) , axis=1)\n",
    "\n",
    "    #Calculating Signal-to-Noise Ratios [V]/[V]\n",
    "    result['SNR_238'] = result.apply(lambda x: x['m238_CPS']/ x['OPZ_238'], axis=1)\n",
    "    result['SNR_232'] = result.apply(lambda x: x['m232_CPS']/ x['OPZ_232'], axis=1)\n",
    "    result['SNR_208'] = result.apply(lambda x: x['m208_CPS']/ x['OPZ_208'], axis=1)\n",
    "    result['SNR_207'] = result.apply(lambda x: x['m207_CPS']/ x['OPZ_207'], axis=1)\n",
    "    result['SNR_206'] = result.apply(lambda x: x['m206_CPS']/ x['OPZ_206'], axis=1)\n",
    "    result['SNR_204'] = result.apply(lambda x: x['m204_CPS']/ x['OPZ_204'], axis=1)\n",
    "    result['SNR_202'] = result.apply(lambda x: x['m202_CPS']/ x['OPZ_202'], axis=1)\n",
    "\n",
    "    #Calculating Ratios\n",
    "    result['206/238'] = result.apply(lambda x: x['206_CPS']/x['238_CPS'], axis=1)\n",
    "    result['208/232'] = result.apply(lambda x: x['208_CPS']/x['232_CPS'], axis=1)\n",
    "    result['207/206'] = result.apply(lambda x: x['207_CPS']/x['206_CPS'], axis=1)\n",
    "    result['208/206'] = result.apply(lambda x: x['208_CPS']/x['206_CPS'], axis=1)\n",
    "    result['206/204'] = result.apply(lambda x: x['206_CPS']/x['204_CPS'], axis=1)\n",
    "    result['208/204'] = result.apply(lambda x: x['208_CPS']/x['204_CPS'], axis=1)\n",
    "    result['207/204'] = result.apply(lambda x: x['207_CPS']/x['204_CPS'], axis=1)\n",
    "\n",
    "    \n",
    "    return result\n",
    "\n",
    "def fractionation_output(excel_file):\n",
    "    frac_tester, frac_combo = frac_np2_timeseries(excel_file)\n",
    "    combo_tester = frac_calc_CPS(frac_combo)\n",
    "    \n",
    "    new_filename = str(excel_file.split('.')[0]) + '_fractionation.xlsx'   \n",
    "    with pd.ExcelWriter(new_filename) as writer:\n",
    "        combo_tester.to_excel(writer, sheet_name = 'Fractionation_ALL', index = False)\n",
    "       \n",
    "        worksheet = writer.sheets['Fractionation_ALL']  # pull worksheet object\n",
    "        worksheet.set_column(0, 0, 24)\n",
    "        worksheet.freeze_panes(1,1)   \n",
    "        \n",
    "    return combo_tester\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functons for graphing and report generation\n",
    "def U_Pb_plots(calc_dict, sample, choice = True):\n",
    "    key_list = ['238_CPS', '232_CPS',\n",
    "       '208_CPS', '207_CPS', '206_CPS', '204_CPS', '202_CPS', '206/238',\n",
    "       '208/232', '207/206', '208/206', '206/204']\n",
    "    \n",
    "    zet = calc_dict[sample]\n",
    "    new_string = sample.split('time')[0].rstrip()\n",
    "    y_list = []\n",
    "    for key in key_list:\n",
    "        y_list.append(zet[key])\n",
    "    \n",
    "    x = zet['Elapsed Time']\n",
    "    \n",
    "    #mod 12 September 2021\n",
    "    y_238over232 = zet['238_CPS'] / zet['232_CPS']\n",
    "    new_list = ['238_CPS', '232_CPS',\n",
    "       '208_CPS', '207_CPS', '206_CPS', '204_CPS', '238/232', '206/238',\n",
    "       '208/232', '207/206', '208/206', '206/204']\n",
    "    \n",
    "    y_list[6] = y_238over232\n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(4, 3, sharex = True, figsize = (12, 12))\n",
    "    fig.suptitle(new_string, fontsize=24)\n",
    "    \n",
    "    ax_list = [\n",
    "        axs[0, 0], \n",
    "        axs[0, 1],   \n",
    "        axs[0, 2], \n",
    "        axs[1, 0], \n",
    "        axs[1, 1],\n",
    "        axs[1, 2],\n",
    "        axs[2, 0], \n",
    "        axs[2, 1], \n",
    "        axs[2, 2],\n",
    "        axs[3, 0], \n",
    "        axs[3, 1], \n",
    "        axs[3, 2]   \n",
    "        ]\n",
    "\n",
    "    axs[0, 0].plot(x, y_list[0])\n",
    "    axs[0, 1].plot(x, y_list[1])\n",
    "    axs[0, 2].plot(x, y_list[2])\n",
    "    axs[1, 0].plot(x, y_list[3])\n",
    "    axs[1, 1].plot(x, y_list[4])\n",
    "    axs[1, 2].plot(x, y_list[5])\n",
    "    axs[2, 0].plot(x,  y_list[6])\n",
    "    axs[2, 1].plot(x, y_list[7])\n",
    "    axs[2, 2].plot(x, y_list[8])\n",
    "    axs[3, 0].plot(x, y_list[9])\n",
    "    axs[3, 0].set(xlabel = 'Time (s)')\n",
    "    axs[3, 1].plot(x, y_list[10])\n",
    "    axs[3, 1].set(xlabel = 'Time (s)')\n",
    "    axs[3, 2].plot(x, y_list[11])\n",
    "    axs[3, 2].set(xlabel = 'Time (s)')\n",
    "    for idx in range(len(ax_list)):\n",
    "        ax_list[idx].ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "        ax_list[idx].set_title(new_list[idx])\n",
    "        y_mean = [np.mean(y_list[idx])]*len(x)\n",
    "        # Plot the average line\n",
    "        mean_line = ax_list[idx].plot(x,y_mean, label='Mean', linestyle='--', color = \"black\")\n",
    "        # Make a legend\n",
    "        legend = ax_list[idx].legend(loc='upper right')\n",
    "    \n",
    "    MYDIR = (\"Figures\")\n",
    "    CHECK_FOLDER = os.path.isdir(MYDIR)\n",
    "\n",
    "    # If folder doesn't exist, then create it.\n",
    "    if not CHECK_FOLDER:\n",
    "        os.makedirs(MYDIR)\n",
    "        #print(\"created folder : \", MYDIR)\n",
    "    \n",
    "    #new_string = sample.replace('time series data', '').rstrip()\n",
    "    \n",
    "    filename = os.path.join(MYDIR, new_string + '.pdf')\n",
    "    plt.savefig(filename)\n",
    "    print('Plot for ', new_string, \" is complete.\")\n",
    "    \n",
    "    if choice == False:\n",
    "        plt.close()\n",
    "    #else:\n",
    "        #plt.close()\n",
    "        \n",
    "\n",
    "def U_Pb_report(calc_dict, intro_filename, intro = False, output_name = 'U-Pb_output.pdf'):\n",
    "    MYDIR = (\"Figures\")\n",
    "    mergedObject = PdfFileMerger()\n",
    "    \n",
    "    if intro:\n",
    "        mergedObject.append(PdfFileReader(intro_filename, 'rb'))\n",
    "        print(f'Succesfully incorporated {intro_filename} into PDF.')\n",
    "    pd.set_option('precision', 2)\n",
    "    stats = statistics_NP2(calc_dict)\n",
    "    stat_dict = {}\n",
    "    stat_dict['stat1'] = stats.iloc[:, 14:]\n",
    "    stat_dict['stat2'] = stats.iloc[:, :8]\n",
    "    stat_dict['stat3'] = stats.iloc[:, 8:14]\n",
    "    html_list = []\n",
    "\n",
    "    for key in stat_dict:\n",
    "        name = key + \".pdf\"\n",
    "        stats_html = stat_dict[key].to_html()\n",
    "        pdfkit.from_string(stats_html, name)\n",
    "        mergedObject.append(PdfFileReader(name, 'rb'))\n",
    "\n",
    "    file_list = []\n",
    "    keys = calc_dict.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        #print(key)\n",
    "        U_Pb_plots(calc_dict, key, False)\n",
    "        new_string = key.split('time')[0].rstrip()\n",
    "        filename = os.path.join(MYDIR, new_string + '.pdf')\n",
    "\n",
    "        mergedObject.append(PdfFileReader(filename, 'rb'))\n",
    "\n",
    "    if '.pdf' in output_name:\n",
    "        pass\n",
    "    else:\n",
    "        output_name = output_name + '.pdf'\n",
    "    \n",
    "    #output_name = \"U-Pb_output.pdf\"  \n",
    "    mergedObject.write(output_name)\n",
    "\n",
    "    print(f'PDF file named: {output_name} is complete.')        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Published values dictionary\n",
    "\n",
    "published_ratios_610 = {\n",
    "    '206/238': 0.258174418887103,\n",
    "    '208/232': 0.546910763260703,\n",
    "    '207/206': 0.909778846717897,\n",
    "    '208/206': 2.16900334369684,\n",
    "    '206/204': 17.047,\n",
    "    '207/204': 15.509,\n",
    "    '208/204': 36.975,\n",
    "\n",
    "}\n",
    "published_ratios_612 = {\n",
    "    '206/238': 0.289090155580635,\n",
    "    '208/232': 0.598868250924868,\n",
    "    '207/206': 0.907335907335907,\n",
    "    '208/206': 2.16450216450216,\n",
    "    '206/204': 17.094,\n",
    "    '207/204': 15.51,\n",
    "    '208/204': 37,      \n",
    "}\n",
    "published_ratios_BHVO = {\n",
    "    '206/238': 1.24298582359808,\n",
    "    '208/232': 0.811596334965975,\n",
    "    '207/206': 0.831206960977953,\n",
    "    '208/206': 2.042918913147926,\n",
    "    '206/204': 18.733,\n",
    "    '207/204': 15.571,\n",
    "    '208/204': 38.27,      \n",
    "}   \n",
    "published_ratios_BCR_2G = {\n",
    "    '206/238': 1.90697056349179,\n",
    "    '208/232': 1.09082521297421,\n",
    "    '207/206': 0.832720490274447,\n",
    "    '208/206': 2.06394884092726,\n",
    "    '206/204': 18.765,\n",
    "    '207/204': 15.626,\n",
    "    '208/204': 38.73,      \n",
    "}\n",
    "published_ratios_GSD_1G = {\n",
    "    '206/238': 0.367307298285922,\n",
    "    '208/232': 0.706245210573853,\n",
    "    '207/206': 0.80417794575821,\n",
    "    '208/206': 1.98723121712038,\n",
    "    '206/204': 19.579,\n",
    "    '207/204': 15.745,\n",
    "    '208/204': 38.908,      \n",
    "}\n",
    "\n",
    "#Mod 9-1-2021: Changed GSE-1G values and MKED values\n",
    "published_ratios_GSE_1G = { \n",
    "    '206/238': 0.367307298285922, ###These are just the GSD-1G ratios\n",
    "    '208/232': 0.706245210573853, ###These are just the GSD-1G ratios\n",
    "    '207/206': 0.79232,\n",
    "    '208/206': 1.96460,\n",
    "    '206/204': 19.9250,\n",
    "    '207/204': 15.7870,\n",
    "    '208/204': 39.1450,      \n",
    "}\n",
    "#Mod 9-13-2021: Change these to measured values.\n",
    "published_ratios_MKED = {\n",
    "    #'206/238': 0.265752478, #Total Sample from Spandler et al., 2015\n",
    "    '206/238': 0.2634133, #ID-ICPMS\n",
    "    '208/232': 0.079191124, #ID-ICPMS\n",
    "    #'207/206': 0.096, #Total Sample from Spandler et al., 2015\n",
    "    '207/206': 0.095909435, #ID-ICPMS\n",
    "    '208/206': 0.852420222, #ID-ICPMS\n",
    "    #'206/204': 9090.909, #Total Sample from Spandler et al., 2015\n",
    "    '206/204': 10474.84592, #ID-ICPMS\n",
    "    '207/204': 1004.661031, #ID-ICPMS\n",
    "    '208/204': 8928.948295, #ID-ICPMS     \n",
    "}    \n",
    "published_ratios_BLR_1 = {\n",
    "    '206/238': 0.176364,\n",
    "    '208/232': 0.0533134203278787,\n",
    "    '207/206': 0.0743078,\n",
    "    '208/206': 0.17982,\n",
    "    '206/204': 975.36,\n",
    "    '207/204': 72.47686,\n",
    "    '208/204': 175.38924,      \n",
    "}\n",
    "published_ratios_OLT_1 = {\n",
    "    '206/238': 0.170681666666667,\n",
    "    '208/232': 0.0516589274888366,\n",
    "    '207/206': 0.0731801314551979,\n",
    "    '208/206': 0.58025,\n",
    "    '206/204': 1847.16667,\n",
    "    '207/204': 135.17590,\n",
    "    '208/204': 1071.81688,      \n",
    "}\n",
    "published_ratios_OLT_2 = {\n",
    "    '206/238': 0.16693,\n",
    "    '208/232': 0.05064,\n",
    "    '207/206': 0.07248,\n",
    "    '208/206': 0.63815,\n",
    "    '206/204': 1453.00000,\n",
    "    '207/204': 105.31206,\n",
    "    '208/204': 927.22713,      \n",
    "}\n",
    "published_ratios_TCB = {\n",
    "    '206/238': 0.17086,\n",
    "    '208/232': 0.05178,\n",
    "    '207/206': 0.07327,\n",
    "    '208/206': 0.63348,\n",
    "    '206/204': 2016.75000,\n",
    "    '207/204': 147.76034,\n",
    "    '208/204': 1277.56162,      \n",
    "}\n",
    "published_ratios_MudTank = {\n",
    "    '206/238': 0.052269449,\n",
    "    '208/232': 0.049381079,\n",
    "    '207/206': 0.071923689,\n",
    "    '208/206': 0.068164043,\n",
    "    '206/204': 766.0145618,\n",
    "    '207/204': 55.09459,\n",
    "    '208/204': 52.21465,      \n",
    "}\n",
    "###Needed to add stuff for apatite\n",
    "#WRONG DO NOT USE\n",
    "published_ratios_MT_apa = {\n",
    "    '206/238': 0.052269449,\n",
    "    '208/232': 0.049381079,\n",
    "    '207/206': 0.071923689,\n",
    "    '208/206': 0.068164043,\n",
    "    '206/204': 766.0145618,\n",
    "    '207/204': 55.09459,\n",
    "    '208/204': 52.21465,      \n",
    "}\n",
    "published_ratios_MT_apa = {\n",
    "    '206/238': 0.052269449,\n",
    "    '208/232': 0.049381079,\n",
    "    '207/206': 0.071923689,\n",
    "    '208/206': 0.068164043,\n",
    "    '206/204': 766.0145618,\n",
    "    '207/204': 55.09459,\n",
    "    '208/204': 52.21465,      \n",
    "}\n",
    "\n",
    "published_ratios_dict = {\n",
    "    'SRM NIST 610': published_ratios_610,\n",
    "    'SRM NIST 612': published_ratios_612,\n",
    "    'BHVO-2G': published_ratios_BHVO,\n",
    "    'BCR-2G': published_ratios_BCR_2G,\n",
    "    'GSD-1G': published_ratios_GSD_1G,\n",
    "    'GSE-1G': published_ratios_GSE_1G,\n",
    "    'Bear Lake': published_ratios_BLR_1,\n",
    "    'MKED-1': published_ratios_MKED,\n",
    "    'Mud Tank': published_ratios_MudTank,\n",
    "    'OLT-1': published_ratios_OLT_1,\n",
    "    'OLT-2': published_ratios_OLT_2,\n",
    "    'TCB': published_ratios_TCB,\n",
    "}\n",
    "\n",
    "published_df = pd.DataFrame(published_ratios_dict)\n",
    "published_df = pd.DataFrame.transpose(published_df)\n",
    "\n",
    "#Developing chronologic function\n",
    "\n",
    "#Check order in Iolite\n",
    "\n",
    "chrono_order = [\n",
    " 'SRM NIST 612 1.1',\n",
    " 'SRM NIST 612 1.2',\n",
    " 'SRM NIST 612 1.3',\n",
    " 'MKED-1 1.1',\n",
    " 'MKED-1 1.2',\n",
    " 'MKED-1 1.3',\n",
    " 'Bear Lake 1.1',\n",
    " 'Bear Lake 1.2',\n",
    " 'Bear Lake 1.3',\n",
    " 'Bancroft 1.1',\n",
    " 'Bancroft 1.2',\n",
    " 'Bancroft 1.3',  \n",
    " 'Yates 1.1',\n",
    " 'Yates 1.2',\n",
    " 'Yates 1.3',\n",
    " 'Plesoviche 1.1',\n",
    " 'Plesoviche 1.2',\n",
    " 'Plesoviche 1.3',\n",
    " 'SRM NIST 612 1.4',\n",
    " 'SRM NIST 612 1.5',\n",
    " 'SRM NIST 612 1.6',   \n",
    " 'MKED-1 1.4',\n",
    " 'MKED-1 1.5',\n",
    " 'MKED-1 1.6',\n",
    " '95-69 t30 1.1',\n",
    " '95-69 t30 1.2',\n",
    " '95-69 t30 1.3',\n",
    " 'Yates 1.1',\n",
    " 'Yates 1.2',\n",
    " 'Yates 1.3',    \n",
    " 'Bancroft 1.4',\n",
    " 'Bancroft 1.5',\n",
    " 'Bancroft 1.6',  \n",
    " 'Bear Lake 1.4',\n",
    " 'Bear Lake 1.5',\n",
    " 'Bear Lake 1.6',  \n",
    " 'MKED-1 1.7',\n",
    " 'MKED-1 1.8',\n",
    " 'MKED-1 1.9',     \n",
    " 'SRM NIST 612 1.7',\n",
    " 'SRM NIST 612 1.8',\n",
    " 'SRM NIST 612 1.9',          \n",
    " ]\n",
    "\n",
    "\n",
    "def chronologic(result_df, order = chrono_order):\n",
    "    new_result = result_df.copy(deep=True)\n",
    "    \n",
    "    return new_result.reindex(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for ratio plots\n",
    "###Still some hardcoded stuff in this...\n",
    "\n",
    "def ratio_plot(group_namer, primary_std, choice, plot_dict, published_df):\n",
    "\n",
    "    # Making groups of the standards for plotting\n",
    "    worksheets = list(plot_dict.keys())\n",
    "    group = None\n",
    "    std_names = []\n",
    "    start = [0]\n",
    "    end = []\n",
    "    for idx in range(len(worksheets)):\n",
    "        if group == None:\n",
    "            group = worksheets[idx].split('_')[0]\n",
    "            std_names.append(group)\n",
    "        #print(samples[idx])\n",
    "        if worksheets[idx].split('_')[0] in group:\n",
    "            pass\n",
    "        else:\n",
    "            end.append(idx)\n",
    "            start.append(idx) \n",
    "            name = worksheets[idx].split('_')[0]\n",
    "            group = name\n",
    "            std_names.append(name)\n",
    "    end.append(len(worksheets))\n",
    "\n",
    "    #print('start:', start)\n",
    "    #print('end:', end)\n",
    "    #print('std names:', std_names)\n",
    "\n",
    "    std_dict = {}\n",
    "\n",
    "    for idx in range(len(std_names)):\n",
    "        group_dict = {}\n",
    "        std_group = worksheets[start[idx]:end[idx]]\n",
    "        #print('std_group', std_group)\n",
    "        group_dict['0%'] = std_group[0]\n",
    "        group_dict['20%'] = std_group[2]\n",
    "        group_dict['40%'] = std_group[4]\n",
    "\n",
    "        group_dict['0% group'] = std_group[1]\n",
    "        group_dict['20% group'] = std_group[3]\n",
    "        group_dict['40% group'] = std_group[5]\n",
    "\n",
    "        std_dict[std_names[idx]] = group_dict\n",
    "\n",
    "\n",
    "    #Creating loop for all entries in plot_dict\n",
    "\n",
    "    plot_dict_grouped_dict = {}\n",
    "\n",
    "    for key in plot_dict:\n",
    "\n",
    "        if 'full' in key:\n",
    "\n",
    "        #Create groups for plotting\n",
    "\n",
    "            samples = plot_dict[key].index.values.tolist()\n",
    "            group = None\n",
    "            group_names = []\n",
    "            start = [0]\n",
    "            end = []\n",
    "            for idx in range(len(samples)):\n",
    "                if group == None:\n",
    "                    group = samples[idx].split(' 1')[0]\n",
    "                    group_names.append(group)\n",
    "                #print(samples[idx])\n",
    "                if group in samples[idx]:\n",
    "                    pass\n",
    "                else:\n",
    "                    end.append(idx)\n",
    "                    start.append(idx) # + 1 was original\n",
    "                    name = samples[idx].split(' 1')[0]\n",
    "                    group = name\n",
    "                    group_names.append(name)\n",
    "            end.append(len(samples))\n",
    "\n",
    "            #print('start:', start)\n",
    "            #print('end:', end)\n",
    "            #print('group names:', group_names)\n",
    "\n",
    "            group_df_list = []\n",
    "            group_df_dict = {}\n",
    "\n",
    "            for idx in range(len(group_names)):\n",
    "                group_dict = {}\n",
    "                #print(idx)\n",
    "                group_df = plot_dict[key].iloc[start[idx]:end[idx]]\n",
    "                #print(group_df.index.values.tolist())\n",
    "\n",
    "                group_df_list.append(group_df)\n",
    "                group_df_dict[group_names[idx]] = group_df\n",
    "\n",
    "            plot_dict_grouped_dict[key] = group_df_dict\n",
    "\n",
    "            if 'group' in key:\n",
    "                 plot_dict_grouped_dict[key] = plot_dict[key]\n",
    "\n",
    "\n",
    "    #Data to be plotted\n",
    "\n",
    "\n",
    "#     group_namer = 'Mudtank'\n",
    "#     primary_std = 'ttn'\n",
    "\n",
    "    primary_std_name = {\n",
    "        'glass': \"SRM NIST 610\",\n",
    "        'glass612': \"SRM NIST 612\",\n",
    "        'glassBHVO': \"BHVO-2G\",\n",
    "        'ttn': \"MKED-1\",\n",
    "    }\n",
    "\n",
    "    test_zero = plot_dict_grouped_dict[ std_dict[primary_std]['0%']][group_namer]\n",
    "    test_20 = plot_dict_grouped_dict[ std_dict[primary_std]['20%']][group_namer]\n",
    "    test_40 = plot_dict_grouped_dict[ std_dict[primary_std]['40%']][group_namer]\n",
    "\n",
    "    test_group_zero = plot_dict[std_dict[primary_std]['0% group']].loc[group_namer]\n",
    "    test_group_20 = plot_dict[std_dict[primary_std]['20% group']].loc[group_namer]\n",
    "    test_group_40 = plot_dict[std_dict[primary_std]['40% group']].loc[group_namer]\n",
    "\n",
    "\n",
    "    if (group_namer == 'Bancroft') or (group_namer =='Yates'):\n",
    "        #print('debug')\n",
    "        pub_ratio = published_df.loc[['OLT-1', 'OLT-2', 'TCB']]\n",
    "    else:\n",
    "        pub_ratio = published_df.loc[group_namer]\n",
    "\n",
    "\n",
    "    color_dict = {\n",
    "        '0%': 'red',\n",
    "        '20%': 'blue',\n",
    "        '40%': 'green',\n",
    "        'pub': 'purple'\n",
    "    }\n",
    "\n",
    "\n",
    "    #Initialize plots\n",
    "    fig, axs = plt.subplots(2, 2, sharex = False, figsize = (24, 12))\n",
    "    fig.suptitle(group_namer + ', Primary Std: ' + primary_std_name[primary_std], fontsize=28)\n",
    "\n",
    "    #207/204 vs 206/204 plot\n",
    "        #Results for each line\n",
    "    x1_entry = '206/204 corrected'\n",
    "    y1_entry = '207/204 corrected'\n",
    "    ax = axs[0,0]\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=16)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=16)\n",
    "\n",
    "    ax.plot(test_zero[x1_entry], test_zero[y1_entry], 'o', color= color_dict['0%']) #0% reject\n",
    "    ax.plot(test_20[x1_entry], test_20[y1_entry], 'o', color= color_dict['20%']) #20% reject\n",
    "    ax.plot(test_40[x1_entry], test_40[y1_entry], 'o', color= color_dict['40%']) #40% reject\n",
    "        #Group averages\n",
    "    x1_entry = '206/204 corrected_mean'\n",
    "    y1_entry = '207/204 corrected_mean'\n",
    "    ax.plot(test_group_zero[x1_entry], test_group_zero[y1_entry], '^', color= color_dict['0%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "    ax.plot(test_group_20[x1_entry], test_group_20[y1_entry], '^', color= color_dict['20%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "    ax.plot(test_group_40[x1_entry], test_group_40[y1_entry], '^', color= color_dict['40%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "        #Group 2sigma\n",
    "    x_error = '206/204 corrected_2σ'\n",
    "    y_error = '206/204 corrected_2σ'\n",
    "    ax.errorbar(test_group_zero[x1_entry], test_group_zero[y1_entry],test_group_zero[x_error], test_group_zero[y_error],color = 'black')\n",
    "    ax.errorbar(test_group_20[x1_entry], test_group_20[y1_entry],test_group_20[x_error], test_group_20[y_error],color = 'black')\n",
    "    ax.errorbar(test_group_40[x1_entry], test_group_40[y1_entry],test_group_40[x_error], test_group_40[y_error],color = 'black')\n",
    "        #Published value\n",
    "    x1_entry = '206/204'\n",
    "    y1_entry = '207/204'\n",
    "    ax.plot(pub_ratio[x1_entry], pub_ratio[y1_entry], 's', color= color_dict['pub'], markersize=18, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "\n",
    "        #Formatting\n",
    "    ax.set(xlabel = '206/204', ylabel = '207/204',)\n",
    "    ax.yaxis.label.set_size(20)\n",
    "    ax.xaxis.label.set_size(20)\n",
    "\n",
    "\n",
    "    #208/204 vs 206/204 plot\n",
    "\n",
    "    x2_entry = '206/204 corrected'\n",
    "    y2_entry = '208/204 corrected'\n",
    "    ax = axs[0,1]\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=16)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=16)\n",
    "\n",
    "    ax.plot(test_zero[x2_entry], test_zero[y2_entry], 'o', color= color_dict['0%']) #0% reject\n",
    "    ax.plot(test_20[x2_entry], test_20[y2_entry], 'o', color= color_dict['20%']) #20% reject\n",
    "    ax.plot(test_40[x2_entry], test_40[y2_entry], 'o', color= color_dict['40%']) #40% reject\n",
    "\n",
    "    x1_entry = '206/204 corrected_mean'\n",
    "    y1_entry = '208/204 corrected_mean'\n",
    "    ax.plot(test_group_zero[x1_entry], test_group_zero[y1_entry], '^', color= color_dict['0%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "    ax.plot(test_group_20[x1_entry], test_group_20[y1_entry], '^', color= color_dict['20%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "    ax.plot(test_group_40[x1_entry], test_group_40[y1_entry], '^', color= color_dict['40%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "\n",
    "     #Group 2sigma\n",
    "    x_error = '206/204 corrected_2σ'\n",
    "    y_error = '208/204 corrected_2σ'\n",
    "    ax.errorbar(test_group_zero[x1_entry], test_group_zero[y1_entry],test_group_zero[x_error], test_group_zero[y_error],color = 'black')\n",
    "    ax.errorbar(test_group_20[x1_entry], test_group_20[y1_entry],test_group_20[x_error], test_group_20[y_error],color = 'black')\n",
    "    ax.errorbar(test_group_40[x1_entry], test_group_40[y1_entry],test_group_40[x_error], test_group_40[y_error],color = 'black')\n",
    "\n",
    "     #Published value\n",
    "    x1_entry = '206/204'\n",
    "    y1_entry = '208/204'\n",
    "    ax.plot(pub_ratio[x1_entry], pub_ratio[y1_entry], 's', color= color_dict['pub'], markersize=18, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "\n",
    "    #Formatting\n",
    "    ax.set(xlabel = '206/204', ylabel = '208/204',)\n",
    "    ax.yaxis.label.set_size(20)\n",
    "    ax.xaxis.label.set_size(20)\n",
    "\n",
    "\n",
    "\n",
    "    #207/206 vs 208/206 plot\n",
    "\n",
    "    x3_entry = '208/206 corrected'\n",
    "    y3_entry = '207/206 corrected'\n",
    "    ax = axs[1,0]\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=16)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=16)\n",
    "\n",
    "\n",
    "    custom_lines = ax.plot(test_zero[x3_entry], test_zero[y3_entry], 'o', color= color_dict['0%']) #0% reject\n",
    "    ax.plot(test_20[x3_entry], test_20[y3_entry], 'o', color= color_dict['20%']) #20% reject\n",
    "    ax.plot(test_40[x3_entry], test_40[y3_entry], 'o', color= color_dict['40%']) #40% reject\n",
    "\n",
    "    x1_entry = '208/206 corrected_mean'\n",
    "    y1_entry = '207/206 corrected_mean'\n",
    "    ax.plot(test_group_zero[x1_entry], test_group_zero[y1_entry], '^', color= color_dict['0%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "    ax.plot(test_group_20[x1_entry], test_group_20[y1_entry], '^', color= color_dict['20%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "    ax.plot(test_group_40[x1_entry], test_group_40[y1_entry], '^', color= color_dict['40%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "\n",
    "     #Group 2sigma\n",
    "    x_error = '208/206 corrected_2σ'\n",
    "    y_error = '207/206 corrected_2σ'\n",
    "    ax.errorbar(test_group_zero[x1_entry], test_group_zero[y1_entry],test_group_zero[x_error], test_group_zero[y_error],color = 'black')\n",
    "    ax.errorbar(test_group_20[x1_entry], test_group_20[y1_entry],test_group_20[x_error], test_group_20[y_error],color = 'black')\n",
    "    ax.errorbar(test_group_40[x1_entry], test_group_40[y1_entry],test_group_40[x_error], test_group_40[y_error],color = 'black')\n",
    "\n",
    "     #Published value\n",
    "    x1_entry = '208/206'\n",
    "    y1_entry = '207/206'\n",
    "    ax.plot(pub_ratio[x1_entry], pub_ratio[y1_entry], 's', color= color_dict['pub'], markersize=18, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "\n",
    "    #Formatting\n",
    "    ax.set(xlabel = '208/206', ylabel = '207/206',)\n",
    "    ax.yaxis.label.set_size(20)\n",
    "    ax.xaxis.label.set_size(20)\n",
    "\n",
    "\n",
    "    #206/238 vs 208/232 plot\n",
    "\n",
    "    x4_entry = '208/232 corrected'\n",
    "    y4_entry = '206/238 corrected'\n",
    "    ax = axs[1,1]\n",
    "    plt.setp(ax.get_xticklabels(), fontsize=16)\n",
    "    plt.setp(ax.get_yticklabels(), fontsize=16)\n",
    "\n",
    "    ax.plot(test_zero[x4_entry], test_zero[y4_entry], 'o', color= color_dict['0%']) #0% reject\n",
    "    ax.plot(test_20[x4_entry], test_20[y4_entry], 'o', color= color_dict['20%']) #20% reject\n",
    "    ax.plot(test_40[x4_entry], test_40[y4_entry], 'o', color= color_dict['40%']) #40% reject\n",
    "\n",
    "    x1_entry = '208/232 corrected_mean'\n",
    "    y1_entry = '206/238 corrected_mean'\n",
    "    ax.plot(test_group_zero[x1_entry], test_group_zero[y1_entry], '^', color= color_dict['0%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "    ax.plot(test_group_20[x1_entry], test_group_20[y1_entry], '^', color= color_dict['20%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "    ax.plot(test_group_40[x1_entry], test_group_40[y1_entry], '^', color= color_dict['40%'], markersize=14, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "\n",
    "     #Group 2sigma\n",
    "    x_error = '208/232 corrected_2σ'\n",
    "    y_error = '206/238 corrected_2σ'\n",
    "    ax.errorbar(test_group_zero[x1_entry], test_group_zero[y1_entry],test_group_zero[x_error], test_group_zero[y_error],color = 'black')\n",
    "    ax.errorbar(test_group_20[x1_entry], test_group_20[y1_entry],test_group_20[x_error], test_group_20[y_error],color = 'black')\n",
    "    ax.errorbar(test_group_40[x1_entry], test_group_40[y1_entry],test_group_40[x_error], test_group_40[y_error],color = 'black')\n",
    "\n",
    "     #Published value\n",
    "    x1_entry = '208/232'\n",
    "    y1_entry = '206/238'\n",
    "    ax.plot(pub_ratio[x1_entry], pub_ratio[y1_entry], 's', color= color_dict['pub'], markersize=18, markeredgewidth=1.5, markeredgecolor= 'black') #0% reject\n",
    "\n",
    "    #Formatting\n",
    "    ax.set(xlabel = '208/232', ylabel = '206/238',)\n",
    "    ax.yaxis.label.set_size(20)\n",
    "    ax.xaxis.label.set_size(20)\n",
    "\n",
    "    # Make a legend\n",
    "\n",
    "    patch_0 = mpatches.Patch(color= color_dict['0%'], label= '0% Rejection')\n",
    "    patch_20 = mpatches.Patch(color=color_dict['20%'], label= '20% Rejection')\n",
    "    patch_40 = mpatches.Patch(color=color_dict['40%'], label= '40% Rejection')\n",
    "    patch_pub = mpatches.Patch(color=color_dict['pub'], label= 'Published Data')\n",
    "    plt.legend(handles=[patch_0, patch_20, patch_40, patch_pub])\n",
    "\n",
    "\n",
    "    MYDIR = (\"Ratio_Figures\")\n",
    "    CHECK_FOLDER = os.path.isdir(MYDIR)\n",
    "\n",
    "    # If folder doesn't exist, then create it.\n",
    "    if not CHECK_FOLDER:\n",
    "        os.makedirs(MYDIR)\n",
    "        #print(\"created folder : \", MYDIR)\n",
    "    \n",
    "    new_string = group_namer + '_PrimaryStd_' + primary_std_name[primary_std]\n",
    "    \n",
    "    filename = os.path.join(MYDIR, new_string + '.pdf')\n",
    "    plt.savefig(filename)\n",
    "    print('Plot for ', new_string, \" is complete.\")\n",
    "    \n",
    "    if choice == False:\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "        \n",
    "#ratio_plot(group_namer, primary_std, choice = True, plot_dict = plot_dict, published_df = published_df)\n",
    "#ratio_report(plot_dict = plot_dict, intro_filename = 'fake.pdf', intro = False, output_name = 'ratio_output.pdf'):\n",
    "    \n",
    "def ratio_report(plot_dict, intro_filename, intro, output_name):\n",
    "    \n",
    "    MYDIR = (\"Ratio_Figures\")\n",
    "    mergedObject = PdfFileMerger()\n",
    "    \n",
    "    if intro:\n",
    "        mergedObject.append(PdfFileReader(intro_filename, 'rb'))\n",
    "        print(f'Succesfully incorporated {intro_filename} into PDF.')\n",
    "    \n",
    "    \n",
    "    #Actual loop of plotting\n",
    "    \n",
    "    # Making groups of the standards for plotting\n",
    "    worksheets = list(plot_dict.keys())\n",
    "    group = None\n",
    "    std_names = []\n",
    "    start = [0]\n",
    "    end = []\n",
    "    for idx in range(len(worksheets)):\n",
    "        if group == None:\n",
    "            group = worksheets[idx].split('_')[0]\n",
    "            std_names.append(group)\n",
    "        #print(samples[idx])\n",
    "        if worksheets[idx].split('_')[0] in group:\n",
    "            pass\n",
    "        else:\n",
    "            end.append(idx)\n",
    "            start.append(idx) \n",
    "            name = worksheets[idx].split('_')[0]\n",
    "            group = name\n",
    "            std_names.append(name)\n",
    "    end.append(len(worksheets))\n",
    "\n",
    "    #print('start:', start)\n",
    "    #print('end:', end)\n",
    "    #print('std names:', std_names)\n",
    "\n",
    "    group_list = None\n",
    "\n",
    "    for key in plot_dict:\n",
    "\n",
    "        if 'full' in key:\n",
    "\n",
    "        #Create groups for plotting\n",
    "\n",
    "            samples = plot_dict[key].index.values.tolist()\n",
    "            group = None\n",
    "            group_names = []\n",
    "            start = [0]\n",
    "            end = []\n",
    "            for idx in range(len(samples)):\n",
    "                if group == None:\n",
    "                    group = samples[idx].split(' 1')[0]\n",
    "                    group_names.append(group)\n",
    "                #print(samples[idx])\n",
    "                if group in samples[idx]:\n",
    "                    pass\n",
    "                else:\n",
    "                    end.append(idx)\n",
    "                    start.append(idx) # + 1 was original\n",
    "                    name = samples[idx].split(' 1')[0]\n",
    "                    group = name\n",
    "                    group_names.append(name)\n",
    "            end.append(len(samples))\n",
    "\n",
    "            #print('start:', start)\n",
    "            #print('end:', end)\n",
    "            #print('group names:', group_names)\n",
    "            group_list = group_names\n",
    "\n",
    "    #print('group names:', group_list)\n",
    "\n",
    "    primary_std_name = {\n",
    "        'glass': \"SRM NIST 610\",\n",
    "        'glass612': \"SRM NIST 612\",\n",
    "        'glassBHVO': \"BHVO-2G\",\n",
    "        'ttn': \"MKED-1\",\n",
    "    }\n",
    "    \n",
    "#ratio_plot(group_namer, primary_std, choice = True, plot_dict = plot_dict, published_df = published_df)    \n",
    "    for std in std_names:\n",
    "        #print(std)\n",
    "\n",
    "        for group in group_list:\n",
    "            if 'apa' in group:\n",
    "                print(group +' was skipped.')\n",
    "                continue\n",
    "            else:\n",
    "                #print(group)\n",
    "                ratio_plot(group, std, False, plot_dict, published_df)\n",
    "                new_string = group + '_PrimaryStd_' + primary_std_name[std]\n",
    "\n",
    "                filename = os.path.join(MYDIR, new_string + '.pdf')\n",
    "                mergedObject.append(PdfFileReader(filename, 'rb'))\n",
    "    \n",
    "    if '.pdf' in output_name:\n",
    "        pass\n",
    "    else:\n",
    "        output_name = output_name + '.pdf'\n",
    "    \n",
    "     \n",
    "    mergedObject.write(output_name)\n",
    "\n",
    "    print(f'PDF file named: {output_name} is complete.')   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Start Here****\n",
    "\n",
    "Input Filename of Iolite-baseline corrected Time-Series .xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check order in Iolite\n",
    "\n",
    "chrono_order = [\n",
    " 'SRM NIST 612 1.1',\n",
    " 'SRM NIST 612 1.2',\n",
    " 'SRM NIST 612 1.3',\n",
    " 'MKED-1 1.1',\n",
    " 'MKED-1 1.2',\n",
    " 'MKED-1 1.3',\n",
    " 'Bear Lake 1.1',\n",
    " 'Bear Lake 1.2',\n",
    " 'Bear Lake 1.3',\n",
    " 'Bancroft 1.1',\n",
    " 'Bancroft 1.2',\n",
    " 'Bancroft 1.3',  \n",
    " 'Yates 1.1',\n",
    " 'Yates 1.2',\n",
    " 'Yates 1.3',\n",
    " 'Plesoviche 1.1',\n",
    " 'Plesoviche 1.2',\n",
    " 'Plesoviche 1.3',\n",
    " 'SRM NIST 612 1.4',\n",
    " 'SRM NIST 612 1.5',\n",
    " 'SRM NIST 612 1.6',   \n",
    " 'MKED-1 1.4',\n",
    " 'MKED-1 1.5',\n",
    " 'MKED-1 1.6',\n",
    " '95-69 t30 1.1',\n",
    " '95-69 t30 1.2',\n",
    " '95-69 t30 1.3',\n",
    " 'Yates 1.1',\n",
    " 'Yates 1.2',\n",
    " 'Yates 1.3',    \n",
    " 'Bancroft 1.4',\n",
    " 'Bancroft 1.5',\n",
    " 'Bancroft 1.6',  \n",
    " 'Bear Lake 1.4',\n",
    " 'Bear Lake 1.5',\n",
    " 'Bear Lake 1.6',  \n",
    " 'MKED-1 1.7',\n",
    " 'MKED-1 1.8',\n",
    " 'MKED-1 1.9',     \n",
    " 'SRM NIST 612 1.7',\n",
    " 'SRM NIST 612 1.8',\n",
    " 'SRM NIST 612 1.9',          \n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'NP2_20211014_mod3_baseline-subtracted.xlsx'\n",
    "date = '14Oct2021'\n",
    "test_df = read_np2_timeseries(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['95-69_t30 1.1', '95-69_t30 1.2', '95-69_t30 1.3', 'Bancroft 1.1', 'Bancroft 1.2', 'Bancroft 1.3', 'Bancroft 1.4', 'Bancroft 1.5', 'Bancroft 1.6', 'Bear Lake 1.1', 'Bear Lake 1.2', 'Bear Lake 1.3', 'Bear Lake 1.4', 'Bear Lake 1.5', 'Bear Lake 1.6', 'SRM NIST 612 1.1', 'SRM NIST 612 1.2', 'SRM NIST 612 1.3', 'SRM NIST 612 1.4', 'SRM NIST 612 1.5', 'SRM NIST 612 1.6', 'SRM NIST 612 1.7', 'SRM NIST 612 1.8', 'SRM NIST 612 1.9', 'MKED-1 1.1', 'MKED-1 1.2', 'MKED-1 1.3', 'MKED-1 1.4', 'MKED-1 1.5', 'MKED-1 1.6', 'MKED-1 1.7', 'MKED-1 1.8', 'MKED-1 1.9', 'Yates 1.1', 'Yates 1.2', 'Yates 1.3', 'Yates 1.4', 'Yates 1.5', 'Yates 1.6', 'Plesoviche 1.1', 'Plesoviche 1.2', 'Plesoviche 1.3'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester = calc_CPS(test_df)\n",
    "tester.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE ID</th>\n",
       "      <th>Absolute Time</th>\n",
       "      <th>Elapsed Time</th>\n",
       "      <th>m238</th>\n",
       "      <th>m232</th>\n",
       "      <th>m208</th>\n",
       "      <th>m207</th>\n",
       "      <th>m206</th>\n",
       "      <th>m204</th>\n",
       "      <th>m202</th>\n",
       "      <th>...</th>\n",
       "      <th>SNR_206</th>\n",
       "      <th>SNR_204</th>\n",
       "      <th>SNR_202</th>\n",
       "      <th>206/238</th>\n",
       "      <th>208/232</th>\n",
       "      <th>207/206</th>\n",
       "      <th>208/206</th>\n",
       "      <th>206/204</th>\n",
       "      <th>208/204</th>\n",
       "      <th>207/204</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95-69_t30 1.1</td>\n",
       "      <td>2021-10-14 15:25:31.200</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>528.530515</td>\n",
       "      <td>0.104675</td>\n",
       "      <td>-0.079461</td>\n",
       "      <td>0.483094</td>\n",
       "      <td>0.262237</td>\n",
       "      <td>0.166237</td>\n",
       "      <td>1.099762</td>\n",
       "      <td>1201.825093</td>\n",
       "      <td>1321.721567</td>\n",
       "      <td>199.787241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95-69_t30 1.1</td>\n",
       "      <td>2021-10-14 15:25:31.400</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>...</td>\n",
       "      <td>478.986137</td>\n",
       "      <td>0.072719</td>\n",
       "      <td>0.008978</td>\n",
       "      <td>0.479268</td>\n",
       "      <td>0.243479</td>\n",
       "      <td>0.163913</td>\n",
       "      <td>1.111594</td>\n",
       "      <td>1567.812421</td>\n",
       "      <td>1742.771162</td>\n",
       "      <td>256.985380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95-69_t30 1.1</td>\n",
       "      <td>2021-10-14 15:25:31.600</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>-0.000362</td>\n",
       "      <td>0.00271</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>...</td>\n",
       "      <td>461.633933</td>\n",
       "      <td>0.129567</td>\n",
       "      <td>-0.007103</td>\n",
       "      <td>0.457327</td>\n",
       "      <td>0.244397</td>\n",
       "      <td>0.165967</td>\n",
       "      <td>1.089818</td>\n",
       "      <td>848.053930</td>\n",
       "      <td>924.224241</td>\n",
       "      <td>140.749146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95-69_t30 1.1</td>\n",
       "      <td>2021-10-14 15:25:31.800</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>-0.000856</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>...</td>\n",
       "      <td>447.457768</td>\n",
       "      <td>0.115371</td>\n",
       "      <td>-0.003083</td>\n",
       "      <td>0.469640</td>\n",
       "      <td>0.249383</td>\n",
       "      <td>0.163917</td>\n",
       "      <td>1.096063</td>\n",
       "      <td>923.160452</td>\n",
       "      <td>1011.842038</td>\n",
       "      <td>151.321905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95-69_t30 1.1</td>\n",
       "      <td>2021-10-14 15:25:32.000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>-0.001459</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.00246</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>...</td>\n",
       "      <td>457.283286</td>\n",
       "      <td>-0.030260</td>\n",
       "      <td>0.053195</td>\n",
       "      <td>0.487798</td>\n",
       "      <td>0.270579</td>\n",
       "      <td>0.166895</td>\n",
       "      <td>1.097159</td>\n",
       "      <td>-3596.971722</td>\n",
       "      <td>-3946.450123</td>\n",
       "      <td>-600.318234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Plesoviche 1.3</td>\n",
       "      <td>2021-10-14 15:13:35.800</td>\n",
       "      <td>34</td>\n",
       "      <td>0.045282</td>\n",
       "      <td>-0.009137</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>387.034385</td>\n",
       "      <td>0.031810</td>\n",
       "      <td>0.157323</td>\n",
       "      <td>0.043259</td>\n",
       "      <td>0.023504</td>\n",
       "      <td>0.038401</td>\n",
       "      <td>0.025238</td>\n",
       "      <td>2937.692467</td>\n",
       "      <td>74.142850</td>\n",
       "      <td>112.810400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Plesoviche 1.3</td>\n",
       "      <td>2021-10-14 15:13:36.000</td>\n",
       "      <td>34.2</td>\n",
       "      <td>0.044659</td>\n",
       "      <td>-0.009184</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>381.188169</td>\n",
       "      <td>-0.017650</td>\n",
       "      <td>-0.050181</td>\n",
       "      <td>0.043155</td>\n",
       "      <td>0.022337</td>\n",
       "      <td>0.039220</td>\n",
       "      <td>0.023859</td>\n",
       "      <td>-5214.531862</td>\n",
       "      <td>-124.413994</td>\n",
       "      <td>-204.513471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Plesoviche 1.3</td>\n",
       "      <td>2021-10-14 15:13:36.200</td>\n",
       "      <td>34.4</td>\n",
       "      <td>0.04136</td>\n",
       "      <td>-0.009439</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.001942</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>353.814728</td>\n",
       "      <td>0.053037</td>\n",
       "      <td>-0.038185</td>\n",
       "      <td>0.042989</td>\n",
       "      <td>0.024946</td>\n",
       "      <td>0.040599</td>\n",
       "      <td>0.025415</td>\n",
       "      <td>1610.774112</td>\n",
       "      <td>40.938027</td>\n",
       "      <td>65.395828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Plesoviche 1.3</td>\n",
       "      <td>2021-10-14 15:13:36.400</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.037535</td>\n",
       "      <td>-0.009361</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>...</td>\n",
       "      <td>322.237324</td>\n",
       "      <td>0.113125</td>\n",
       "      <td>0.025696</td>\n",
       "      <td>0.042786</td>\n",
       "      <td>0.022043</td>\n",
       "      <td>0.041715</td>\n",
       "      <td>0.025634</td>\n",
       "      <td>687.806831</td>\n",
       "      <td>17.631568</td>\n",
       "      <td>28.692160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Plesoviche 1.3</td>\n",
       "      <td>2021-10-14 15:13:36.600</td>\n",
       "      <td>34.8</td>\n",
       "      <td>0.037628</td>\n",
       "      <td>-0.009439</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>...</td>\n",
       "      <td>322.204092</td>\n",
       "      <td>0.014189</td>\n",
       "      <td>0.113527</td>\n",
       "      <td>0.042686</td>\n",
       "      <td>0.023362</td>\n",
       "      <td>0.040811</td>\n",
       "      <td>0.026136</td>\n",
       "      <td>5483.083404</td>\n",
       "      <td>143.308291</td>\n",
       "      <td>223.768826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7352 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SAMPLE ID            Absolute Time Elapsed Time      m238      m232  \\\n",
       "0     95-69_t30 1.1  2021-10-14 15:25:31.200            0  0.002179  0.000486   \n",
       "1     95-69_t30 1.1  2021-10-14 15:25:31.400          0.2  0.001671  0.000327   \n",
       "2     95-69_t30 1.1  2021-10-14 15:25:31.600          0.4  0.001725 -0.000362   \n",
       "3     95-69_t30 1.1  2021-10-14 15:25:31.800          0.6  0.001421 -0.000856   \n",
       "4     95-69_t30 1.1  2021-10-14 15:25:32.000          0.8  0.001338 -0.001459   \n",
       "..              ...                      ...          ...       ...       ...   \n",
       "170  Plesoviche 1.3  2021-10-14 15:13:35.800           34  0.045282 -0.009137   \n",
       "171  Plesoviche 1.3  2021-10-14 15:13:36.000         34.2  0.044659 -0.009184   \n",
       "172  Plesoviche 1.3  2021-10-14 15:13:36.200         34.4   0.04136 -0.009439   \n",
       "173  Plesoviche 1.3  2021-10-14 15:13:36.400         34.6  0.037535 -0.009361   \n",
       "174  Plesoviche 1.3  2021-10-14 15:13:36.600         34.8  0.037628 -0.009439   \n",
       "\n",
       "         m208      m207      m206      m204      m202  ...     SNR_206  \\\n",
       "0    0.003129  0.000475  0.002843  0.000025  0.000018  ...  528.530515   \n",
       "1    0.002867  0.000425  0.002577  0.000024   0.00002  ...  478.986137   \n",
       "2     0.00271  0.000415  0.002484  0.000025   0.00002  ...  461.633933   \n",
       "3    0.002642  0.000397  0.002407  0.000025   0.00002  ...  447.457768   \n",
       "4    0.002702  0.000413   0.00246  0.000022  0.000021  ...  457.283286   \n",
       "..        ...       ...       ...       ...       ...  ...         ...   \n",
       "170  0.000062  0.000085  0.002124  0.000023  0.000023  ...  387.034385   \n",
       "171  0.000059  0.000085  0.002092  0.000022  0.000019  ...  381.188169   \n",
       "172  0.000058  0.000082  0.001942  0.000024  0.000019  ...  353.814728   \n",
       "173  0.000054  0.000077  0.001769  0.000025  0.000021  ...  322.237324   \n",
       "174  0.000055  0.000075  0.001769  0.000023  0.000022  ...  322.204092   \n",
       "\n",
       "      SNR_204   SNR_202   206/238   208/232   207/206   208/206      206/204  \\\n",
       "0    0.104675 -0.079461  0.483094  0.262237  0.166237  1.099762  1201.825093   \n",
       "1    0.072719  0.008978  0.479268  0.243479  0.163913  1.111594  1567.812421   \n",
       "2    0.129567 -0.007103  0.457327  0.244397  0.165967  1.089818   848.053930   \n",
       "3    0.115371 -0.003083  0.469640  0.249383  0.163917  1.096063   923.160452   \n",
       "4   -0.030260  0.053195  0.487798  0.270579  0.166895  1.097159 -3596.971722   \n",
       "..        ...       ...       ...       ...       ...       ...          ...   \n",
       "170  0.031810  0.157323  0.043259  0.023504  0.038401  0.025238  2937.692467   \n",
       "171 -0.017650 -0.050181  0.043155  0.022337  0.039220  0.023859 -5214.531862   \n",
       "172  0.053037 -0.038185  0.042989  0.024946  0.040599  0.025415  1610.774112   \n",
       "173  0.113125  0.025696  0.042786  0.022043  0.041715  0.025634   687.806831   \n",
       "174  0.014189  0.113527  0.042686  0.023362  0.040811  0.026136  5483.083404   \n",
       "\n",
       "         208/204     207/204  \n",
       "0    1321.721567  199.787241  \n",
       "1    1742.771162  256.985380  \n",
       "2     924.224241  140.749146  \n",
       "3    1011.842038  151.321905  \n",
       "4   -3946.450123 -600.318234  \n",
       "..           ...         ...  \n",
       "170    74.142850  112.810400  \n",
       "171  -124.413994 -204.513471  \n",
       "172    40.938027   65.395828  \n",
       "173    17.631568   28.692160  \n",
       "174   143.308291  223.768826  \n",
       "\n",
       "[7352 rows x 50 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fractionation output\n",
    "\n",
    "frac_file = 'NP2_20211014_mod3_baseline-subtracted.xlsx'\n",
    "fractionation_output(frac_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pages (1/6)\n",
      "Counting pages (2/6)                                               \n",
      "Resolving links (4/6)                                                       \n",
      "Loading headers and footers (5/6)                                           \n",
      "Printing pages (6/6)\n",
      "Done                                                                      \n",
      "Loading pages (1/6)\n",
      "Counting pages (2/6)                                               \n",
      "Resolving links (4/6)                                                       \n",
      "Loading headers and footers (5/6)                                           \n",
      "Printing pages (6/6)\n",
      "Done                                                                      \n",
      "Loading pages (1/6)\n",
      "Counting pages (2/6)                                               \n",
      "Resolving links (4/6)                                                       \n",
      "Loading headers and footers (5/6)                                           \n",
      "Printing pages (6/6)\n",
      "Done                                                                      \n",
      "Plot for  95-69_t30 1.1  is complete.\n",
      "Plot for  95-69_t30 1.2  is complete.\n",
      "Plot for  95-69_t30 1.3  is complete.\n",
      "Plot for  Bancroft 1.1  is complete.\n",
      "Plot for  Bancroft 1.2  is complete.\n",
      "Plot for  Bancroft 1.3  is complete.\n",
      "Plot for  Bancroft 1.4  is complete.\n",
      "Plot for  Bancroft 1.5  is complete.\n",
      "Plot for  Bancroft 1.6  is complete.\n",
      "Plot for  Bear Lake 1.1  is complete.\n",
      "Plot for  Bear Lake 1.2  is complete.\n",
      "Plot for  Bear Lake 1.3  is complete.\n",
      "Plot for  Bear Lake 1.4  is complete.\n",
      "Plot for  Bear Lake 1.5  is complete.\n",
      "Plot for  Bear Lake 1.6  is complete.\n",
      "Plot for  SRM NIST 612 1.1  is complete.\n",
      "Plot for  SRM NIST 612 1.2  is complete.\n",
      "Plot for  SRM NIST 612 1.3  is complete.\n",
      "Plot for  SRM NIST 612 1.4  is complete.\n",
      "Plot for  SRM NIST 612 1.5  is complete.\n",
      "Plot for  SRM NIST 612 1.6  is complete.\n",
      "Plot for  SRM NIST 612 1.7  is complete.\n",
      "Plot for  SRM NIST 612 1.8  is complete.\n",
      "Plot for  SRM NIST 612 1.9  is complete.\n",
      "Plot for  MKED-1 1.1  is complete.\n",
      "Plot for  MKED-1 1.2  is complete.\n",
      "Plot for  MKED-1 1.3  is complete.\n",
      "Plot for  MKED-1 1.4  is complete.\n",
      "Plot for  MKED-1 1.5  is complete.\n",
      "Plot for  MKED-1 1.6  is complete.\n",
      "Plot for  MKED-1 1.7  is complete.\n",
      "Plot for  MKED-1 1.8  is complete.\n",
      "Plot for  MKED-1 1.9  is complete.\n",
      "Plot for  Yates 1.1  is complete.\n",
      "Plot for  Yates 1.2  is complete.\n",
      "Plot for  Yates 1.3  is complete.\n",
      "Plot for  Yates 1.4  is complete.\n",
      "Plot for  Yates 1.5  is complete.\n",
      "Plot for  Yates 1.6  is complete.\n",
      "Plot for  Plesoviche 1.1  is complete.\n",
      "Plot for  Plesoviche 1.2  is complete.\n",
      "Plot for  Plesoviche 1.3  is complete.\n",
      "PDF file named: Splitstream_14Oct2021_results.pdf is complete.\n"
     ]
    }
   ],
   "source": [
    "excel_name = str(filename.split('.')[0]) + '_processed.xlsx'\n",
    "files_process_toEXCEL(tester, excel_name)\n",
    "\n",
    "report_name = 'Splitstream_' + date + '_results'\n",
    "U_Pb_report(tester, 'SS2_14Oct.pdf', False, report_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612 primary std\n",
      "612 primary std\n",
      "612 primary std\n",
      "MKED primary std\n",
      "MKED primary std\n",
      "MKED primary std\n"
     ]
    }
   ],
   "source": [
    "### Takes  2:20 to run... be careful\n",
    "# glass_zero_full, glass_zero_groups, glass_zero_full_plot, glass_zero_groups_plot  = stat_rank_and_correct(tester, 0, 'glass')\n",
    "# glass_20_full, glass_20_groups, glass_20_full_plot, glass_20_groups_plot = stat_rank_and_correct(tester, 20, 'glass')\n",
    "# glass_40_full, glass_40_groups, glass_40_full_plot, glass_40_groups_plot = stat_rank_and_correct(tester, 40, 'glass')\n",
    "\n",
    "glass612_zero_full, glass612_zero_groups, glass612_zero_full_plot, glass612_zero_groups_plot  = stat_rank_and_correct(tester, 0, '612')\n",
    "glass612_20_full, glass612_20_groups, glass612_20_full_plot, glass612_20_groups_plot = stat_rank_and_correct(tester, 20, '612')\n",
    "glass612_40_full, glass612_40_groups, glass612_40_full_plot, glass612_40_groups_plot = stat_rank_and_correct(tester, 40, '612')\n",
    "\n",
    "# glassBHVO_zero_full, glassBHVO_zero_groups, glassBHVO_zero_full_plot, glassBHVO_zero_groups_plot  = stat_rank_and_correct(tester, 0, 'BHVO')\n",
    "# glassBHVO_20_full, glassBHVO_20_groups, glassBHVO_20_full_plot, glassBHVO_20_groups_plot = stat_rank_and_correct(tester, 20, 'BHVO')\n",
    "# glassBHVO_40_full, glassBHVO_40_groups, glassBHVO_40_full_plot, glassBHVO_40_groups_plot = stat_rank_and_correct(tester, 40, 'BHVO')\n",
    "\n",
    "\n",
    "ttn_zero_full, ttn_zero_groups, ttn_zero_full_plot, ttn_zero_groups_plot  = stat_rank_and_correct(tester, 0, 'ttn')\n",
    "ttn_20_full, ttn_20_groups, ttn_20_full_plot, ttn_20_groups_plot = stat_rank_and_correct(tester, 20, 'ttn')\n",
    "ttn_40_full, ttn_40_groups, ttn_40_full_plot, ttn_40_groups_plot = stat_rank_and_correct(tester, 40, 'ttn')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dict = {'glass612_zero_full':glass612_zero_full, \n",
    "               'glass612_zero_groups':glass612_zero_groups,\n",
    "               'glass612_20_full': glass612_20_full, \n",
    "               'glass612_20_groups': glass612_20_groups,\n",
    "               'glass612_40_full':glass612_40_full, \n",
    "               'glass612_40_groups':glass612_40_groups,\n",
    "               \n",
    "               \n",
    "               'ttn_zero_full': ttn_zero_full, \n",
    "               'ttn_zero_groups': ttn_zero_groups,\n",
    "               'ttn_20_full': ttn_20_full, \n",
    "               'ttn_20_groups': ttn_20_groups,\n",
    "               'ttn_40_full': ttn_40_full, \n",
    "               'ttn_40_groups':ttn_40_groups\n",
    "              }\n",
    "\n",
    "plot_dict = {\n",
    "               'glass612_zero_full':glass612_zero_full_plot, \n",
    "               'glass612_zero_groups':glass612_zero_groups_plot,\n",
    "               'glass612_20_full': glass612_20_full_plot, \n",
    "               'glass612_20_groups': glass612_20_groups_plot,\n",
    "               'glass612_40_full':glass612_40_full_plot, \n",
    "               'glass612_40_groups':glass612_40_groups_plot,\n",
    "             \n",
    "             \n",
    "               'ttn_zero_full': ttn_zero_full_plot, \n",
    "               'ttn_zero_groups': ttn_zero_groups_plot,\n",
    "               'ttn_20_full': ttn_20_full_plot, \n",
    "               'ttn_20_groups': ttn_20_groups_plot,\n",
    "               'ttn_40_full': ttn_40_full_plot, \n",
    "               'ttn_40_groups':ttn_40_groups_plot\n",
    "              }\n",
    "\n",
    "chrono_dict = {\n",
    "            'glass612_zero_full': chronologic(glass612_zero_full, chrono_order), \n",
    "            'glass612_zero_groups': glass612_zero_groups,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Export for full data worksheets\n",
    "\n",
    "\n",
    "excel_name = 'Processed_LASS_data_difRejects_difStds_2021Oct14.xlsx'\n",
    "with pd.ExcelWriter(excel_name, engine = 'xlsxwriter') as writer:\n",
    "      # Get the xlsxwriter workbook objects.\n",
    "    workbook  = writer.book\n",
    "    col_format1 = workbook.add_format()\n",
    "    col_format2 = workbook.add_format()\n",
    "    col_format3 = workbook.add_format()\n",
    "    col_format1.set_bg_color('#E6E6FA') #Lavender\n",
    "    col_format2.set_bg_color('#98FB98') #Pale Green\n",
    "    col_format3.set_bg_color('#98FB98') #Antique White\n",
    "    \n",
    "    \n",
    "    for sheet in export_dict:\n",
    "        export_dict[sheet].to_excel(writer, sheet_name = sheet, index = True)\n",
    "        worksheet = writer.sheets[sheet]  # pull worksheet object\n",
    "        test_list = list(export_dict[sheet].keys())\n",
    "        worksheet.set_column(0, 0, 20)\n",
    "        worksheet.freeze_panes(1,1)\n",
    "        for idx in enumerate(test_list):\n",
    "            if 'full' in sheet:\n",
    "                if 'BB' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                elif 'corrected' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format1)\n",
    "                elif 'SNR_20' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format2)\n",
    "                    \n",
    "                else:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "            elif 'groups' in sheet:\n",
    "                if 'BB' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                elif 'mean' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format2)\n",
    "                elif 'MSWD' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format3)\n",
    "                elif 'Weighted' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format1)        \n",
    "                else:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Export for plotting worksheets\n",
    "\n",
    "\n",
    "excel_name = 'Processed_LASS_data_difRejects_difStds_plottingNEW2.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(excel_name, engine = 'xlsxwriter') as writer:\n",
    "    # Get the xlsxwriter workbook objects.\n",
    "    workbook  = writer.book\n",
    "    col_format1 = workbook.add_format()\n",
    "    col_format2 = workbook.add_format()\n",
    "    col_format3 = workbook.add_format()\n",
    "    col_format1.set_bg_color('#E6E6FA') #Lavender\n",
    "    col_format2.set_bg_color('#98FB98') #Pale Green\n",
    "    col_format3.set_bg_color('#98FB98') #Antique White\n",
    "    \n",
    "    for sheet in plot_dict:\n",
    "        \n",
    "        plot_dict[sheet].to_excel(writer, sheet_name = sheet, index = True)\n",
    "        worksheet = writer.sheets[sheet]  # pull worksheet object\n",
    "        test_list = list(plot_dict[sheet].keys())\n",
    "        worksheet.set_column(0, 0, 20)\n",
    "        worksheet.freeze_panes(1,1)\n",
    "        for idx in enumerate(test_list):\n",
    "            if 'full' in sheet:\n",
    "                if 'BB' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                else:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format1)\n",
    "            elif 'groups' in sheet:\n",
    "                if 'BB' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                elif 'mean' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format1)\n",
    "                elif 'MSWD' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format3)\n",
    "                elif 'Weighted' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format2)         \n",
    "                else:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Export for chronologic worksheets\n",
    "\n",
    "excel_name = 'Processed_LASS_data_difRejects_difStds_Chrono_test1.xlsx'\n",
    "with pd.ExcelWriter(excel_name, engine = 'xlsxwriter') as writer:\n",
    "      # Get the xlsxwriter workbook objects.\n",
    "    workbook  = writer.book\n",
    "    col_format1 = workbook.add_format()\n",
    "    col_format2 = workbook.add_format()\n",
    "    col_format1.set_bg_color('#E6E6FA') #Lavender\n",
    "    col_format2.set_bg_color('#98FB98') #Pale Green\n",
    "    \n",
    "    for sheet in chrono_dict:\n",
    "\n",
    "        chrono_dict[sheet].to_excel(writer, sheet_name = sheet, index = True)\n",
    "        worksheet = writer.sheets[sheet]  # pull worksheet object\n",
    "        test_list = list(chrono_dict[sheet].keys())\n",
    "        worksheet.set_column(0, 0, 20)\n",
    "        worksheet.freeze_panes(1,1)\n",
    "        for idx in enumerate(test_list):\n",
    "            if 'full' in sheet:\n",
    "                if 'BB' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                elif 'corrected' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format1)\n",
    "                elif 'SNR_20' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format2)\n",
    "                elif 'OPZ' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format1)   \n",
    "                else:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "            elif 'groups' in sheet:\n",
    "                if 'BB' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20)\n",
    "                elif 'mean' in idx[1]:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20, col_format2)\n",
    "                else:\n",
    "                    worksheet.set_column(idx[0]+1, idx[0]+1, 20)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ratio_report(plot_dict, 'fake.pdf', False, 'ratio_output.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****End Here****\n",
    "\n",
    "You should have done 'Export Dict\", U-Pb output report, ratio report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# excel_name = '06May_ttnSS2_NP2_processed.xlsx'\n",
    "# files_ranked_toEXCEL(tester, excel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = '0416_glass_SS_NPII_baseline.xlsx'\n",
    "#calc_dict = calc_CPS(read_np2_timeseries(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filename = '06May_ttnSS2_NP2_baseline_corrected.xlsx'\n",
    "# SS_dict = calc_CPS(read_np2_timeseries(filename))\n",
    "# excel_name = str(filename.split('.')[0]) + '_processed.xlsx'\n",
    "\n",
    "# files_process_toEXCEL(SS_dict, excel_name)\n",
    "\n",
    "# U_Pb_report(SS_dict, 'SS2_6May.pdf', True, 'Splitstream_06May2021_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = '0416_ttn_SS_NP2_data1.xlsx'\n",
    "# ttn_dict = calc_CPS(read_np2_timeseries(filename))\n",
    "\n",
    "# U_Pb_report(ttn_dict, 'titaniteTE.pdf',True, 'Titanite_splitstream_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#U_Pb_report(calc_dict, 'glassTE.pdf',True, 'Glass_splitstream_results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
